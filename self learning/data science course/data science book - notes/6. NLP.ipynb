{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. n-Gram language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.gca().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We'\",\n",
       " 've',\n",
       " 'all',\n",
       " 'heard',\n",
       " 'it',\n",
       " 'according',\n",
       " 'to',\n",
       " 'Hal',\n",
       " 'Varian',\n",
       " 'statistics',\n",
       " 'is',\n",
       " 'the',\n",
       " 'next',\n",
       " 'sexy',\n",
       " 'job',\n",
       " '.',\n",
       " 'Five',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'in',\n",
       " 'What',\n",
       " 'is',\n",
       " 'Web',\n",
       " '2',\n",
       " '.',\n",
       " '0',\n",
       " 'Tim',\n",
       " \"O'\",\n",
       " 'Reilly',\n",
       " 'said',\n",
       " 'that',\n",
       " \"'\",\n",
       " 'data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'next',\n",
       " 'Intel',\n",
       " 'Inside',\n",
       " '.',\n",
       " \"'\",\n",
       " 'But',\n",
       " 'what',\n",
       " 'does',\n",
       " 'that',\n",
       " 'statement',\n",
       " 'mean',\n",
       " 'Why',\n",
       " 'do',\n",
       " 'we',\n",
       " 'suddenly',\n",
       " 'care',\n",
       " 'about',\n",
       " 'statistics',\n",
       " 'and',\n",
       " 'about',\n",
       " 'data',\n",
       " 'In',\n",
       " 'this',\n",
       " 'post',\n",
       " 'I',\n",
       " 'examine',\n",
       " 'the',\n",
       " 'many',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " \"'\",\n",
       " 'the',\n",
       " 'technologies',\n",
       " 'the',\n",
       " 'companies',\n",
       " 'and',\n",
       " 'the',\n",
       " 'unique',\n",
       " 'skill',\n",
       " 'sets',\n",
       " '.',\n",
       " 'Join',\n",
       " 'the',\n",
       " \"O'Reilly\",\n",
       " 'online',\n",
       " 'learning',\n",
       " 'platform',\n",
       " '.',\n",
       " 'Get',\n",
       " 'a',\n",
       " 'free',\n",
       " 'trial',\n",
       " 'today',\n",
       " 'and',\n",
       " 'find',\n",
       " 'answers',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'or',\n",
       " 'master',\n",
       " 'something',\n",
       " 'new',\n",
       " 'and',\n",
       " 'useful',\n",
       " '.',\n",
       " 'The',\n",
       " 'web',\n",
       " 'is',\n",
       " 'full',\n",
       " 'of',\n",
       " \"'\",\n",
       " 'data',\n",
       " 'driven',\n",
       " 'apps',\n",
       " '.',\n",
       " \"'\",\n",
       " 'Almost',\n",
       " 'any',\n",
       " 'e',\n",
       " 'commerce',\n",
       " 'application',\n",
       " 'is',\n",
       " 'a',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'application',\n",
       " '.',\n",
       " \"There'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'database',\n",
       " 'behind',\n",
       " 'a',\n",
       " 'web',\n",
       " 'front',\n",
       " 'end',\n",
       " 'and',\n",
       " 'middleware',\n",
       " 'that',\n",
       " 'talks',\n",
       " 'to',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'other',\n",
       " 'databases',\n",
       " 'and',\n",
       " 'data',\n",
       " 'services',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'processing',\n",
       " 'companies',\n",
       " 'banks',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " '.',\n",
       " 'But',\n",
       " 'merely',\n",
       " 'using',\n",
       " 'data',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'really',\n",
       " 'what',\n",
       " 'we',\n",
       " 'mean',\n",
       " 'by',\n",
       " \"'\",\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " \"'\",\n",
       " 'A',\n",
       " 'data',\n",
       " 'application',\n",
       " 'acquires',\n",
       " 'its',\n",
       " 'value',\n",
       " 'from',\n",
       " 'the',\n",
       " 'data',\n",
       " 'itself',\n",
       " 'and',\n",
       " 'creates',\n",
       " 'more',\n",
       " 'data',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " '.',\n",
       " \"It'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'just',\n",
       " 'an',\n",
       " 'application',\n",
       " 'with',\n",
       " 'data',\n",
       " \"it'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'data',\n",
       " 'product',\n",
       " '.',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'enables',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'data',\n",
       " 'products',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earlier',\n",
       " 'data',\n",
       " 'products',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Web',\n",
       " 'was',\n",
       " 'the',\n",
       " 'CDDB',\n",
       " 'database',\n",
       " '.',\n",
       " 'The',\n",
       " 'developers',\n",
       " 'of',\n",
       " 'CDDB',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'any',\n",
       " 'CD',\n",
       " 'had',\n",
       " 'a',\n",
       " 'unique',\n",
       " 'signature',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'length',\n",
       " 'in',\n",
       " 'samples',\n",
       " 'of',\n",
       " 'each',\n",
       " 'track',\n",
       " 'on',\n",
       " 'the',\n",
       " 'CD',\n",
       " '.',\n",
       " 'Gracenote',\n",
       " 'built',\n",
       " 'a',\n",
       " 'database',\n",
       " 'of',\n",
       " 'track',\n",
       " 'lengths',\n",
       " 'and',\n",
       " 'coupled',\n",
       " 'it',\n",
       " 'to',\n",
       " 'a',\n",
       " 'database',\n",
       " 'of',\n",
       " 'album',\n",
       " 'metadata',\n",
       " 'track',\n",
       " 'titles',\n",
       " 'artists',\n",
       " 'album',\n",
       " 'titles',\n",
       " '.',\n",
       " 'If',\n",
       " \"you'\",\n",
       " 've',\n",
       " 'ever',\n",
       " 'used',\n",
       " 'iTunes',\n",
       " 'to',\n",
       " 'rip',\n",
       " 'a',\n",
       " 'CD',\n",
       " \"you'\",\n",
       " 've',\n",
       " 'taken',\n",
       " 'advantage',\n",
       " 'of',\n",
       " 'this',\n",
       " 'database',\n",
       " '.',\n",
       " 'Before',\n",
       " 'it',\n",
       " 'does',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'iTunes',\n",
       " 'reads',\n",
       " 'the',\n",
       " 'length',\n",
       " 'of',\n",
       " 'every',\n",
       " 'track',\n",
       " 'sends',\n",
       " 'it',\n",
       " 'to',\n",
       " 'CDDB',\n",
       " 'and',\n",
       " 'gets',\n",
       " 'back',\n",
       " 'the',\n",
       " 'track',\n",
       " 'titles',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'CD',\n",
       " \"that'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'in',\n",
       " 'the',\n",
       " 'database',\n",
       " 'including',\n",
       " 'a',\n",
       " 'CD',\n",
       " \"you'\",\n",
       " 've',\n",
       " 'made',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'can',\n",
       " 'create',\n",
       " 'an',\n",
       " 'entry',\n",
       " 'for',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'album',\n",
       " '.',\n",
       " 'While',\n",
       " 'this',\n",
       " 'sounds',\n",
       " 'simple',\n",
       " 'enough',\n",
       " \"it'\",\n",
       " 's',\n",
       " 'revolutionary',\n",
       " 'CDDB',\n",
       " 'views',\n",
       " 'music',\n",
       " 'as',\n",
       " 'data',\n",
       " 'not',\n",
       " 'as',\n",
       " 'audio',\n",
       " 'and',\n",
       " 'creates',\n",
       " 'new',\n",
       " 'value',\n",
       " 'in',\n",
       " 'doing',\n",
       " 'so',\n",
       " '.',\n",
       " 'Their',\n",
       " 'business',\n",
       " 'is',\n",
       " 'fundamentally',\n",
       " 'different',\n",
       " 'from',\n",
       " 'selling',\n",
       " 'music',\n",
       " 'sharing',\n",
       " 'music',\n",
       " 'or',\n",
       " 'analyzing',\n",
       " 'musical',\n",
       " 'tastes',\n",
       " 'though',\n",
       " 'these',\n",
       " 'can',\n",
       " 'also',\n",
       " 'be',\n",
       " \"'\",\n",
       " 'data',\n",
       " \"products'\",\n",
       " '.',\n",
       " 'CDDB',\n",
       " 'arises',\n",
       " 'entirely',\n",
       " 'from',\n",
       " 'viewing',\n",
       " 'a',\n",
       " 'musical',\n",
       " 'problem',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'problem',\n",
       " '.',\n",
       " 'Google',\n",
       " 'is',\n",
       " 'a',\n",
       " 'master',\n",
       " 'at',\n",
       " 'creating',\n",
       " 'data',\n",
       " 'products',\n",
       " '.',\n",
       " \"Here'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'few',\n",
       " 'examples',\n",
       " 'Google',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'the',\n",
       " 'only',\n",
       " 'company',\n",
       " 'that',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " 'data',\n",
       " '.',\n",
       " 'Facebook',\n",
       " 'and',\n",
       " 'LinkedIn',\n",
       " 'use',\n",
       " 'patterns',\n",
       " 'of',\n",
       " 'friendship',\n",
       " 'relationships',\n",
       " 'to',\n",
       " 'suggest',\n",
       " 'other',\n",
       " 'people',\n",
       " 'you',\n",
       " 'may',\n",
       " 'know',\n",
       " 'or',\n",
       " 'should',\n",
       " 'know',\n",
       " 'with',\n",
       " 'sometimes',\n",
       " 'frightening',\n",
       " 'accuracy',\n",
       " '.',\n",
       " 'Amazon',\n",
       " 'saves',\n",
       " 'your',\n",
       " 'searches',\n",
       " 'correlates',\n",
       " 'what',\n",
       " 'you',\n",
       " 'search',\n",
       " 'for',\n",
       " 'with',\n",
       " 'what',\n",
       " 'other',\n",
       " 'users',\n",
       " 'search',\n",
       " 'for',\n",
       " 'and',\n",
       " 'uses',\n",
       " 'it',\n",
       " 'to',\n",
       " 'create',\n",
       " 'surprisingly',\n",
       " 'appropriate',\n",
       " 'recommendations',\n",
       " '.',\n",
       " 'These',\n",
       " 'recommendations',\n",
       " 'are',\n",
       " \"'\",\n",
       " 'data',\n",
       " \"products'\",\n",
       " 'that',\n",
       " 'help',\n",
       " 'to',\n",
       " 'drive',\n",
       " \"Amazon'\",\n",
       " 's',\n",
       " 'more',\n",
       " 'traditional',\n",
       " 'retail',\n",
       " 'business',\n",
       " '.',\n",
       " 'They',\n",
       " 'come',\n",
       " 'about',\n",
       " 'because',\n",
       " 'Amazon',\n",
       " 'understands',\n",
       " 'that',\n",
       " 'a',\n",
       " 'book',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'just',\n",
       " 'a',\n",
       " 'book',\n",
       " 'a',\n",
       " 'camera',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'just',\n",
       " 'a',\n",
       " 'camera',\n",
       " 'and',\n",
       " 'a',\n",
       " 'customer',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'ust',\n",
       " 'a',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'generate',\n",
       " 'a',\n",
       " 'trail',\n",
       " 'of',\n",
       " \"'\",\n",
       " 'data',\n",
       " \"exhaust'\",\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'mined',\n",
       " 'and',\n",
       " 'put',\n",
       " 'to',\n",
       " 'use',\n",
       " 'and',\n",
       " 'a',\n",
       " 'camera',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cloud',\n",
       " 'of',\n",
       " 'data',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'correlated',\n",
       " 'with',\n",
       " 'the',\n",
       " \"customers'\",\n",
       " 'behavior',\n",
       " 'the',\n",
       " 'data',\n",
       " 'they',\n",
       " 'leave',\n",
       " 'every',\n",
       " 'time',\n",
       " 'they',\n",
       " 'visit',\n",
       " 'the',\n",
       " 'site',\n",
       " '.',\n",
       " 'The',\n",
       " 'thread',\n",
       " 'that',\n",
       " 'ties',\n",
       " 'most',\n",
       " 'of',\n",
       " 'these',\n",
       " 'applications',\n",
       " 'together',\n",
       " 'is',\n",
       " 'that',\n",
       " 'data',\n",
       " 'collected',\n",
       " 'from',\n",
       " 'users',\n",
       " 'provides',\n",
       " 'added',\n",
       " 'value',\n",
       " '.',\n",
       " 'Whether',\n",
       " 'that',\n",
       " 'data',\n",
       " 'is',\n",
       " 'search',\n",
       " 'terms',\n",
       " 'voice',\n",
       " 'samples',\n",
       " 'or',\n",
       " 'product',\n",
       " 'reviews',\n",
       " 'the',\n",
       " 'users',\n",
       " 'are',\n",
       " 'in',\n",
       " 'a',\n",
       " 'feedback',\n",
       " 'loop',\n",
       " 'in',\n",
       " 'which',\n",
       " 'they',\n",
       " 'contribute',\n",
       " 'to',\n",
       " 'the',\n",
       " 'products',\n",
       " 'they',\n",
       " 'use',\n",
       " '.',\n",
       " \"That'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'last',\n",
       " 'few',\n",
       " 'years',\n",
       " 'there',\n",
       " 'has',\n",
       " 'been',\n",
       " 'an',\n",
       " 'explosion',\n",
       " 'in',\n",
       " 'the',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'data',\n",
       " \"that'\",\n",
       " 's',\n",
       " 'available',\n",
       " '.',\n",
       " 'Whether',\n",
       " \"we'\",\n",
       " 're',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'web',\n",
       " 'server',\n",
       " 'logs',\n",
       " 'tweet',\n",
       " 'streams',\n",
       " 'online',\n",
       " 'transaction',\n",
       " 'records',\n",
       " \"'\",\n",
       " 'citizen',\n",
       " 'science',\n",
       " \"'\",\n",
       " 'data',\n",
       " 'from',\n",
       " 'sensors',\n",
       " 'government',\n",
       " 'data',\n",
       " 'or',\n",
       " 'some',\n",
       " 'other',\n",
       " 'source',\n",
       " 'the',\n",
       " 'problem',\n",
       " \"isn'\",\n",
       " 't',\n",
       " 'finding',\n",
       " 'data',\n",
       " \"it'\",\n",
       " 's',\n",
       " 'figuring',\n",
       " 'out',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'it',\n",
       " '.',\n",
       " 'And',\n",
       " \"it'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'just',\n",
       " 'companies',\n",
       " 'using',\n",
       " 'their',\n",
       " 'own',\n",
       " 'data',\n",
       " 'or',\n",
       " 'the',\n",
       " 'data',\n",
       " 'contributed',\n",
       " 'by',\n",
       " 'their',\n",
       " 'users',\n",
       " '.',\n",
       " \"It'\",\n",
       " 's',\n",
       " 'increasingly',\n",
       " 'common',\n",
       " 'to',\n",
       " 'mashup',\n",
       " 'data',\n",
       " 'from',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'sources',\n",
       " '.',\n",
       " \"'\",\n",
       " 'Data',\n",
       " 'Mashups',\n",
       " 'in',\n",
       " \"R'\",\n",
       " 'analyzes',\n",
       " 'mortgage',\n",
       " 'foreclosures',\n",
       " 'in',\n",
       " 'Philadelphia',\n",
       " 'County',\n",
       " 'by',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'public',\n",
       " 'report',\n",
       " 'from',\n",
       " 'the',\n",
       " 'county',\n",
       " \"sheriff'\",\n",
       " 's',\n",
       " 'office',\n",
       " 'extracting',\n",
       " 'addresses',\n",
       " 'and',\n",
       " 'using',\n",
       " 'Yahoo',\n",
       " 'to',\n",
       " 'convert',\n",
       " 'the',\n",
       " 'addresses',\n",
       " 'to',\n",
       " 'latitude',\n",
       " 'and',\n",
       " 'longitude',\n",
       " 'then',\n",
       " 'using',\n",
       " 'the',\n",
       " 'geographical',\n",
       " 'data',\n",
       " 'to',\n",
       " 'place',\n",
       " 'the',\n",
       " 'foreclosures',\n",
       " 'on',\n",
       " 'a',\n",
       " 'map',\n",
       " 'another',\n",
       " 'data',\n",
       " 'source',\n",
       " 'and',\n",
       " 'group',\n",
       " 'them',\n",
       " 'by',\n",
       " 'neighborhood',\n",
       " 'valuation',\n",
       " 'neighborhood',\n",
       " 'per',\n",
       " 'capita',\n",
       " 'income',\n",
       " 'and',\n",
       " 'other',\n",
       " 'socio',\n",
       " 'economic',\n",
       " 'factors',\n",
       " '.',\n",
       " 'The',\n",
       " 'question',\n",
       " 'facing',\n",
       " 'every',\n",
       " 'company',\n",
       " 'today',\n",
       " 'every',\n",
       " 'startup',\n",
       " 'every',\n",
       " 'non',\n",
       " 'profit',\n",
       " 'every',\n",
       " 'project',\n",
       " 'site',\n",
       " 'that',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'attract',\n",
       " 'a',\n",
       " 'community',\n",
       " 'is',\n",
       " 'how',\n",
       " 'to',\n",
       " 'use',\n",
       " 'data',\n",
       " 'effectively',\n",
       " \"'\",\n",
       " 'not',\n",
       " 'just',\n",
       " 'their',\n",
       " 'own',\n",
       " 'data',\n",
       " 'but',\n",
       " 'all',\n",
       " 'the',\n",
       " 'data',\n",
       " \"that'\",\n",
       " 's',\n",
       " 'available',\n",
       " 'and',\n",
       " 'relevant',\n",
       " '.',\n",
       " 'Using',\n",
       " 'data',\n",
       " 'effectively',\n",
       " 'requires',\n",
       " 'something',\n",
       " 'different',\n",
       " 'from',\n",
       " 'traditional',\n",
       " 'statistics',\n",
       " 'where',\n",
       " 'actuaries',\n",
       " 'in',\n",
       " 'business',\n",
       " 'suits',\n",
       " 'perform',\n",
       " 'arcane',\n",
       " 'but',\n",
       " 'fairly',\n",
       " 'well',\n",
       " 'defined',\n",
       " 'kinds',\n",
       " 'of',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'What',\n",
       " 'differentiates',\n",
       " 'data',\n",
       " 'science',\n",
       " 'from',\n",
       " 'statistics',\n",
       " 'is',\n",
       " 'that',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'a',\n",
       " 'holistic',\n",
       " 'approach',\n",
       " '.',\n",
       " \"We'\",\n",
       " 're',\n",
       " 'increasingly',\n",
       " 'finding',\n",
       " 'data',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wild',\n",
       " 'and',\n",
       " 'data',\n",
       " 'scientists',\n",
       " 'are',\n",
       " 'involved',\n",
       " 'with',\n",
       " 'gathering',\n",
       " 'data',\n",
       " 'massaging',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'tractable',\n",
       " 'form',\n",
       " 'making',\n",
       " 'it',\n",
       " 'tell',\n",
       " 'its',\n",
       " 'story',\n",
       " 'and',\n",
       " 'presenting',\n",
       " 'that',\n",
       " 'story',\n",
       " 'to',\n",
       " 'others',\n",
       " '.',\n",
       " 'To',\n",
       " 'get',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'for',\n",
       " 'what',\n",
       " 'skills',\n",
       " 'are',\n",
       " 'required',\n",
       " \"let'\",\n",
       " 's',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'data',\n",
       " 'lifecycle',\n",
       " 'where',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'how',\n",
       " 'you',\n",
       " 'use',\n",
       " 'it',\n",
       " 'and',\n",
       " 'where',\n",
       " 'it',\n",
       " 'goes',\n",
       " '.',\n",
       " 'Data',\n",
       " 'is',\n",
       " 'everywhere',\n",
       " 'your',\n",
       " 'government',\n",
       " 'your',\n",
       " 'web',\n",
       " 'server',\n",
       " 'your',\n",
       " 'business',\n",
       " 'partners',\n",
       " 'even',\n",
       " 'your',\n",
       " 'body',\n",
       " '.',\n",
       " 'While',\n",
       " 'we',\n",
       " \"aren'\",\n",
       " 't',\n",
       " 'drowning',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sea',\n",
       " 'of',\n",
       " 'data',\n",
       " \"we'\",\n",
       " 're',\n",
       " 'finding',\n",
       " 'that',\n",
       " 'almost',\n",
       " 'everything',\n",
       " 'can',\n",
       " 'or',\n",
       " 'has',\n",
       " 'been',\n",
       " 'instrumented',\n",
       " '.',\n",
       " 'At',\n",
       " \"O'\",\n",
       " 'Reilly',\n",
       " 'we',\n",
       " 'frequently',\n",
       " 'combine',\n",
       " 'publishing',\n",
       " 'industry',\n",
       " 'data',\n",
       " 'from',\n",
       " 'Nielsen',\n",
       " 'BookScan',\n",
       " 'with',\n",
       " 'our',\n",
       " 'own',\n",
       " 'sales',\n",
       " 'data',\n",
       " 'publicly',\n",
       " 'available',\n",
       " 'Amazon',\n",
       " 'data',\n",
       " 'and',\n",
       " 'even',\n",
       " 'job',\n",
       " 'data',\n",
       " 'to',\n",
       " 'see',\n",
       " \"what'\",\n",
       " 's',\n",
       " 'happening',\n",
       " 'in',\n",
       " 'the',\n",
       " 'publishing',\n",
       " 'industry',\n",
       " '.',\n",
       " 'Sites',\n",
       " 'like',\n",
       " 'Infochimps',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_unicode(text: str) -> str:\n",
    "    return text.replace(u\"\\u00E2\", \"'\")\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://www.oreilly.com/ideas/what-is-data-science\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "content = soup.find(\"div\", \"main-post-radar-content\")   # find article-body div\n",
    "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period\n",
    "\n",
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "    document.extend(words)\n",
    "document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\\w` is any alphanumeric character (equivalent to [a-zA-Z0-9_])\n",
    "\n",
    "`+` matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",
    "\n",
    "`|` is or, especially with the `[]` on both sides. It defines them as a subpattern.\n",
    "\n",
    "`\\.` is period (`.` is any character)\n",
    "\n",
    "See https://regex101.com/, https://regexr.com/ and https://regexone.com/lesson/introduction_abcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We' ve all heard it according to Hal Varian statistics is the next sexy job . Five years ago in What is Web 2 . 0 Tim O' Reilly said that ' data is the next Intel Inside . ' But what does that statement mean Why do we suddenly care about statistics and about data In this post I examine the many sides of data science ' the technologies the companies and the unique skill sets . Join the O'Reilly online learning platform . Get a free trial today and find answers on the fly or master something new and useful . The web is full of ' data driven apps . ' Almost any e commerce application is a data driven application . There' s a database behind a web front end and middleware that talks to a number of other databases and data services credit card processing companies banks and so on . But merely using data isn' t really what we mean by ' data science . ' A data application acquires its value from the data itself and creates more data as a result . It' s not just an application with data it' s a data product . Data science enables the creation of data products . One of the earlier data products on the Web was the CDDB database . The developers of CDDB realized that any CD had a unique signature based on the exact length in samples of each track on the CD . Gracenote built a database of track lengths and coupled it to a database of album metadata track titles artists album titles . If you' ve ever used iTunes to rip a CD you' ve taken advantage of this database . Before it does anything else iTunes reads the length of every track sends it to CDDB and gets back the track titles . If you have a CD that' s not in the database including a CD you' ve made yourself you can create an entry for an unknown album . While this sounds simple enough it' s revolutionary CDDB views music as data not as audio and creates new value in doing so . Their business is fundamentally different from selling music sharing music or analyzing musical tastes though these can also be ' data products' . CDDB arises entirely from viewing a musical problem as a data problem . Google is a master at creating data products . Here' s a few examples Google isn' t the only company that knows how to use data . Facebook and LinkedIn use patterns of friendship relationships to suggest other people you may know or should know with sometimes frightening accuracy . Amazon saves your searches correlates what you search for with what other users search for and uses it to create surprisingly appropriate recommendations . These recommendations are ' data products' that help to drive Amazon' s more traditional retail business . They come about because Amazon understands that a book isn' t just a book a camera isn' t just a camera and a customer isn' t ust a customer customers generate a trail of ' data exhaust' that can be mined and put to use and a camera is a cloud of data that can be correlated with the customers' behavior the data they leave every time they visit the site . The thread that ties most of these applications together is that data collected from users provides added value . Whether that data is search terms voice samples or product reviews the users are in a feedback loop in which they contribute to the products they use . That' s the beginning of data science . In the last few years there has been an explosion in the amount of data that' s available . Whether we' re talking about web server logs tweet streams online transaction records ' citizen science ' data from sensors government data or some other source the problem isn' t finding data it' s figuring out what to do with it . And it' s not just companies using their own data or the data contributed by their users . It' s increasingly common to mashup data from a number of sources . ' Data Mashups in R' analyzes mortgage foreclosures in Philadelphia County by taking a public report from the county sheriff' s office extracting addresses and using Yahoo to convert the addresses to latitude and longitude then using the geographical data to place the foreclosures on a map another data source and group them by neighborhood valuation neighborhood per capita income and other socio economic factors . The question facing every company today every startup every non profit every project site that wants to attract a community is how to use data effectively ' not just their own data but all the data that' s available and relevant . Using data effectively requires something different from traditional statistics where actuaries in business suits perform arcane but fairly well defined kinds of analysis . What differentiates data science from statistics is that data science is a holistic approach . We' re increasingly finding data in the wild and data scientists are involved with gathering data massaging it into a tractable form making it tell its story and presenting that story to others . To get a sense for what skills are required let' s look at the data lifecycle where it comes from how you use it and where it goes . Data is everywhere your government your web server your business partners even your body . While we aren' t drowning in a sea of data we' re finding that almost everything can or has been instrumented . At O' Reilly we frequently combine publishing industry data from Nielsen BookScan with our own sales data publicly available Amazon data and even job data to see what' s happening in the publishing industry . Sites like Infochimps and Factual provide access to many large datasets including climate data MySpace activity streams and game logs from sporting events . Factual enlists users to update and improve its datasets which cover topics as diverse as endocrinologists to hiking trails . Much of the data we currently work with is the direct consequence of Web 2 . 0 and of Moore' s Law applied to data . The web has people spending more time online and leaving a trail of data wherever they go . Mobile applications leave an even richer data trail since many of them are annotated with geolocation or involve video or audio all of which can be mined . Point of sale devices and frequent shopper' s cards make it possible to capture all of your retail transactions not just the ones you make online . All of this data would be useless if we couldn' t store it and that' s where Moore' s Law comes in . Since the early ' 80s processor speed has increased from 10 MHz to 3 . 6 GHz ' an increase of 360 not counting increases in word length and number of cores . But we' ve seen much bigger increases in storage capacity on every level . RAM has moved from 1 000 MB to roughly 25 GB ' a price reduction of about 40000 to say nothing of the reduction in size and increase in speed . Hitachi made the first gigabyte disk drives in 1982 weighing in at roughly 250 pounds now terabyte drives are consumer equipment and a 32 GB microSD card weighs about half a gram . Whether you look at bits per gram bits per dollar or raw capacity storage has more than kept pace with the increase of CPU speed . The importance of Moore' s law as applied to data isn' t just geek pyrotechnics . Data expands to fill the space you have to store it . The more storage is available the more data you will find to put into it . The data exhaust you leave behind whenever you surf the web friend someone on Facebook or make a purchase in your local supermarket is all carefully collected and analyzed . Increased storage capacity demands increased sophistication in the analysis and use of that data . That' s the foundation of data science . So how do we make that data useful The first step of any data analysis project is ' data conditioning ' or getting data into a state where it' s usable . We are seeing more data in formats that are easier to consume Atom data feeds web services microformats and other newer technologies provide data in formats that' s directly machine consumable . But old style screen scraping hasn' t died and isn' t going to die . Many sources of ' wild data' are extremely messy . They aren' t well behaved XML files with all the metadata nicely in place . The foreclosure data used in ' Data Mashups in R' was posted on a public website by the Philadelphia county sheriff' s office . This data was presented as an HTML file that was probably generated automatically from a spreadsheet . If you' ve ever seen the HTML that' s generated by Excel you know that' s going to be fun to process . Data conditioning can involve cleaning up messy HTML with tools like Beautiful Soup natural language processing to parse plain text in English and other languages or even getting humans to do the dirty work . You' re likely to be dealing with an array of data sources all in different forms . It would be nice if there was a standard set of tools to do the job but there isn' t . To do data conditioning you have to be ready for whatever comes and be willing to use anything from ancient Unix utilities such as awk to XML parsers and machine learning libraries . Scripting languages such as Perl and Python are essential . Once you' ve parsed the data you can start thinking about the quality of your data . Data is frequently missing or incongruous . If data is missing do you simply ignore the missing points That isn' t always possible . If data is incongruous do you decide that something is wrong with badly behaved data after all equipment fails or that the incongruous data is telling its own story which may be more interesting It' s reported that the discovery of ozone layer depletion was delayed because automated data collection tools discarded readings that were too low 1 . In data science what you have is frequently all you' re going to get . It' s usually impossible to get ' better' data and you have no alternative but to work with the data at hand . If the problem involves human language understanding the data adds another dimension to the problem . Roger Magoulas who runs the data analysis group at O' Reilly was recently searching a database for Apple job listings requiring geolocation skills . While that sounds like a simple task the trick was disambiguating ' Apple' from many job postings in the growing Apple industry . To do it well you need to understand the grammatical structure of a job posting you need to be able to parse the English . And that problem is showing up more and more frequently . Try using Google Trends to figure out what' s happening with the Cassandra database or the Python language and you' ll get a sense of the problem . Google has indexed many many websites about large snakes . Disambiguation is never an easy task but tools like the Natural Language Toolkit library can make it simpler . When natural language processing fails you can replace artificial intelligence with human intelligence . That' s where services like Amazon' s Mechanical Turk come in . If you can split your task up into a large number of subtasks that are easily described you can use Mechanical Turk' s marketplace for cheap labor . For example if you' re looking at job listings and want to know which originated with Apple you can have real people do the classification for roughly 0 . 01 each . If you have already reduced the set to 10 000 postings with the word ' Apple ' paying humans 0 . 01 to classify them only costs 100 . We' ve all heard a lot about ' big data ' but ' big' is really a red herring . Oil companies telecommunications companies and other data centric industries have had huge datasets for a long time . And as storage capacity continues to expand today' s ' big' is certainly tomorrow' s ' medium' and next week' s ' small . ' The most meaningful definition I' ve heard ' big data' is when the size of the data itself becomes part of the problem . We' re discussing data problems ranging from gigabytes to petabytes of data . At some point traditional techniques for working with data run out of steam . What are we trying to do with data that' s different According to Jeff Hammerbacher 2 hackingdata we' re trying to build information platforms or dataspaces . Information platforms are similar to traditional data warehouses but different . They expose rich APIs and are designed for exploring and understanding the data rather than for traditional analysis and reporting . They accept all data formats including the most messy and their schemas evolve as the understanding of the data changes . Most of the organizations that have built data platforms have found it necessary to go beyond the relational database model . Traditional relational database systems stop being effective at this scale . Managing sharding and replication across a horde of database servers is difficult and slow . The need to define a schema in advance conflicts with reality of multiple unstructured data sources in which you may not know what' s important until after you' ve analyzed the data . Relational databases are designed for consistency to support complex transactions that can easily be rolled back if any one of a complex set of operations fails . While rock solid consistency is crucial to many applications it' s not really necessary for the kind of analysis we' re discussing here . Do you really care if you have 1 010 or 1 012 Twitter followers Precision has an allure but in most data driven applications outside of finance that allure is deceptive . Most data analysis is comparative if you' re asking whether sales to Northern Europe are increasing faster than sales to Southern Europe you aren' t concerned about the difference between 5 . 92 percent annual growth and 5 . 93 percent . To store huge datasets effectively we' ve seen a new breed of databases appear . These are frequently called NoSQL databases or Non Relational databases though neither term is very useful . They group together fundamentally dissimilar products by telling you what they aren' t . Many of these databases are the logical descendants of Google' s BigTable and Amazon' s Dynamo and are designed to be distributed across many nodes to provide ' eventual consistency' but not absolute consistency and to have very flexible schema . While there are two dozen or so products available almost all of them open source a few leaders have established themselves Storing data is only part of building a data platform though . Data is only useful if you can do something with it and enormous datasets present computational problems . Google popularized the MapReduce approach which is basically a divide and conquer strategy for distributing an extremely large problem across an extremely large computing cluster . In the ' map' stage a programming task is divided into a number of identical subtasks which are then distributed across many processors the intermediate results are then combined by a single reduce task . In hindsight MapReduce seems like an obvious solution to Google' s biggest problem creating large searches . It' s easy to distribute a search across thousands of processors and then combine the results into a single set of answers . What' s less obvious is that MapReduce has proven to be widely applicable to many large data problems ranging from search to machine learning . The most popular open source implementation of MapReduce is the Hadoop project . Yahoo' s claim that they had built the world' s largest production Hadoop application with 10 000 cores running Linux brought it onto center stage . Many of the key Hadoop developers have found a home at Cloudera which provides commercial support . Amazon' s Elastic MapReduce makes it much easier to put Hadoop to work without investing in racks of Linux machines by providing preconfigured Hadoop images for its EC2 clusters . You can allocate and de allocate processors as needed paying only for the time you use them . Hadoop goes far beyond a simple MapReduce implementation of which there are several it' s the key component of a data platform . It incorporates HDFS a distributed filesystem designed for the performance and reliability requirements of huge datasets the HBase database Hive which lets developers explore Hadoop datasets using SQL like queries a high level dataflow language called Pig and other components . If anything can be called a one stop information platform Hadoop is it . Hadoop has been instrumental in enabling ' agile' data analysis . In software development ' agile practices' are associated with faster product cycles closer interaction between developers and consumers and testing . Traditional data analysis has been hampered by extremely long turn around times . If you start a calculation it might not finish for hours or even days . But Hadoop and particularly Elastic MapReduce make it easy to build clusters that can perform computations on long datasets quickly . Faster computations make it easier to test different assumptions different datasets and different algorithms . It' s easer to consult with clients to figure out whether you' re asking the right questions and it' s possible to pursue intriguing possibilities that you' d otherwise have to drop for lack of time . Hadoop is essentially a batch system but Hadoop Online Prototype HOP is an experimental project that enables stream processing . Hadoop processes data as it arrives and delivers intermediate results in near real time . Near real time data analysis enables features like trending topics on sites like Twitter . These features only require soft real time reports on trending topics don' t require millisecond accuracy . As with the number of followers on Twitter a ' trending topics' report only needs to be current to within five minutes ' or even an hour . According to Hilary Mason hmason data scientist at bit . ly it' s possible to precompute much of the calculation then use one of the experiments in real time MapReduce to get presentable results . Machine learning is another essential tool for the data scientist . We now expect web and mobile applications to incorporate recommendation engines and building a recommendation engine is a quintessential artificial intelligence problem . You don' t have to look at many modern web applications to see classification error detection image matching behind Google Goggles and SnapTell and even face detection ' an ill advised mobile application lets you take someone' s picture with a cell phone and look up that person' s identity using photos available online . Andrew Ng' s Machine Learning course is one of the most popular courses in computer science at Stanford with hundreds of students this video is highly recommended . There are many libraries available for machine learning PyBrain in Python Elefant Weka in Java and Mahout coupled to Hadoop . Google has just announced their Prediction API which exposes their machine learning algorithms for public use via a RESTful interface . For computer vision the OpenCV library is a de facto standard . Mechanical Turk is also an important part of the toolbox . Machine learning almost always requires a ' training set ' or a significant body of known data with which to develop and tune the application . The Turk is an excellent way to develop training sets . Once you' ve collected your training data perhaps a large collection of public photos from Twitter you can have humans classify them inexpensively ' possibly sorting them into categories possibly drawing circles around faces cars or whatever interests you . It' s an excellent way to classify a few thousand data points at a cost of a few cents each . Even a relatively large job only costs a few hundred dollars . While I haven' t stressed traditional statistics building statistical models plays an important role in any data analysis . According to Mike Driscoll dataspora statistics is the ' grammar of data science . ' It is crucial to ' making data speak coherently . ' We' ve all heard the joke that eating pickles causes death because everyone who dies has eaten pickles . That joke doesn' t work if you understand what correlation means . More to the point it' s easy to notice that one advertisement for R in a Nutshell generated 2 percent more conversions than another . But it takes statistics to know whether this difference is significant or just a random fluctuation . Data science isn' t just about the existence of data or making guesses about what that data might mean it' s about testing hypotheses and making sure that the conclusions you' re drawing from the data are valid . Statistics plays a role in everything from traditional business intelligence BI to understanding how Google' s ad auctions work . Statistics has become a basic skill . It isn' t superseded by newer techniques from machine learning and other disciplines it complements them . While there are many commercial statistical packages the open source R language ' and its comprehensive package library CRAN ' is an essential tool . Although R is an odd and quirky language particularly to someone with a background in computer science it comes close to providing ' one stop shopping' for most statistical work . It has excellent graphics facilities CRAN includes parsers for many kinds of data and newer extensions extend R into distributed computing . If there' s a single tool that provides an end to end solution for statistics work R is it . A picture may or may not be worth a thousand words but a picture is certainly worth a thousand numbers . The problem with most data analysis algorithms is that they generate a set of numbers . To understand what the numbers mean the stories they are really telling you need to generate a graph . Edward Tufte' s Visual Display of Quantitative Information is the classic for data visualization and a foundational text for anyone practicing data science . But that' s not really what concerns us here . Visualization is crucial to each stage of the data scientist . According to Martin Wattenberg wattenberg founder of Flowing Media visualization is key to data conditioning if you want to find out just how bad your data is try plotting it . Visualization is also frequently the first step in analysis . Hilary Mason says that when she gets a new data set she starts by making a dozen or more scatter plots trying to get a sense of what might be interesting . Once you' ve gotten some hints at what the data might be saying you can follow it up with more detailed analysis . There are many packages for plotting and presenting data . GnuPlot is very effective R incorporates a fairly comprehensive graphics package Casey Reas' and Ben Fry' s Processing is the state of the art particularly if you need to create animations that show how things change over time . At IBM' s Many Eyes many of the visualizations are full fledged interactive applications . Nathan Yau' s FlowingData blog is a great place to look for creative visualizations . One of my favorites is this animation of the growth of Walmart over time . And this is one place where ' art' comes in not just the aesthetics of the visualization itself but how you understand it . Does it look like the spread of cancer throughout a body Or the spread of a flu virus through a population Making data tell its story isn' t just a matter of presenting results it involves making connections then going back to other data sources to verify them . Does a successful retail chain spread like an epidemic and if so does that give us new insights into how economies work That' s not a question we could even have asked a few years ago . There was insufficient computing power the data was all locked up in proprietary sources and the tools for working with the data were insufficient . It' s the kind of question we now ask routinely . Data science requires skills ranging from traditional computer science to mathematics to art . Describing the data science group he put together at Facebook possibly the first data science group at a consumer oriented web property Jeff Hammerbacher said ' on any given day a team member could author a multistage processing pipeline in Python design a hypothesis test perform a regression analysis over data samples with R design and implement an algorithm for some data intensive product or service in Hadoop or communicate the results of our analyses to other members of the organization 3 Where do you find the people this versatile According to DJ Patil chief scientist at LinkedIn dpatil the best data scientists tend to be ' hard scientists ' particularly physicists rather than computer science majors . Physicists have a strong mathematical background computing skills and come from a discipline in which survival depends on getting the most from the data . They have to think about the big picture the big problem . When you' ve just spent a lot of grant money generating data you can' t just throw the data out if it isn' t as clean as you' d like . You have to make it tell its story . You need some creativity for when the story the data is telling isn' t what you think it' s telling . Scientists also know how to break large problems up into smaller problems . Patil described the process of creating the group recommendation feature at LinkedIn . It would have been easy to turn this into a high ceremony development project that would take thousands of hours of developer time plus thousands of hours of computing time to do massive correlations across LinkedIn' s membership . But the process worked quite differently it started out with a relatively small simple program that looked at members' profiles and made recommendations accordingly . Asking things like did you go to Cornell Then you might like to join the Cornell Alumni group . It then branched out incrementally . In addition to looking at profiles LinkedIn' s data scientists started looking at events that members attended . Then at books members had in their libraries . The result was a valuable data product that analyzed a huge database ' but it was never conceived as such . It started small and added value iteratively . It was an agile flexible process that built toward its goal incrementally rather than tackling a huge mountain of data all at once . This is the heart of what Patil calls ' data jiujitsu' ' using smaller auxiliary problems to solve a large difficult problem that appears intractable . CDDB is a great example of data jiujitsu identifying music by analyzing an audio stream directly is a very difficult problem though not unsolvable ' see midomi for example . But the CDDB staff used data creatively to solve a much more tractable problem that gave them the same result . Computing a signature based on track lengths and then looking up that signature in a database is trivially simple . Entrepreneurship is another piece of the puzzle . Patil' s first flippant answer to ' what kind of person are you looking for when you hire a data scientist ' was ' someone you would start a company with . ' That' s an important insight we' re entering the era of products that are built on data . We don' t yet know what those products are but we do know that the winners will be the people and the companies that find those products . Hilary Mason came to the same conclusion . Her job as scientist at bit . ly is really to investigate the data that bit . ly is generating and find out how to build interesting products from it . No one in the nascent data industry is trying to build the 2012 Nissan Stanza or Office 2015 they' re all trying to find new products . In addition to being physicists mathematicians programmers and artists they' re entrepreneurs . Data scientists combine entrepreneurship with patience the willingness to build data products incrementally the ability to explore and the ability to iterate over a solution . They are inherently interdiscplinary . They can tackle all aspects of a problem from initial data collection and data conditioning to drawing conclusions . They can think outside the box to come up with new ways to view the problem or to work with very broadly defined problems ' here' s a lot of data what can you make from it ' The future belongs to the companies who figure out how to collect and use data successfully . Google Amazon Facebook and LinkedIn have all tapped into their datastreams and made that the core of their success . They were the vanguard but newer companies like bit . ly are following their path . Whether it' s mining your personal biology building maps from the shared experience of millions of travellers or studying the URLs that people pass to others the next generation of successful businesses will be built around data . The part of Hal Varian' s quote that nobody remembers says it all The ability to take data ' to be able to understand it to process it to extract value from it to visualize it to communicate it ' that' s going to be a hugely important skill in the next decades . Data is indeed the new Intel Inside . 1 The NASA article denies this but also says that in 1984 they decided that the low values whch went back to the 70s were ' real . ' Whether humans or software decided to ignore anomalous data it appears that data was ignored . 2 ' Information Platforms as Dataspaces ' by Jeff Hammerbacher in Beautiful Data 3 ' Information Platforms as Dataspaces ' by Jeff Hammerbacher in Beautiful Data\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "transitions = defaultdict(list)\n",
    "for prev, current in zip(document, document[1:]):\n",
    "    transitions[prev].append(current)\n",
    "\n",
    "def generate_using_bigrams() -> str:\n",
    "    current = \".\"   # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # choose one at random\n",
    "        result.append(current)                         # append it to results\n",
    "        if current == \".\": return \" \".join(result)     # if \".\" we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It is significant or the number of a programming task but different datasets using Google popularized the next week' s an application lets developers and tune the low 1 The web applications to say nothing of successful retail business .\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that's it. Rerun generate to produce more nonsense.\n",
    "- Note that transitions is actually a dict of all possible consecutive elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {\"We'\": ['ve', 're', 've', 're', 've'],\n",
       "             've': ['all',\n",
       "              'ever',\n",
       "              'taken',\n",
       "              'made',\n",
       "              'seen',\n",
       "              'ever',\n",
       "              'parsed',\n",
       "              'all',\n",
       "              'heard',\n",
       "              'analyzed',\n",
       "              'seen',\n",
       "              'collected',\n",
       "              'all',\n",
       "              'gotten',\n",
       "              'just'],\n",
       "             'all': ['heard',\n",
       "              'the',\n",
       "              'of',\n",
       "              'of',\n",
       "              'carefully',\n",
       "              'the',\n",
       "              'in',\n",
       "              'equipment',\n",
       "              \"you'\",\n",
       "              'heard',\n",
       "              'data',\n",
       "              'of',\n",
       "              'heard',\n",
       "              'locked',\n",
       "              'at',\n",
       "              'trying',\n",
       "              'aspects',\n",
       "              'tapped',\n",
       "              'The'],\n",
       "             'heard': ['it', 'a', \"'\", 'the'],\n",
       "             'it': ['according',\n",
       "              'to',\n",
       "              'does',\n",
       "              'to',\n",
       "              'to',\n",
       "              '.',\n",
       "              'into',\n",
       "              'tell',\n",
       "              'comes',\n",
       "              'and',\n",
       "              'goes',\n",
       "              'possible',\n",
       "              'and',\n",
       "              '.',\n",
       "              '.',\n",
       "              'well',\n",
       "              'simpler',\n",
       "              'necessary',\n",
       "              'and',\n",
       "              'onto',\n",
       "              'much',\n",
       "              '.',\n",
       "              'might',\n",
       "              'easy',\n",
       "              'easier',\n",
       "              'arrives',\n",
       "              'takes',\n",
       "              'complements',\n",
       "              'comes',\n",
       "              '.',\n",
       "              '.',\n",
       "              'up',\n",
       "              '.',\n",
       "              'look',\n",
       "              'involves',\n",
       "              \"isn'\",\n",
       "              'tell',\n",
       "              'started',\n",
       "              'was',\n",
       "              '.',\n",
       "              \"'\",\n",
       "              'all',\n",
       "              'to',\n",
       "              'to',\n",
       "              'to',\n",
       "              'to',\n",
       "              \"'\",\n",
       "              'appears'],\n",
       "             'according': ['to'],\n",
       "             'to': ['Hal',\n",
       "              'a',\n",
       "              'a',\n",
       "              'rip',\n",
       "              'CDDB',\n",
       "              'use',\n",
       "              'suggest',\n",
       "              'create',\n",
       "              'drive',\n",
       "              'use',\n",
       "              'the',\n",
       "              'do',\n",
       "              'mashup',\n",
       "              'convert',\n",
       "              'latitude',\n",
       "              'place',\n",
       "              'attract',\n",
       "              'use',\n",
       "              'others',\n",
       "              'see',\n",
       "              'many',\n",
       "              'update',\n",
       "              'hiking',\n",
       "              'data',\n",
       "              'capture',\n",
       "              '3',\n",
       "              'roughly',\n",
       "              'say',\n",
       "              'data',\n",
       "              'fill',\n",
       "              'store',\n",
       "              'put',\n",
       "              'consume',\n",
       "              'die',\n",
       "              'be',\n",
       "              'process',\n",
       "              'parse',\n",
       "              'do',\n",
       "              'be',\n",
       "              'do',\n",
       "              'be',\n",
       "              'use',\n",
       "              'XML',\n",
       "              'get',\n",
       "              'get',\n",
       "              'work',\n",
       "              'the',\n",
       "              'understand',\n",
       "              'be',\n",
       "              'parse',\n",
       "              'figure',\n",
       "              'know',\n",
       "              '10',\n",
       "              'classify',\n",
       "              'expand',\n",
       "              'petabytes',\n",
       "              'do',\n",
       "              'Jeff',\n",
       "              'build',\n",
       "              'traditional',\n",
       "              'go',\n",
       "              'define',\n",
       "              'support',\n",
       "              'many',\n",
       "              'Northern',\n",
       "              'Southern',\n",
       "              'be',\n",
       "              'provide',\n",
       "              'have',\n",
       "              \"Google'\",\n",
       "              'distribute',\n",
       "              'be',\n",
       "              'many',\n",
       "              'machine',\n",
       "              'put',\n",
       "              'work',\n",
       "              'build',\n",
       "              'test',\n",
       "              'consult',\n",
       "              'figure',\n",
       "              'pursue',\n",
       "              'drop',\n",
       "              'be',\n",
       "              'within',\n",
       "              'Hilary',\n",
       "              'precompute',\n",
       "              'get',\n",
       "              'incorporate',\n",
       "              'look',\n",
       "              'see',\n",
       "              'Hadoop',\n",
       "              'develop',\n",
       "              'develop',\n",
       "              'classify',\n",
       "              'Mike',\n",
       "              \"'\",\n",
       "              'the',\n",
       "              'notice',\n",
       "              'know',\n",
       "              'understanding',\n",
       "              'someone',\n",
       "              'providing',\n",
       "              'end',\n",
       "              'generate',\n",
       "              'each',\n",
       "              'Martin',\n",
       "              'data',\n",
       "              'find',\n",
       "              'get',\n",
       "              'create',\n",
       "              'look',\n",
       "              'other',\n",
       "              'verify',\n",
       "              'mathematics',\n",
       "              'art',\n",
       "              'other',\n",
       "              'DJ',\n",
       "              'be',\n",
       "              'think',\n",
       "              'make',\n",
       "              'break',\n",
       "              'turn',\n",
       "              'do',\n",
       "              'Cornell',\n",
       "              'join',\n",
       "              'looking',\n",
       "              'solve',\n",
       "              'solve',\n",
       "              \"'\",\n",
       "              'the',\n",
       "              'investigate',\n",
       "              'build',\n",
       "              'build',\n",
       "              'find',\n",
       "              'being',\n",
       "              'build',\n",
       "              'explore',\n",
       "              'iterate',\n",
       "              'drawing',\n",
       "              'come',\n",
       "              'view',\n",
       "              'work',\n",
       "              'the',\n",
       "              'collect',\n",
       "              'others',\n",
       "              'take',\n",
       "              'be',\n",
       "              'understand',\n",
       "              'process',\n",
       "              'extract',\n",
       "              'visualize',\n",
       "              'communicate',\n",
       "              'be',\n",
       "              'the',\n",
       "              'ignore'],\n",
       "             'Hal': ['Varian', \"Varian'\"],\n",
       "             'Varian': ['statistics'],\n",
       "             'statistics': ['is',\n",
       "              'and',\n",
       "              'where',\n",
       "              'is',\n",
       "              'building',\n",
       "              'is',\n",
       "              'to',\n",
       "              'work'],\n",
       "             'is': ['the',\n",
       "              'Web',\n",
       "              'the',\n",
       "              'full',\n",
       "              'a',\n",
       "              'fundamentally',\n",
       "              'a',\n",
       "              'a',\n",
       "              'that',\n",
       "              'search',\n",
       "              'how',\n",
       "              'that',\n",
       "              'a',\n",
       "              'everywhere',\n",
       "              'the',\n",
       "              'available',\n",
       "              'all',\n",
       "              \"'\",\n",
       "              'frequently',\n",
       "              'missing',\n",
       "              'incongruous',\n",
       "              'wrong',\n",
       "              'telling',\n",
       "              'frequently',\n",
       "              'showing',\n",
       "              'never',\n",
       "              'really',\n",
       "              'certainly',\n",
       "              'when',\n",
       "              'difficult',\n",
       "              'crucial',\n",
       "              'deceptive',\n",
       "              'comparative',\n",
       "              'very',\n",
       "              'only',\n",
       "              'only',\n",
       "              'basically',\n",
       "              'divided',\n",
       "              'that',\n",
       "              'the',\n",
       "              'it',\n",
       "              'essentially',\n",
       "              'an',\n",
       "              'another',\n",
       "              'a',\n",
       "              'one',\n",
       "              'highly',\n",
       "              'a',\n",
       "              'also',\n",
       "              'an',\n",
       "              'the',\n",
       "              'crucial',\n",
       "              'significant',\n",
       "              'an',\n",
       "              'an',\n",
       "              'it',\n",
       "              'certainly',\n",
       "              'that',\n",
       "              'the',\n",
       "              'crucial',\n",
       "              'key',\n",
       "              'try',\n",
       "              'also',\n",
       "              'very',\n",
       "              'the',\n",
       "              'a',\n",
       "              'this',\n",
       "              'one',\n",
       "              'telling',\n",
       "              'the',\n",
       "              'a',\n",
       "              'a',\n",
       "              'trivially',\n",
       "              'another',\n",
       "              'really',\n",
       "              'generating',\n",
       "              'trying',\n",
       "              'indeed'],\n",
       "             'the': ['next',\n",
       "              'next',\n",
       "              'many',\n",
       "              'technologies',\n",
       "              'companies',\n",
       "              'unique',\n",
       "              \"O'Reilly\",\n",
       "              'fly',\n",
       "              'data',\n",
       "              'creation',\n",
       "              'earlier',\n",
       "              'Web',\n",
       "              'CDDB',\n",
       "              'exact',\n",
       "              'CD',\n",
       "              'length',\n",
       "              'track',\n",
       "              'database',\n",
       "              'only',\n",
       "              \"customers'\",\n",
       "              'data',\n",
       "              'site',\n",
       "              'users',\n",
       "              'products',\n",
       "              'beginning',\n",
       "              'last',\n",
       "              'amount',\n",
       "              'problem',\n",
       "              'data',\n",
       "              'county',\n",
       "              'addresses',\n",
       "              'geographical',\n",
       "              'foreclosures',\n",
       "              'data',\n",
       "              'wild',\n",
       "              'data',\n",
       "              'publishing',\n",
       "              'data',\n",
       "              'direct',\n",
       "              'ones',\n",
       "              'early',\n",
       "              'reduction',\n",
       "              'first',\n",
       "              'increase',\n",
       "              'space',\n",
       "              'more',\n",
       "              'web',\n",
       "              'analysis',\n",
       "              'foundation',\n",
       "              'metadata',\n",
       "              'Philadelphia',\n",
       "              'HTML',\n",
       "              'dirty',\n",
       "              'job',\n",
       "              'data',\n",
       "              'quality',\n",
       "              'missing',\n",
       "              'incongruous',\n",
       "              'discovery',\n",
       "              'data',\n",
       "              'problem',\n",
       "              'data',\n",
       "              'problem',\n",
       "              'data',\n",
       "              'trick',\n",
       "              'growing',\n",
       "              'grammatical',\n",
       "              'English',\n",
       "              'Cassandra',\n",
       "              'Python',\n",
       "              'problem',\n",
       "              'Natural',\n",
       "              'classification',\n",
       "              'set',\n",
       "              'word',\n",
       "              'size',\n",
       "              'data',\n",
       "              'problem',\n",
       "              'data',\n",
       "              'most',\n",
       "              'understanding',\n",
       "              'data',\n",
       "              'organizations',\n",
       "              'relational',\n",
       "              'data',\n",
       "              'kind',\n",
       "              'difference',\n",
       "              'logical',\n",
       "              'MapReduce',\n",
       "              \"'\",\n",
       "              'intermediate',\n",
       "              'results',\n",
       "              'Hadoop',\n",
       "              \"world'\",\n",
       "              'key',\n",
       "              'time',\n",
       "              'key',\n",
       "              'performance',\n",
       "              'HBase',\n",
       "              'right',\n",
       "              'number',\n",
       "              'calculation',\n",
       "              'experiments',\n",
       "              'data',\n",
       "              'most',\n",
       "              'OpenCV',\n",
       "              'toolbox',\n",
       "              'application',\n",
       "              \"'\",\n",
       "              'joke',\n",
       "              'point',\n",
       "              'existence',\n",
       "              'conclusions',\n",
       "              'data',\n",
       "              'open',\n",
       "              'numbers',\n",
       "              'stories',\n",
       "              'classic',\n",
       "              'data',\n",
       "              'first',\n",
       "              'data',\n",
       "              'state',\n",
       "              'art',\n",
       "              'visualizations',\n",
       "              'growth',\n",
       "              'aesthetics',\n",
       "              'visualization',\n",
       "              'spread',\n",
       "              'spread',\n",
       "              'data',\n",
       "              'tools',\n",
       "              'data',\n",
       "              'kind',\n",
       "              'data',\n",
       "              'first',\n",
       "              'results',\n",
       "              'organization',\n",
       "              'people',\n",
       "              'best',\n",
       "              'most',\n",
       "              'data',\n",
       "              'big',\n",
       "              'big',\n",
       "              'data',\n",
       "              'story',\n",
       "              'data',\n",
       "              'process',\n",
       "              'group',\n",
       "              'process',\n",
       "              'Cornell',\n",
       "              'heart',\n",
       "              'CDDB',\n",
       "              'same',\n",
       "              'puzzle',\n",
       "              'era',\n",
       "              'winners',\n",
       "              'people',\n",
       "              'companies',\n",
       "              'same',\n",
       "              'data',\n",
       "              'nascent',\n",
       "              '2012',\n",
       "              'willingness',\n",
       "              'ability',\n",
       "              'ability',\n",
       "              'box',\n",
       "              'problem',\n",
       "              'companies',\n",
       "              'core',\n",
       "              'vanguard',\n",
       "              'shared',\n",
       "              'URLs',\n",
       "              'next',\n",
       "              'next',\n",
       "              'new',\n",
       "              'low',\n",
       "              '70s'],\n",
       "             'next': ['sexy', 'Intel', \"week'\", 'generation', 'decades'],\n",
       "             'sexy': ['job'],\n",
       "             'job': ['.',\n",
       "              'data',\n",
       "              'but',\n",
       "              'listings',\n",
       "              'postings',\n",
       "              'posting',\n",
       "              'listings',\n",
       "              'only',\n",
       "              'as'],\n",
       "             '.': ['Five',\n",
       "              '0',\n",
       "              \"'\",\n",
       "              'Join',\n",
       "              'Get',\n",
       "              'The',\n",
       "              \"'\",\n",
       "              \"There'\",\n",
       "              'But',\n",
       "              \"'\",\n",
       "              \"It'\",\n",
       "              'Data',\n",
       "              'One',\n",
       "              'The',\n",
       "              'Gracenote',\n",
       "              'If',\n",
       "              'Before',\n",
       "              'If',\n",
       "              'While',\n",
       "              'Their',\n",
       "              'CDDB',\n",
       "              'Google',\n",
       "              \"Here'\",\n",
       "              'Facebook',\n",
       "              'Amazon',\n",
       "              'These',\n",
       "              'They',\n",
       "              'The',\n",
       "              'Whether',\n",
       "              \"That'\",\n",
       "              'In',\n",
       "              'Whether',\n",
       "              'And',\n",
       "              \"It'\",\n",
       "              \"'\",\n",
       "              'The',\n",
       "              'Using',\n",
       "              'What',\n",
       "              \"We'\",\n",
       "              'To',\n",
       "              'Data',\n",
       "              'While',\n",
       "              'At',\n",
       "              'Sites',\n",
       "              'Factual',\n",
       "              'Much',\n",
       "              '0',\n",
       "              'The',\n",
       "              'Mobile',\n",
       "              'Point',\n",
       "              'All',\n",
       "              'Since',\n",
       "              '6',\n",
       "              'But',\n",
       "              'RAM',\n",
       "              'Hitachi',\n",
       "              'Whether',\n",
       "              'The',\n",
       "              'Data',\n",
       "              'The',\n",
       "              'The',\n",
       "              'Increased',\n",
       "              \"That'\",\n",
       "              'So',\n",
       "              'We',\n",
       "              'But',\n",
       "              'Many',\n",
       "              'They',\n",
       "              'The',\n",
       "              'This',\n",
       "              'If',\n",
       "              'Data',\n",
       "              \"You'\",\n",
       "              'It',\n",
       "              'To',\n",
       "              'Scripting',\n",
       "              'Once',\n",
       "              'Data',\n",
       "              'If',\n",
       "              'If',\n",
       "              'In',\n",
       "              \"It'\",\n",
       "              'If',\n",
       "              'Roger',\n",
       "              'While',\n",
       "              'To',\n",
       "              'And',\n",
       "              'Try',\n",
       "              'Google',\n",
       "              'Disambiguation',\n",
       "              'When',\n",
       "              \"That'\",\n",
       "              'If',\n",
       "              'For',\n",
       "              '01',\n",
       "              'If',\n",
       "              '01',\n",
       "              \"We'\",\n",
       "              'Oil',\n",
       "              'And',\n",
       "              \"'\",\n",
       "              \"We'\",\n",
       "              'At',\n",
       "              'What',\n",
       "              'Information',\n",
       "              'They',\n",
       "              'They',\n",
       "              'Most',\n",
       "              'Traditional',\n",
       "              'Managing',\n",
       "              'The',\n",
       "              'Relational',\n",
       "              'While',\n",
       "              'Do',\n",
       "              'Most',\n",
       "              '92',\n",
       "              '93',\n",
       "              'To',\n",
       "              'These',\n",
       "              'They',\n",
       "              'Many',\n",
       "              'While',\n",
       "              'Data',\n",
       "              'Google',\n",
       "              'In',\n",
       "              'In',\n",
       "              \"It'\",\n",
       "              \"What'\",\n",
       "              'The',\n",
       "              \"Yahoo'\",\n",
       "              'Many',\n",
       "              \"Amazon'\",\n",
       "              'You',\n",
       "              'Hadoop',\n",
       "              'It',\n",
       "              'If',\n",
       "              'Hadoop',\n",
       "              'In',\n",
       "              'Traditional',\n",
       "              'If',\n",
       "              'But',\n",
       "              'Faster',\n",
       "              \"It'\",\n",
       "              'Hadoop',\n",
       "              'Hadoop',\n",
       "              'Near',\n",
       "              'These',\n",
       "              'As',\n",
       "              'According',\n",
       "              'ly',\n",
       "              'Machine',\n",
       "              'We',\n",
       "              'You',\n",
       "              'Andrew',\n",
       "              'There',\n",
       "              'Google',\n",
       "              'For',\n",
       "              'Mechanical',\n",
       "              'Machine',\n",
       "              'The',\n",
       "              'Once',\n",
       "              \"It'\",\n",
       "              'Even',\n",
       "              'While',\n",
       "              'According',\n",
       "              \"'\",\n",
       "              \"'\",\n",
       "              'That',\n",
       "              'More',\n",
       "              'But',\n",
       "              'Data',\n",
       "              'Statistics',\n",
       "              'Statistics',\n",
       "              'It',\n",
       "              'While',\n",
       "              'Although',\n",
       "              'It',\n",
       "              'If',\n",
       "              'A',\n",
       "              'The',\n",
       "              'To',\n",
       "              'Edward',\n",
       "              'But',\n",
       "              'Visualization',\n",
       "              'According',\n",
       "              'Visualization',\n",
       "              'Hilary',\n",
       "              'Once',\n",
       "              'There',\n",
       "              'GnuPlot',\n",
       "              'At',\n",
       "              'Nathan',\n",
       "              'One',\n",
       "              'And',\n",
       "              'Does',\n",
       "              'Does',\n",
       "              'There',\n",
       "              \"It'\",\n",
       "              'Data',\n",
       "              'Describing',\n",
       "              'Physicists',\n",
       "              'They',\n",
       "              'When',\n",
       "              'You',\n",
       "              'You',\n",
       "              'Scientists',\n",
       "              'Patil',\n",
       "              'It',\n",
       "              'But',\n",
       "              'Asking',\n",
       "              'It',\n",
       "              'In',\n",
       "              'Then',\n",
       "              'The',\n",
       "              'It',\n",
       "              'It',\n",
       "              'This',\n",
       "              'CDDB',\n",
       "              'But',\n",
       "              'Computing',\n",
       "              'Entrepreneurship',\n",
       "              \"Patil'\",\n",
       "              \"'\",\n",
       "              'We',\n",
       "              'Hilary',\n",
       "              'Her',\n",
       "              'ly',\n",
       "              'ly',\n",
       "              'No',\n",
       "              'In',\n",
       "              'Data',\n",
       "              'They',\n",
       "              'They',\n",
       "              'They',\n",
       "              'Google',\n",
       "              'They',\n",
       "              'ly',\n",
       "              'Whether',\n",
       "              'The',\n",
       "              'Data',\n",
       "              '1',\n",
       "              \"'\",\n",
       "              '2'],\n",
       "             'Five': ['years'],\n",
       "             'years': ['ago', 'there', 'ago'],\n",
       "             'ago': ['in', '.'],\n",
       "             'in': ['What',\n",
       "              'samples',\n",
       "              'the',\n",
       "              'doing',\n",
       "              'a',\n",
       "              'which',\n",
       "              'the',\n",
       "              \"R'\",\n",
       "              'Philadelphia',\n",
       "              'business',\n",
       "              'the',\n",
       "              'a',\n",
       "              'the',\n",
       "              '.',\n",
       "              'word',\n",
       "              'storage',\n",
       "              'size',\n",
       "              'speed',\n",
       "              '1982',\n",
       "              'at',\n",
       "              'your',\n",
       "              'the',\n",
       "              'formats',\n",
       "              'formats',\n",
       "              'place',\n",
       "              \"'\",\n",
       "              \"R'\",\n",
       "              'English',\n",
       "              'different',\n",
       "              'the',\n",
       "              '.',\n",
       "              'advance',\n",
       "              'which',\n",
       "              'most',\n",
       "              'racks',\n",
       "              'enabling',\n",
       "              'near',\n",
       "              'real',\n",
       "              'computer',\n",
       "              'Python',\n",
       "              'Java',\n",
       "              'any',\n",
       "              'a',\n",
       "              'everything',\n",
       "              'computer',\n",
       "              'analysis',\n",
       "              'not',\n",
       "              'proprietary',\n",
       "              'Python',\n",
       "              'Hadoop',\n",
       "              'which',\n",
       "              'their',\n",
       "              'a',\n",
       "              'the',\n",
       "              'the',\n",
       "              '1984',\n",
       "              'Beautiful',\n",
       "              'Beautiful'],\n",
       "             'What': ['is', 'differentiates', 'are'],\n",
       "             'Web': ['2', 'was', '2'],\n",
       "             '2': ['.', '.', 'hackingdata', 'percent', \"'\"],\n",
       "             '0': ['Tim', 'and', '.', '.'],\n",
       "             'Tim': [\"O'\"],\n",
       "             \"O'\": ['Reilly', 'Reilly', 'Reilly'],\n",
       "             'Reilly': ['said', 'we', 'was'],\n",
       "             'said': ['that', \"'\"],\n",
       "             'that': [\"'\",\n",
       "              'statement',\n",
       "              'talks',\n",
       "              'any',\n",
       "              'knows',\n",
       "              'help',\n",
       "              'a',\n",
       "              'can',\n",
       "              'can',\n",
       "              'ties',\n",
       "              'data',\n",
       "              'data',\n",
       "              'wants',\n",
       "              'data',\n",
       "              'story',\n",
       "              'almost',\n",
       "              'data',\n",
       "              'data',\n",
       "              'are',\n",
       "              'was',\n",
       "              'something',\n",
       "              'the',\n",
       "              'the',\n",
       "              'were',\n",
       "              'sounds',\n",
       "              'problem',\n",
       "              'are',\n",
       "              'have',\n",
       "              'can',\n",
       "              'allure',\n",
       "              'MapReduce',\n",
       "              'they',\n",
       "              'can',\n",
       "              \"you'\",\n",
       "              'enables',\n",
       "              \"person'\",\n",
       "              'eating',\n",
       "              'one',\n",
       "              'data',\n",
       "              'the',\n",
       "              'provides',\n",
       "              'they',\n",
       "              'when',\n",
       "              'show',\n",
       "              'give',\n",
       "              'would',\n",
       "              'looked',\n",
       "              'members',\n",
       "              'analyzed',\n",
       "              'built',\n",
       "              'appears',\n",
       "              'gave',\n",
       "              'signature',\n",
       "              'are',\n",
       "              'the',\n",
       "              'find',\n",
       "              'bit',\n",
       "              'the',\n",
       "              'people',\n",
       "              'nobody',\n",
       "              'in',\n",
       "              'the',\n",
       "              'data'],\n",
       "             \"'\": ['data',\n",
       "              'But',\n",
       "              'the',\n",
       "              'data',\n",
       "              'Almost',\n",
       "              'data',\n",
       "              'A',\n",
       "              'data',\n",
       "              'data',\n",
       "              'data',\n",
       "              'citizen',\n",
       "              'data',\n",
       "              'Data',\n",
       "              'not',\n",
       "              '80s',\n",
       "              'an',\n",
       "              'a',\n",
       "              'data',\n",
       "              'or',\n",
       "              'wild',\n",
       "              'Data',\n",
       "              \"better'\",\n",
       "              \"Apple'\",\n",
       "              'Apple',\n",
       "              'paying',\n",
       "              'big',\n",
       "              'but',\n",
       "              \"big'\",\n",
       "              \"big'\",\n",
       "              \"medium'\",\n",
       "              'small',\n",
       "              'The',\n",
       "              'big',\n",
       "              'eventual',\n",
       "              \"map'\",\n",
       "              \"agile'\",\n",
       "              'agile',\n",
       "              'trending',\n",
       "              'or',\n",
       "              'an',\n",
       "              'training',\n",
       "              'or',\n",
       "              'possibly',\n",
       "              'grammar',\n",
       "              'It',\n",
       "              'making',\n",
       "              \"We'\",\n",
       "              'and',\n",
       "              'is',\n",
       "              'one',\n",
       "              \"art'\",\n",
       "              'on',\n",
       "              'hard',\n",
       "              'particularly',\n",
       "              'but',\n",
       "              'data',\n",
       "              'using',\n",
       "              'see',\n",
       "              'what',\n",
       "              'was',\n",
       "              'someone',\n",
       "              \"That'\",\n",
       "              \"here'\",\n",
       "              'The',\n",
       "              'to',\n",
       "              \"that'\",\n",
       "              'real',\n",
       "              'Whether',\n",
       "              'Information',\n",
       "              'by',\n",
       "              'Information',\n",
       "              'by'],\n",
       "             'data': ['is',\n",
       "              'In',\n",
       "              'science',\n",
       "              'driven',\n",
       "              'driven',\n",
       "              'services',\n",
       "              \"isn'\",\n",
       "              'science',\n",
       "              'application',\n",
       "              'itself',\n",
       "              'as',\n",
       "              \"it'\",\n",
       "              'product',\n",
       "              'products',\n",
       "              'products',\n",
       "              'not',\n",
       "              \"products'\",\n",
       "              'problem',\n",
       "              'products',\n",
       "              '.',\n",
       "              \"products'\",\n",
       "              \"exhaust'\",\n",
       "              'that',\n",
       "              'they',\n",
       "              'collected',\n",
       "              'is',\n",
       "              'science',\n",
       "              \"that'\",\n",
       "              'from',\n",
       "              'or',\n",
       "              \"it'\",\n",
       "              'or',\n",
       "              'contributed',\n",
       "              'from',\n",
       "              'to',\n",
       "              'source',\n",
       "              'effectively',\n",
       "              'but',\n",
       "              \"that'\",\n",
       "              'effectively',\n",
       "              'science',\n",
       "              'science',\n",
       "              'in',\n",
       "              'scientists',\n",
       "              'massaging',\n",
       "              'lifecycle',\n",
       "              \"we'\",\n",
       "              'from',\n",
       "              'publicly',\n",
       "              'and',\n",
       "              'to',\n",
       "              'MySpace',\n",
       "              'we',\n",
       "              '.',\n",
       "              'wherever',\n",
       "              'trail',\n",
       "              'would',\n",
       "              \"isn'\",\n",
       "              'you',\n",
       "              'exhaust',\n",
       "              '.',\n",
       "              'science',\n",
       "              'useful',\n",
       "              'analysis',\n",
       "              'conditioning',\n",
       "              'into',\n",
       "              'in',\n",
       "              'feeds',\n",
       "              'in',\n",
       "              'used',\n",
       "              'was',\n",
       "              'sources',\n",
       "              'conditioning',\n",
       "              'you',\n",
       "              '.',\n",
       "              'is',\n",
       "              'is',\n",
       "              'after',\n",
       "              'is',\n",
       "              'collection',\n",
       "              'science',\n",
       "              'and',\n",
       "              'at',\n",
       "              'adds',\n",
       "              'analysis',\n",
       "              \"'\",\n",
       "              'centric',\n",
       "              'itself',\n",
       "              'problems',\n",
       "              '.',\n",
       "              'run',\n",
       "              \"that'\",\n",
       "              'warehouses',\n",
       "              'rather',\n",
       "              'formats',\n",
       "              'changes',\n",
       "              'platforms',\n",
       "              'sources',\n",
       "              '.',\n",
       "              'driven',\n",
       "              'analysis',\n",
       "              'is',\n",
       "              'platform',\n",
       "              'problems',\n",
       "              'platform',\n",
       "              'analysis',\n",
       "              'analysis',\n",
       "              'as',\n",
       "              'analysis',\n",
       "              'scientist',\n",
       "              'scientist',\n",
       "              'with',\n",
       "              'perhaps',\n",
       "              'points',\n",
       "              'analysis',\n",
       "              'science',\n",
       "              'speak',\n",
       "              'or',\n",
       "              'might',\n",
       "              'are',\n",
       "              'and',\n",
       "              'analysis',\n",
       "              'visualization',\n",
       "              'science',\n",
       "              'scientist',\n",
       "              'conditioning',\n",
       "              'is',\n",
       "              'set',\n",
       "              'might',\n",
       "              '.',\n",
       "              'tell',\n",
       "              'sources',\n",
       "              'was',\n",
       "              'were',\n",
       "              'science',\n",
       "              'science',\n",
       "              'samples',\n",
       "              'intensive',\n",
       "              'scientists',\n",
       "              '.',\n",
       "              'you',\n",
       "              'out',\n",
       "              'is',\n",
       "              'scientists',\n",
       "              'product',\n",
       "              'all',\n",
       "              \"jiujitsu'\",\n",
       "              'jiujitsu',\n",
       "              'creatively',\n",
       "              'scientist',\n",
       "              '.',\n",
       "              'that',\n",
       "              'industry',\n",
       "              'products',\n",
       "              'collection',\n",
       "              'conditioning',\n",
       "              'what',\n",
       "              'successfully',\n",
       "              '.',\n",
       "              \"'\",\n",
       "              'it',\n",
       "              'was'],\n",
       "             'Intel': ['Inside', 'Inside'],\n",
       "             'Inside': ['.', '.'],\n",
       "             'But': ['what',\n",
       "              'merely',\n",
       "              \"we'\",\n",
       "              'old',\n",
       "              'Hadoop',\n",
       "              'it',\n",
       "              \"that'\",\n",
       "              'the',\n",
       "              'the'],\n",
       "             'what': ['does',\n",
       "              'we',\n",
       "              'you',\n",
       "              'other',\n",
       "              'to',\n",
       "              'skills',\n",
       "              'you',\n",
       "              'they',\n",
       "              'correlation',\n",
       "              'that',\n",
       "              'the',\n",
       "              'concerns',\n",
       "              'might',\n",
       "              'the',\n",
       "              'you',\n",
       "              'Patil',\n",
       "              'kind',\n",
       "              'those',\n",
       "              'can'],\n",
       "             'does': ['that', 'anything', 'that'],\n",
       "             'statement': ['mean'],\n",
       "             'mean': ['Why', 'by', \"it'\", 'the'],\n",
       "             'Why': ['do'],\n",
       "             'do': ['we',\n",
       "              'with',\n",
       "              'we',\n",
       "              'the',\n",
       "              'the',\n",
       "              'data',\n",
       "              'you',\n",
       "              'you',\n",
       "              'it',\n",
       "              'the',\n",
       "              'with',\n",
       "              'something',\n",
       "              'you',\n",
       "              'massive',\n",
       "              'know'],\n",
       "             'we': ['suddenly',\n",
       "              'mean',\n",
       "              \"aren'\",\n",
       "              'frequently',\n",
       "              'currently',\n",
       "              \"couldn'\",\n",
       "              'make',\n",
       "              'trying',\n",
       "              'could',\n",
       "              'now',\n",
       "              'do'],\n",
       "             'suddenly': ['care'],\n",
       "             'care': ['about', 'if'],\n",
       "             'about': ['statistics',\n",
       "              'data',\n",
       "              'because',\n",
       "              'web',\n",
       "              '40000',\n",
       "              'half',\n",
       "              'the',\n",
       "              'large',\n",
       "              \"'\",\n",
       "              'the',\n",
       "              'the',\n",
       "              'what',\n",
       "              'testing',\n",
       "              'the'],\n",
       "             'and': ['about',\n",
       "              'the',\n",
       "              'find',\n",
       "              'useful',\n",
       "              'middleware',\n",
       "              'data',\n",
       "              'so',\n",
       "              'creates',\n",
       "              'coupled',\n",
       "              'gets',\n",
       "              'creates',\n",
       "              'LinkedIn',\n",
       "              'uses',\n",
       "              'a',\n",
       "              'put',\n",
       "              'a',\n",
       "              'using',\n",
       "              'longitude',\n",
       "              'group',\n",
       "              'other',\n",
       "              'relevant',\n",
       "              'data',\n",
       "              'presenting',\n",
       "              'where',\n",
       "              'even',\n",
       "              'Factual',\n",
       "              'game',\n",
       "              'improve',\n",
       "              'of',\n",
       "              'leaving',\n",
       "              'frequent',\n",
       "              \"that'\",\n",
       "              'number',\n",
       "              'increase',\n",
       "              'a',\n",
       "              'analyzed',\n",
       "              'use',\n",
       "              'other',\n",
       "              \"isn'\",\n",
       "              'other',\n",
       "              'be',\n",
       "              'machine',\n",
       "              'Python',\n",
       "              'you',\n",
       "              'more',\n",
       "              \"you'\",\n",
       "              'want',\n",
       "              'other',\n",
       "              'next',\n",
       "              'are',\n",
       "              'understanding',\n",
       "              'reporting',\n",
       "              'their',\n",
       "              'replication',\n",
       "              'slow',\n",
       "              '5',\n",
       "              \"Amazon'\",\n",
       "              'are',\n",
       "              'to',\n",
       "              'enormous',\n",
       "              'conquer',\n",
       "              'then',\n",
       "              'de',\n",
       "              'reliability',\n",
       "              'other',\n",
       "              'consumers',\n",
       "              'testing',\n",
       "              'particularly',\n",
       "              'different',\n",
       "              \"it'\",\n",
       "              'delivers',\n",
       "              'mobile',\n",
       "              'building',\n",
       "              'SnapTell',\n",
       "              'even',\n",
       "              'look',\n",
       "              'Mahout',\n",
       "              'tune',\n",
       "              'making',\n",
       "              'other',\n",
       "              'its',\n",
       "              'quirky',\n",
       "              'newer',\n",
       "              'a',\n",
       "              'presenting',\n",
       "              'Ben',\n",
       "              'if',\n",
       "              'the',\n",
       "              'implement',\n",
       "              'come',\n",
       "              'made',\n",
       "              'added',\n",
       "              'then',\n",
       "              'the',\n",
       "              'find',\n",
       "              'artists',\n",
       "              'the',\n",
       "              'data',\n",
       "              'use',\n",
       "              'LinkedIn',\n",
       "              'made'],\n",
       "             'In': ['this',\n",
       "              'the',\n",
       "              'data',\n",
       "              'the',\n",
       "              'hindsight',\n",
       "              'software',\n",
       "              'addition',\n",
       "              'addition'],\n",
       "             'this': ['post',\n",
       "              'database',\n",
       "              'sounds',\n",
       "              'data',\n",
       "              'scale',\n",
       "              'video',\n",
       "              'difference',\n",
       "              'animation',\n",
       "              'is',\n",
       "              'versatile',\n",
       "              'into',\n",
       "              'but'],\n",
       "             'post': ['I'],\n",
       "             'I': ['examine', \"haven'\"],\n",
       "             'examine': ['the'],\n",
       "             'many': ['sides',\n",
       "              'large',\n",
       "              'of',\n",
       "              'job',\n",
       "              'many',\n",
       "              'websites',\n",
       "              'applications',\n",
       "              'nodes',\n",
       "              'processors',\n",
       "              'large',\n",
       "              'modern',\n",
       "              'libraries',\n",
       "              'commercial',\n",
       "              'kinds',\n",
       "              'packages',\n",
       "              'of'],\n",
       "             'sides': ['of'],\n",
       "             'of': ['data',\n",
       "              \"'\",\n",
       "              'other',\n",
       "              'data',\n",
       "              'the',\n",
       "              'CDDB',\n",
       "              'each',\n",
       "              'track',\n",
       "              'album',\n",
       "              'this',\n",
       "              'every',\n",
       "              'friendship',\n",
       "              \"'\",\n",
       "              'data',\n",
       "              'these',\n",
       "              'data',\n",
       "              'data',\n",
       "              'sources',\n",
       "              'analysis',\n",
       "              'data',\n",
       "              'the',\n",
       "              'Web',\n",
       "              \"Moore'\",\n",
       "              'data',\n",
       "              'them',\n",
       "              'which',\n",
       "              'sale',\n",
       "              'your',\n",
       "              'this',\n",
       "              '360',\n",
       "              'cores',\n",
       "              'about',\n",
       "              'the',\n",
       "              'CPU',\n",
       "              \"Moore'\",\n",
       "              'that',\n",
       "              'data',\n",
       "              'any',\n",
       "              \"'\",\n",
       "              'data',\n",
       "              'tools',\n",
       "              'your',\n",
       "              'ozone',\n",
       "              'a',\n",
       "              'the',\n",
       "              'subtasks',\n",
       "              'the',\n",
       "              'the',\n",
       "              'data',\n",
       "              'steam',\n",
       "              'the',\n",
       "              'the',\n",
       "              'database',\n",
       "              'multiple',\n",
       "              'a',\n",
       "              'operations',\n",
       "              'analysis',\n",
       "              'finance',\n",
       "              'databases',\n",
       "              'these',\n",
       "              \"Google'\",\n",
       "              'them',\n",
       "              'building',\n",
       "              'identical',\n",
       "              'processors',\n",
       "              'answers',\n",
       "              'MapReduce',\n",
       "              'the',\n",
       "              'Linux',\n",
       "              'which',\n",
       "              'a',\n",
       "              'huge',\n",
       "              'time',\n",
       "              'followers',\n",
       "              'the',\n",
       "              'the',\n",
       "              'the',\n",
       "              'students',\n",
       "              'the',\n",
       "              'known',\n",
       "              'public',\n",
       "              'a',\n",
       "              'data',\n",
       "              'data',\n",
       "              'data',\n",
       "              'numbers',\n",
       "              'Quantitative',\n",
       "              'the',\n",
       "              'Flowing',\n",
       "              'what',\n",
       "              'the',\n",
       "              'the',\n",
       "              'my',\n",
       "              'the',\n",
       "              'Walmart',\n",
       "              'the',\n",
       "              'cancer',\n",
       "              'a',\n",
       "              'presenting',\n",
       "              'question',\n",
       "              'our',\n",
       "              'the',\n",
       "              'grant',\n",
       "              'creating',\n",
       "              'hours',\n",
       "              'developer',\n",
       "              'hours',\n",
       "              'computing',\n",
       "              'data',\n",
       "              'what',\n",
       "              'data',\n",
       "              'the',\n",
       "              'person',\n",
       "              'products',\n",
       "              'a',\n",
       "              'data',\n",
       "              'their',\n",
       "              'millions',\n",
       "              'travellers',\n",
       "              'successful',\n",
       "              'Hal'],\n",
       "             'science': [\"'\",\n",
       "              '.',\n",
       "              'enables',\n",
       "              '.',\n",
       "              \"'\",\n",
       "              'from',\n",
       "              'is',\n",
       "              '.',\n",
       "              'what',\n",
       "              'at',\n",
       "              '.',\n",
       "              \"isn'\",\n",
       "              'it',\n",
       "              '.',\n",
       "              'requires',\n",
       "              'to',\n",
       "              'group',\n",
       "              'group',\n",
       "              'majors'],\n",
       "             'technologies': ['the', 'provide'],\n",
       "             'companies': ['and',\n",
       "              'banks',\n",
       "              'using',\n",
       "              'telecommunications',\n",
       "              'and',\n",
       "              'that',\n",
       "              'who',\n",
       "              'like'],\n",
       "             'unique': ['skill', 'signature'],\n",
       "             'skill': ['sets', '.', 'in'],\n",
       "             'sets': ['.', '.'],\n",
       "             'Join': ['the'],\n",
       "             \"O'Reilly\": ['online'],\n",
       "             'online': ['learning', 'transaction', 'and', '.', '.'],\n",
       "             'learning': ['platform',\n",
       "              'libraries',\n",
       "              '.',\n",
       "              'is',\n",
       "              'PyBrain',\n",
       "              'algorithms',\n",
       "              'almost',\n",
       "              'and'],\n",
       "             'platform': ['.', 'though', '.', 'Hadoop'],\n",
       "             'Get': ['a'],\n",
       "             'a': ['free',\n",
       "              'data',\n",
       "              'database',\n",
       "              'web',\n",
       "              'number',\n",
       "              'result',\n",
       "              'data',\n",
       "              'unique',\n",
       "              'database',\n",
       "              'database',\n",
       "              'CD',\n",
       "              'CD',\n",
       "              'CD',\n",
       "              'musical',\n",
       "              'data',\n",
       "              'master',\n",
       "              'few',\n",
       "              'book',\n",
       "              'book',\n",
       "              'camera',\n",
       "              'camera',\n",
       "              'customer',\n",
       "              'customer',\n",
       "              'trail',\n",
       "              'camera',\n",
       "              'cloud',\n",
       "              'feedback',\n",
       "              'number',\n",
       "              'public',\n",
       "              'map',\n",
       "              'community',\n",
       "              'holistic',\n",
       "              'tractable',\n",
       "              'sense',\n",
       "              'sea',\n",
       "              'trail',\n",
       "              'price',\n",
       "              '32',\n",
       "              'gram',\n",
       "              'purchase',\n",
       "              'state',\n",
       "              'public',\n",
       "              'spreadsheet',\n",
       "              'standard',\n",
       "              'database',\n",
       "              'simple',\n",
       "              'job',\n",
       "              'sense',\n",
       "              'large',\n",
       "              'lot',\n",
       "              'red',\n",
       "              'long',\n",
       "              'horde',\n",
       "              'schema',\n",
       "              'complex',\n",
       "              'new',\n",
       "              'few',\n",
       "              'data',\n",
       "              'divide',\n",
       "              'programming',\n",
       "              'number',\n",
       "              'single',\n",
       "              'search',\n",
       "              'single',\n",
       "              'home',\n",
       "              'simple',\n",
       "              'data',\n",
       "              'distributed',\n",
       "              'high',\n",
       "              'one',\n",
       "              'calculation',\n",
       "              'batch',\n",
       "              \"'\",\n",
       "              'recommendation',\n",
       "              'quintessential',\n",
       "              'cell',\n",
       "              'RESTful',\n",
       "              'de',\n",
       "              \"'\",\n",
       "              'significant',\n",
       "              'large',\n",
       "              'few',\n",
       "              'cost',\n",
       "              'few',\n",
       "              'relatively',\n",
       "              'few',\n",
       "              'Nutshell',\n",
       "              'random',\n",
       "              'role',\n",
       "              'basic',\n",
       "              'background',\n",
       "              'single',\n",
       "              'thousand',\n",
       "              'picture',\n",
       "              'thousand',\n",
       "              'set',\n",
       "              'graph',\n",
       "              'foundational',\n",
       "              'new',\n",
       "              'dozen',\n",
       "              'sense',\n",
       "              'fairly',\n",
       "              'great',\n",
       "              'body',\n",
       "              'flu',\n",
       "              'population',\n",
       "              'matter',\n",
       "              'successful',\n",
       "              'question',\n",
       "              'few',\n",
       "              'consumer',\n",
       "              'team',\n",
       "              'multistage',\n",
       "              'hypothesis',\n",
       "              'regression',\n",
       "              'strong',\n",
       "              'discipline',\n",
       "              'lot',\n",
       "              'high',\n",
       "              'relatively',\n",
       "              'valuable',\n",
       "              'huge',\n",
       "              'huge',\n",
       "              'large',\n",
       "              'great',\n",
       "              'very',\n",
       "              'much',\n",
       "              'signature',\n",
       "              'database',\n",
       "              'data',\n",
       "              'company',\n",
       "              'solution',\n",
       "              'problem',\n",
       "              'lot',\n",
       "              'hugely'],\n",
       "             'free': ['trial'],\n",
       "             'trial': ['today'],\n",
       "             'today': ['and', 'every'],\n",
       "             'find': ['answers', 'to', 'out', 'the', 'those', 'out', 'new'],\n",
       "             'answers': ['on', '.'],\n",
       "             'on': ['the',\n",
       "              '.',\n",
       "              'the',\n",
       "              'the',\n",
       "              'the',\n",
       "              'a',\n",
       "              'every',\n",
       "              'Facebook',\n",
       "              'a',\n",
       "              'long',\n",
       "              'sites',\n",
       "              'trending',\n",
       "              'Twitter',\n",
       "              'any',\n",
       "              'getting',\n",
       "              'track',\n",
       "              'data'],\n",
       "             'fly': ['or'],\n",
       "             'or': ['master',\n",
       "              'analyzing',\n",
       "              'should',\n",
       "              'product',\n",
       "              'some',\n",
       "              'the',\n",
       "              'has',\n",
       "              'involve',\n",
       "              'audio',\n",
       "              'raw',\n",
       "              'make',\n",
       "              'getting',\n",
       "              'even',\n",
       "              'incongruous',\n",
       "              'that',\n",
       "              'the',\n",
       "              'dataspaces',\n",
       "              '1',\n",
       "              'Non',\n",
       "              'so',\n",
       "              'even',\n",
       "              'even',\n",
       "              'a',\n",
       "              'whatever',\n",
       "              'just',\n",
       "              'making',\n",
       "              'may',\n",
       "              'more',\n",
       "              'service',\n",
       "              'communicate',\n",
       "              'Office',\n",
       "              'to',\n",
       "              'studying',\n",
       "              'software'],\n",
       "             'master': ['something', 'at'],\n",
       "             'something': ['new', 'different', 'is', 'with'],\n",
       "             'new': ['and',\n",
       "              'value',\n",
       "              'breed',\n",
       "              'data',\n",
       "              'insights',\n",
       "              'products',\n",
       "              'ways',\n",
       "              'Intel'],\n",
       "             'useful': ['.', 'The', '.', 'if'],\n",
       "             'The': ['web',\n",
       "              'developers',\n",
       "              'thread',\n",
       "              'question',\n",
       "              'web',\n",
       "              'importance',\n",
       "              'more',\n",
       "              'data',\n",
       "              'first',\n",
       "              'foreclosure',\n",
       "              'most',\n",
       "              'need',\n",
       "              'most',\n",
       "              'Turk',\n",
       "              'problem',\n",
       "              'result',\n",
       "              'future',\n",
       "              'part',\n",
       "              'ability',\n",
       "              'NASA'],\n",
       "             'web': ['is',\n",
       "              'front',\n",
       "              'server',\n",
       "              'server',\n",
       "              'has',\n",
       "              'friend',\n",
       "              'services',\n",
       "              'and',\n",
       "              'applications',\n",
       "              'property'],\n",
       "             'full': ['of', 'fledged'],\n",
       "             'driven': ['apps', 'application', 'applications'],\n",
       "             'apps': ['.'],\n",
       "             'Almost': ['any'],\n",
       "             'any': ['e', 'CD', 'data', 'one', 'data', 'given'],\n",
       "             'e': ['commerce'],\n",
       "             'commerce': ['application'],\n",
       "             'application': ['is',\n",
       "              '.',\n",
       "              'acquires',\n",
       "              'with',\n",
       "              'with',\n",
       "              'lets',\n",
       "              '.'],\n",
       "             \"There'\": ['s'],\n",
       "             's': ['a',\n",
       "              'not',\n",
       "              'a',\n",
       "              'not',\n",
       "              'revolutionary',\n",
       "              'a',\n",
       "              'more',\n",
       "              'the',\n",
       "              'available',\n",
       "              'figuring',\n",
       "              'not',\n",
       "              'increasingly',\n",
       "              'office',\n",
       "              'available',\n",
       "              'look',\n",
       "              'happening',\n",
       "              'Law',\n",
       "              'cards',\n",
       "              'where',\n",
       "              'Law',\n",
       "              'law',\n",
       "              'the',\n",
       "              'usable',\n",
       "              'directly',\n",
       "              'office',\n",
       "              'generated',\n",
       "              'going',\n",
       "              'reported',\n",
       "              'usually',\n",
       "              'happening',\n",
       "              'where',\n",
       "              'Mechanical',\n",
       "              'marketplace',\n",
       "              \"'\",\n",
       "              \"'\",\n",
       "              \"'\",\n",
       "              'different',\n",
       "              'important',\n",
       "              'not',\n",
       "              'BigTable',\n",
       "              'Dynamo',\n",
       "              'biggest',\n",
       "              'easy',\n",
       "              'less',\n",
       "              'claim',\n",
       "              'largest',\n",
       "              'Elastic',\n",
       "              'the',\n",
       "              'easer',\n",
       "              'possible',\n",
       "              'possible',\n",
       "              'picture',\n",
       "              'identity',\n",
       "              'Machine',\n",
       "              'an',\n",
       "              'easy',\n",
       "              'about',\n",
       "              'ad',\n",
       "              'a',\n",
       "              'Visual',\n",
       "              'not',\n",
       "              'Processing',\n",
       "              'Many',\n",
       "              'FlowingData',\n",
       "              'not',\n",
       "              'the',\n",
       "              'telling',\n",
       "              'membership',\n",
       "              'data',\n",
       "              'first',\n",
       "              'an',\n",
       "              'a',\n",
       "              'mining',\n",
       "              'quote',\n",
       "              'going'],\n",
       "             'database': ['behind',\n",
       "              '.',\n",
       "              'of',\n",
       "              'of',\n",
       "              '.',\n",
       "              'including',\n",
       "              'for',\n",
       "              'or',\n",
       "              'model',\n",
       "              'systems',\n",
       "              'servers',\n",
       "              'Hive',\n",
       "              \"'\",\n",
       "              'is'],\n",
       "             'behind': ['a', 'whenever', 'Google'],\n",
       "             'front': ['end'],\n",
       "             'end': ['and', 'to', 'solution'],\n",
       "             'middleware': ['that'],\n",
       "             'talks': ['to'],\n",
       "             'number': ['of', 'of', 'of', 'of', 'of', 'of'],\n",
       "             'other': ['databases',\n",
       "              'people',\n",
       "              'users',\n",
       "              'source',\n",
       "              'socio',\n",
       "              'newer',\n",
       "              'languages',\n",
       "              'data',\n",
       "              'components',\n",
       "              'disciplines',\n",
       "              'data',\n",
       "              'members'],\n",
       "             'databases': ['and', 'are', 'appear', 'or', 'though', 'are'],\n",
       "             'services': ['credit', 'microformats', 'like'],\n",
       "             'credit': ['card'],\n",
       "             'card': ['processing', 'weighs'],\n",
       "             'processing': ['companies', 'to', 'fails', '.', 'pipeline'],\n",
       "             'banks': ['and'],\n",
       "             'so': ['on', '.', 'products', 'does'],\n",
       "             'merely': ['using'],\n",
       "             'using': ['data',\n",
       "              'their',\n",
       "              'Yahoo',\n",
       "              'the',\n",
       "              'Google',\n",
       "              'SQL',\n",
       "              'photos',\n",
       "              'smaller'],\n",
       "             \"isn'\": ['t',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't',\n",
       "              't'],\n",
       "             't': ['really',\n",
       "              'the',\n",
       "              'just',\n",
       "              'just',\n",
       "              'ust',\n",
       "              'finding',\n",
       "              'drowning',\n",
       "              'store',\n",
       "              'just',\n",
       "              'died',\n",
       "              'going',\n",
       "              'well',\n",
       "              '.',\n",
       "              'always',\n",
       "              'concerned',\n",
       "              '.',\n",
       "              'require',\n",
       "              'have',\n",
       "              'stressed',\n",
       "              'work',\n",
       "              'just',\n",
       "              'superseded',\n",
       "              'just',\n",
       "              'just',\n",
       "              'as',\n",
       "              'what',\n",
       "              'yet'],\n",
       "             'really': ['what',\n",
       "              'a',\n",
       "              'necessary',\n",
       "              'care',\n",
       "              'telling',\n",
       "              'what',\n",
       "              'to'],\n",
       "             'by': [\"'\",\n",
       "              'their',\n",
       "              'taking',\n",
       "              'neighborhood',\n",
       "              'the',\n",
       "              'Excel',\n",
       "              'telling',\n",
       "              'a',\n",
       "              'providing',\n",
       "              'extremely',\n",
       "              'newer',\n",
       "              'making',\n",
       "              'analyzing',\n",
       "              'Jeff',\n",
       "              'Jeff'],\n",
       "             'A': ['data', 'picture'],\n",
       "             'acquires': ['its'],\n",
       "             'its': ['value',\n",
       "              'story',\n",
       "              'datasets',\n",
       "              'own',\n",
       "              'EC2',\n",
       "              'comprehensive',\n",
       "              'story',\n",
       "              'story',\n",
       "              'goal'],\n",
       "             'value': ['from', 'in', '.', 'iteratively', 'from'],\n",
       "             'from': ['the',\n",
       "              'selling',\n",
       "              'viewing',\n",
       "              'users',\n",
       "              'sensors',\n",
       "              'a',\n",
       "              'the',\n",
       "              'traditional',\n",
       "              'statistics',\n",
       "              'how',\n",
       "              'Nielsen',\n",
       "              'sporting',\n",
       "              '10',\n",
       "              '1',\n",
       "              'a',\n",
       "              'ancient',\n",
       "              'many',\n",
       "              'gigabytes',\n",
       "              'search',\n",
       "              'Twitter',\n",
       "              'the',\n",
       "              'traditional',\n",
       "              'machine',\n",
       "              'traditional',\n",
       "              'a',\n",
       "              'the',\n",
       "              'it',\n",
       "              'initial',\n",
       "              'it',\n",
       "              'the',\n",
       "              'it'],\n",
       "             'itself': ['and', 'becomes', 'but'],\n",
       "             'creates': ['more', 'new'],\n",
       "             'more': ['data',\n",
       "              'traditional',\n",
       "              'time',\n",
       "              'than',\n",
       "              'storage',\n",
       "              'data',\n",
       "              'data',\n",
       "              'interesting',\n",
       "              'and',\n",
       "              'frequently',\n",
       "              'conversions',\n",
       "              'scatter',\n",
       "              'detailed',\n",
       "              'tractable'],\n",
       "             'as': ['a',\n",
       "              'data',\n",
       "              'audio',\n",
       "              'a',\n",
       "              'diverse',\n",
       "              'endocrinologists',\n",
       "              'applied',\n",
       "              'an',\n",
       "              'awk',\n",
       "              'Perl',\n",
       "              'storage',\n",
       "              'the',\n",
       "              'needed',\n",
       "              'it',\n",
       "              'clean',\n",
       "              \"you'\",\n",
       "              'such',\n",
       "              'scientist',\n",
       "              'Dataspaces',\n",
       "              'Dataspaces'],\n",
       "             'result': ['.', 'was', '.'],\n",
       "             \"It'\": ['s', 's', 's', 's', 's', 's', 's', 's'],\n",
       "             'not': ['just',\n",
       "              'in',\n",
       "              'as',\n",
       "              'just',\n",
       "              'just',\n",
       "              'just',\n",
       "              'counting',\n",
       "              'know',\n",
       "              'really',\n",
       "              'absolute',\n",
       "              'finish',\n",
       "              'be',\n",
       "              'really',\n",
       "              'just',\n",
       "              'a',\n",
       "              'unsolvable'],\n",
       "             'just': ['an',\n",
       "              'a',\n",
       "              'a',\n",
       "              'companies',\n",
       "              'their',\n",
       "              'the',\n",
       "              'geek',\n",
       "              'announced',\n",
       "              'a',\n",
       "              'about',\n",
       "              'how',\n",
       "              'the',\n",
       "              'a',\n",
       "              'spent',\n",
       "              'throw'],\n",
       "             'an': ['application',\n",
       "              'entry',\n",
       "              'unknown',\n",
       "              'explosion',\n",
       "              'even',\n",
       "              'increase',\n",
       "              'HTML',\n",
       "              'array',\n",
       "              'easy',\n",
       "              'allure',\n",
       "              'extremely',\n",
       "              'extremely',\n",
       "              'obvious',\n",
       "              'experimental',\n",
       "              'hour',\n",
       "              'ill',\n",
       "              'important',\n",
       "              'excellent',\n",
       "              'excellent',\n",
       "              'important',\n",
       "              'essential',\n",
       "              'odd',\n",
       "              'end',\n",
       "              'epidemic',\n",
       "              'algorithm',\n",
       "              'agile',\n",
       "              'audio',\n",
       "              'important'],\n",
       "             'with': ['data',\n",
       "              'sometimes',\n",
       "              'what',\n",
       "              'the',\n",
       "              'it',\n",
       "              'gathering',\n",
       "              'our',\n",
       "              'is',\n",
       "              'geolocation',\n",
       "              'the',\n",
       "              'all',\n",
       "              'tools',\n",
       "              'an',\n",
       "              'badly',\n",
       "              'the',\n",
       "              'the',\n",
       "              'human',\n",
       "              'Apple',\n",
       "              'the',\n",
       "              'data',\n",
       "              'data',\n",
       "              'reality',\n",
       "              'it',\n",
       "              '10',\n",
       "              'faster',\n",
       "              'clients',\n",
       "              'the',\n",
       "              'a',\n",
       "              'hundreds',\n",
       "              'which',\n",
       "              'a',\n",
       "              'most',\n",
       "              'more',\n",
       "              'the',\n",
       "              'R',\n",
       "              'a',\n",
       "              '.',\n",
       "              'patience',\n",
       "              'new',\n",
       "              'very'],\n",
       "             \"it'\": ['s',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's'],\n",
       "             'product': ['.', 'reviews', 'cycles', 'or', 'that'],\n",
       "             'Data': ['science',\n",
       "              'Mashups',\n",
       "              'is',\n",
       "              'expands',\n",
       "              'Mashups',\n",
       "              'conditioning',\n",
       "              'is',\n",
       "              'is',\n",
       "              'science',\n",
       "              'science',\n",
       "              'scientists',\n",
       "              'is',\n",
       "              '3'],\n",
       "             'enables': ['the', 'stream', 'features'],\n",
       "             'creation': ['of'],\n",
       "             'products': ['.',\n",
       "              'on',\n",
       "              '.',\n",
       "              'they',\n",
       "              'by',\n",
       "              'available',\n",
       "              'that',\n",
       "              'are',\n",
       "              '.',\n",
       "              'from',\n",
       "              '.',\n",
       "              'incrementally'],\n",
       "             'One': ['of', 'of'],\n",
       "             'earlier': ['data'],\n",
       "             'was': ['the',\n",
       "              'posted',\n",
       "              'presented',\n",
       "              'probably',\n",
       "              'a',\n",
       "              'delayed',\n",
       "              'recently',\n",
       "              'disambiguating',\n",
       "              'insufficient',\n",
       "              'all',\n",
       "              'a',\n",
       "              'never',\n",
       "              'an',\n",
       "              \"'\",\n",
       "              'ignored'],\n",
       "             'CDDB': ['database',\n",
       "              'realized',\n",
       "              'and',\n",
       "              'views',\n",
       "              'arises',\n",
       "              'is',\n",
       "              'staff'],\n",
       "             'developers': ['of', 'have', 'explore', 'and'],\n",
       "             'realized': ['that'],\n",
       "             'CD': ['had', '.', \"you'\", \"that'\", \"you'\"],\n",
       "             'had': ['a', 'huge', 'built', 'in'],\n",
       "             'signature': ['based', 'based', 'in'],\n",
       "             'based': ['on', 'on'],\n",
       "             'exact': ['length'],\n",
       "             'length': ['in', 'of', 'and'],\n",
       "             'samples': ['of', 'or', 'with'],\n",
       "             'each': ['track', '.', '.', 'stage'],\n",
       "             'track': ['on',\n",
       "              'lengths',\n",
       "              'titles',\n",
       "              'sends',\n",
       "              'titles',\n",
       "              'lengths'],\n",
       "             'Gracenote': ['built'],\n",
       "             'built': ['a', 'data', 'the', 'toward', 'on', 'around'],\n",
       "             'lengths': ['and', 'and'],\n",
       "             'coupled': ['it', 'to'],\n",
       "             'album': ['metadata', 'titles', '.'],\n",
       "             'metadata': ['track', 'nicely'],\n",
       "             'titles': ['artists', '.', '.'],\n",
       "             'artists': ['album', \"they'\"],\n",
       "             'If': [\"you'\",\n",
       "              'you',\n",
       "              \"you'\",\n",
       "              'data',\n",
       "              'data',\n",
       "              'the',\n",
       "              'you',\n",
       "              'you',\n",
       "              'anything',\n",
       "              'you',\n",
       "              \"there'\"],\n",
       "             \"you'\": ['ve',\n",
       "              've',\n",
       "              've',\n",
       "              've',\n",
       "              've',\n",
       "              're',\n",
       "              'll',\n",
       "              're',\n",
       "              've',\n",
       "              're',\n",
       "              're',\n",
       "              'd',\n",
       "              've',\n",
       "              're',\n",
       "              've',\n",
       "              've',\n",
       "              'd'],\n",
       "             'ever': ['used', 'seen'],\n",
       "             'used': ['iTunes', 'in', 'data'],\n",
       "             'iTunes': ['to', 'reads'],\n",
       "             'rip': ['a'],\n",
       "             'taken': ['advantage'],\n",
       "             'advantage': ['of'],\n",
       "             'Before': ['it'],\n",
       "             'anything': ['else', 'from', 'can'],\n",
       "             'else': ['iTunes'],\n",
       "             'reads': ['the'],\n",
       "             'every': ['track',\n",
       "              'time',\n",
       "              'company',\n",
       "              'startup',\n",
       "              'non',\n",
       "              'project',\n",
       "              'level'],\n",
       "             'sends': ['it'],\n",
       "             'gets': ['back', 'a'],\n",
       "             'back': ['the', 'if', 'to', 'to'],\n",
       "             'you': ['have',\n",
       "              'can',\n",
       "              'may',\n",
       "              'search',\n",
       "              'use',\n",
       "              'make',\n",
       "              'look',\n",
       "              'have',\n",
       "              'will',\n",
       "              'leave',\n",
       "              'surf',\n",
       "              'know',\n",
       "              'have',\n",
       "              'can',\n",
       "              'simply',\n",
       "              'decide',\n",
       "              'have',\n",
       "              'have',\n",
       "              'need',\n",
       "              'need',\n",
       "              'can',\n",
       "              'can',\n",
       "              'can',\n",
       "              'can',\n",
       "              'have',\n",
       "              'may',\n",
       "              'really',\n",
       "              'have',\n",
       "              \"aren'\",\n",
       "              'what',\n",
       "              'can',\n",
       "              'use',\n",
       "              'start',\n",
       "              'take',\n",
       "              'can',\n",
       "              '.',\n",
       "              'understand',\n",
       "              'need',\n",
       "              'want',\n",
       "              'can',\n",
       "              'need',\n",
       "              'understand',\n",
       "              'find',\n",
       "              \"can'\",\n",
       "              'think',\n",
       "              'go',\n",
       "              'might',\n",
       "              'looking',\n",
       "              'hire',\n",
       "              'would',\n",
       "              'make'],\n",
       "             'have': ['a',\n",
       "              'to',\n",
       "              'to',\n",
       "              'is',\n",
       "              'no',\n",
       "              'real',\n",
       "              'already',\n",
       "              'had',\n",
       "              'built',\n",
       "              'found',\n",
       "              '1',\n",
       "              'very',\n",
       "              'established',\n",
       "              'found',\n",
       "              'to',\n",
       "              'to',\n",
       "              'humans',\n",
       "              'asked',\n",
       "              'a',\n",
       "              'to',\n",
       "              'to',\n",
       "              'been',\n",
       "              'all'],\n",
       "             \"that'\": ['s', 's', 's', 's', 's', 's', 's', 's', 's', 's'],\n",
       "             'including': ['a', 'climate', 'the'],\n",
       "             'made': ['yourself', 'the', 'recommendations', 'that'],\n",
       "             'yourself': ['you'],\n",
       "             'can': ['create',\n",
       "              'also',\n",
       "              'be',\n",
       "              'be',\n",
       "              'or',\n",
       "              'be',\n",
       "              'involve',\n",
       "              'start',\n",
       "              'make',\n",
       "              'replace',\n",
       "              'split',\n",
       "              'use',\n",
       "              'have',\n",
       "              'easily',\n",
       "              'do',\n",
       "              'allocate',\n",
       "              'be',\n",
       "              'perform',\n",
       "              'have',\n",
       "              'follow',\n",
       "              'tackle',\n",
       "              'think',\n",
       "              'you'],\n",
       "             'create': ['an', 'surprisingly', 'animations'],\n",
       "             'entry': ['for'],\n",
       "             'for': ['an',\n",
       "              'with',\n",
       "              'and',\n",
       "              'what',\n",
       "              'whatever',\n",
       "              'Apple',\n",
       "              'cheap',\n",
       "              'roughly',\n",
       "              'a',\n",
       "              'working',\n",
       "              'exploring',\n",
       "              'traditional',\n",
       "              'consistency',\n",
       "              'the',\n",
       "              'distributing',\n",
       "              'its',\n",
       "              'the',\n",
       "              'the',\n",
       "              'hours',\n",
       "              'lack',\n",
       "              'the',\n",
       "              'machine',\n",
       "              'public',\n",
       "              'R',\n",
       "              'most',\n",
       "              'many',\n",
       "              'statistics',\n",
       "              'data',\n",
       "              'anyone',\n",
       "              'plotting',\n",
       "              'creative',\n",
       "              'working',\n",
       "              'some',\n",
       "              'when',\n",
       "              'example',\n",
       "              'when'],\n",
       "             'unknown': ['album'],\n",
       "             'While': ['this', 'we', 'that', 'rock', 'there', 'I', 'there'],\n",
       "             'sounds': ['simple', 'like'],\n",
       "             'simple': ['enough', 'task', 'MapReduce', 'program', '.'],\n",
       "             'enough': [\"it'\"],\n",
       "             'revolutionary': ['CDDB'],\n",
       "             'views': ['music'],\n",
       "             'music': ['as', 'sharing', 'or', 'by'],\n",
       "             'audio': ['and', 'all', 'stream'],\n",
       "             'doing': ['so'],\n",
       "             'Their': ['business'],\n",
       "             'business': ['is', '.', 'suits', 'partners', 'intelligence'],\n",
       "             'fundamentally': ['different', 'dissimilar'],\n",
       "             'different': ['from',\n",
       "              'from',\n",
       "              'forms',\n",
       "              'According',\n",
       "              '.',\n",
       "              'assumptions',\n",
       "              'datasets',\n",
       "              'algorithms'],\n",
       "             'selling': ['music'],\n",
       "             'sharing': ['music'],\n",
       "             'analyzing': ['musical', 'an'],\n",
       "             'musical': ['tastes', 'problem'],\n",
       "             'tastes': ['though'],\n",
       "             'though': ['these', 'neither', '.', 'not'],\n",
       "             'these': ['can', 'applications', 'databases'],\n",
       "             'also': ['be', 'an', 'frequently', 'know', 'says'],\n",
       "             'be': [\"'\",\n",
       "              'mined',\n",
       "              'correlated',\n",
       "              'mined',\n",
       "              'useless',\n",
       "              'fun',\n",
       "              'dealing',\n",
       "              'nice',\n",
       "              'ready',\n",
       "              'willing',\n",
       "              'more',\n",
       "              'able',\n",
       "              'rolled',\n",
       "              'distributed',\n",
       "              'widely',\n",
       "              'called',\n",
       "              'current',\n",
       "              'worth',\n",
       "              'interesting',\n",
       "              'saying',\n",
       "              \"'\",\n",
       "              'the',\n",
       "              'built',\n",
       "              'able',\n",
       "              'a'],\n",
       "             \"products'\": ['.', 'that'],\n",
       "             'arises': ['entirely'],\n",
       "             'entirely': ['from'],\n",
       "             'viewing': ['a'],\n",
       "             'problem': ['as',\n",
       "              '.',\n",
       "              \"isn'\",\n",
       "              'involves',\n",
       "              '.',\n",
       "              'is',\n",
       "              '.',\n",
       "              '.',\n",
       "              'across',\n",
       "              'creating',\n",
       "              '.',\n",
       "              'with',\n",
       "              '.',\n",
       "              'that',\n",
       "              'though',\n",
       "              'that',\n",
       "              'from',\n",
       "              'or'],\n",
       "             'Google': ['is',\n",
       "              \"isn'\",\n",
       "              'Trends',\n",
       "              'has',\n",
       "              'popularized',\n",
       "              'Goggles',\n",
       "              'has',\n",
       "              'Amazon'],\n",
       "             'at': ['creating',\n",
       "              'the',\n",
       "              'roughly',\n",
       "              'bits',\n",
       "              'hand',\n",
       "              \"O'\",\n",
       "              'job',\n",
       "              'this',\n",
       "              'Cloudera',\n",
       "              'bit',\n",
       "              'many',\n",
       "              'Stanford',\n",
       "              'a',\n",
       "              'what',\n",
       "              'Facebook',\n",
       "              'a',\n",
       "              'LinkedIn',\n",
       "              'LinkedIn',\n",
       "              \"members'\",\n",
       "              'profiles',\n",
       "              'events',\n",
       "              'books',\n",
       "              'once',\n",
       "              'bit'],\n",
       "             'creating': ['data', 'large', 'the'],\n",
       "             \"Here'\": ['s'],\n",
       "             'few': ['examples',\n",
       "              'years',\n",
       "              'leaders',\n",
       "              'thousand',\n",
       "              'cents',\n",
       "              'hundred',\n",
       "              'years'],\n",
       "             'examples': ['Google'],\n",
       "             'only': ['company',\n",
       "              'costs',\n",
       "              'part',\n",
       "              'useful',\n",
       "              'for',\n",
       "              'require',\n",
       "              'needs',\n",
       "              'costs'],\n",
       "             'company': ['that', 'today', 'with'],\n",
       "             'knows': ['how'],\n",
       "             'how': ['to',\n",
       "              'to',\n",
       "              'you',\n",
       "              'do',\n",
       "              \"Google'\",\n",
       "              'bad',\n",
       "              'things',\n",
       "              'you',\n",
       "              'economies',\n",
       "              'to',\n",
       "              'to',\n",
       "              'to'],\n",
       "             'use': ['data',\n",
       "              'patterns',\n",
       "              'and',\n",
       "              '.',\n",
       "              'data',\n",
       "              'it',\n",
       "              'of',\n",
       "              'anything',\n",
       "              'Mechanical',\n",
       "              'them',\n",
       "              'one',\n",
       "              'via',\n",
       "              'data'],\n",
       "             'Facebook': ['and', 'or', 'possibly', 'and'],\n",
       "             'LinkedIn': ['use', 'dpatil', '.', 'have'],\n",
       "             'patterns': ['of'],\n",
       "             'friendship': ['relationships'],\n",
       "             'relationships': ['to'],\n",
       "             'suggest': ['other'],\n",
       "             'people': ['you', 'spending', 'do', 'this', 'and', 'pass'],\n",
       "             'may': ['know', 'be', 'not', 'or', 'not'],\n",
       "             'know': ['or',\n",
       "              'with',\n",
       "              \"that'\",\n",
       "              'which',\n",
       "              \"what'\",\n",
       "              'whether',\n",
       "              'how',\n",
       "              'what',\n",
       "              'that'],\n",
       "             'should': ['know'],\n",
       "             'sometimes': ['frightening'],\n",
       "             'frightening': ['accuracy'],\n",
       "             'accuracy': ['.', '.'],\n",
       "             'Amazon': ['saves', 'understands', 'data', 'Facebook'],\n",
       "             'saves': ['your'],\n",
       "             'your': ['searches',\n",
       "              'government',\n",
       "              'web',\n",
       "              'business',\n",
       "              'body',\n",
       "              'retail',\n",
       "              'local',\n",
       "              'data',\n",
       "              'task',\n",
       "              'training',\n",
       "              'data',\n",
       "              'personal'],\n",
       "             'searches': ['correlates', '.'],\n",
       "             'correlates': ['what'],\n",
       "             'search': ['for', 'for', 'terms', 'across', 'to'],\n",
       "             'users': ['search', 'provides', 'are', '.', 'to'],\n",
       "             'uses': ['it'],\n",
       "             'surprisingly': ['appropriate'],\n",
       "             'appropriate': ['recommendations'],\n",
       "             'recommendations': ['.', 'are', 'accordingly'],\n",
       "             'These': ['recommendations', 'are', 'features'],\n",
       "             'are': [\"'\",\n",
       "              'in',\n",
       "              'involved',\n",
       "              'required',\n",
       "              'annotated',\n",
       "              'consumer',\n",
       "              'seeing',\n",
       "              'easier',\n",
       "              'extremely',\n",
       "              'essential',\n",
       "              'easily',\n",
       "              'we',\n",
       "              'similar',\n",
       "              'designed',\n",
       "              'designed',\n",
       "              'increasing',\n",
       "              'frequently',\n",
       "              'the',\n",
       "              'designed',\n",
       "              'two',\n",
       "              'then',\n",
       "              'then',\n",
       "              'several',\n",
       "              'associated',\n",
       "              'many',\n",
       "              'valid',\n",
       "              'many',\n",
       "              'really',\n",
       "              'many',\n",
       "              'full',\n",
       "              'you',\n",
       "              'built',\n",
       "              'but',\n",
       "              'inherently',\n",
       "              'following'],\n",
       "             'help': ['to'],\n",
       "             'drive': [\"Amazon'\"],\n",
       "             \"Amazon'\": ['s', 's', 's', 's'],\n",
       "             'traditional': ['retail',\n",
       "              'statistics',\n",
       "              'techniques',\n",
       "              'data',\n",
       "              'analysis',\n",
       "              'statistics',\n",
       "              'business',\n",
       "              'computer'],\n",
       "             'retail': ['business', 'transactions', 'chain'],\n",
       "             'They': ['come',\n",
       "              \"aren'\",\n",
       "              'expose',\n",
       "              'accept',\n",
       "              'group',\n",
       "              'have',\n",
       "              'are',\n",
       "              'can',\n",
       "              'can',\n",
       "              'were'],\n",
       "             'come': ['about', 'in', 'from', 'up'],\n",
       "             'because': ['Amazon', 'automated', 'everyone'],\n",
       "             'understands': ['that'],\n",
       "             'book': [\"isn'\", 'a'],\n",
       "             'camera': [\"isn'\", 'and', 'is'],\n",
       "             'customer': [\"isn'\", 'customers'],\n",
       "             'ust': ['a'],\n",
       "             'customers': ['generate'],\n",
       "             'generate': ['a', 'a', 'a'],\n",
       "             'trail': ['of', 'of', 'since'],\n",
       "             \"exhaust'\": ['that'],\n",
       "             'mined': ['and', '.'],\n",
       "             'put': ['to', 'into', 'Hadoop', 'together'],\n",
       "             'cloud': ['of'],\n",
       "             'correlated': ['with'],\n",
       "             \"customers'\": ['behavior'],\n",
       "             'behavior': ['the'],\n",
       "             'they': ['leave',\n",
       "              'visit',\n",
       "              'contribute',\n",
       "              'use',\n",
       "              'go',\n",
       "              \"aren'\",\n",
       "              'had',\n",
       "              'generate',\n",
       "              'are',\n",
       "              'decided'],\n",
       "             'leave': ['every', 'an', 'behind'],\n",
       "             'time': ['they',\n",
       "              'online',\n",
       "              '.',\n",
       "              'you',\n",
       "              '.',\n",
       "              '.',\n",
       "              'data',\n",
       "              'reports',\n",
       "              'MapReduce',\n",
       "              '.',\n",
       "              '.',\n",
       "              'plus',\n",
       "              'to'],\n",
       "             'visit': ['the'],\n",
       "             'site': ['.', 'that'],\n",
       "             'thread': ['that'],\n",
       "             'ties': ['most'],\n",
       "             'most': ['of',\n",
       "              'meaningful',\n",
       "              'messy',\n",
       "              'data',\n",
       "              'popular',\n",
       "              'popular',\n",
       "              'statistical',\n",
       "              'data',\n",
       "              'from'],\n",
       "             'applications': ['together',\n",
       "              'leave',\n",
       "              \"it'\",\n",
       "              'outside',\n",
       "              'to',\n",
       "              'to',\n",
       "              '.'],\n",
       "             'together': ['is', 'fundamentally', 'at'],\n",
       "             'collected': ['from', 'and', 'your'],\n",
       "             'provides': ['added', 'commercial', 'an'],\n",
       "             'added': ['value', 'value'],\n",
       "             'Whether': ['that', \"we'\", 'you', \"it'\", 'humans'],\n",
       "             'terms': ['voice'],\n",
       "             'voice': ['samples'],\n",
       "             'reviews': ['the'],\n",
       "             'feedback': ['loop'],\n",
       "             'loop': ['in'],\n",
       "             'which': ['they',\n",
       "              'cover',\n",
       "              'can',\n",
       "              'may',\n",
       "              'originated',\n",
       "              'you',\n",
       "              'is',\n",
       "              'are',\n",
       "              'provides',\n",
       "              'there',\n",
       "              'lets',\n",
       "              'exposes',\n",
       "              'to',\n",
       "              'survival'],\n",
       "             'contribute': ['to'],\n",
       "             \"That'\": ['s', 's', 's', 's', 's'],\n",
       "             'beginning': ['of'],\n",
       "             'last': ['few'],\n",
       "             'there': ['has', 'was', \"isn'\", 'are', 'are', 'are'],\n",
       "             'has': ['been',\n",
       "              'been',\n",
       "              'people',\n",
       "              'increased',\n",
       "              'moved',\n",
       "              'more',\n",
       "              'indexed',\n",
       "              'an',\n",
       "              'proven',\n",
       "              'been',\n",
       "              'been',\n",
       "              'just',\n",
       "              'eaten',\n",
       "              'become',\n",
       "              'excellent'],\n",
       "             'been': ['an',\n",
       "              'instrumented',\n",
       "              'instrumental',\n",
       "              'hampered',\n",
       "              'easy'],\n",
       "             'explosion': ['in'],\n",
       "             'amount': ['of'],\n",
       "             'available': ['.',\n",
       "              'and',\n",
       "              'Amazon',\n",
       "              'the',\n",
       "              'almost',\n",
       "              'online',\n",
       "              'for'],\n",
       "             \"we'\": ['re', 're', 've', 're', 're', 've', 're'],\n",
       "             're': ['talking',\n",
       "              'increasingly',\n",
       "              'finding',\n",
       "              'likely',\n",
       "              'going',\n",
       "              'looking',\n",
       "              'discussing',\n",
       "              'trying',\n",
       "              'discussing',\n",
       "              'asking',\n",
       "              'asking',\n",
       "              'drawing',\n",
       "              'entering',\n",
       "              'all',\n",
       "              'entrepreneurs'],\n",
       "             'talking': ['about'],\n",
       "             'server': ['logs', 'your'],\n",
       "             'logs': ['tweet', 'from'],\n",
       "             'tweet': ['streams'],\n",
       "             'streams': ['online', 'and'],\n",
       "             'transaction': ['records'],\n",
       "             'records': [\"'\"],\n",
       "             'citizen': ['science'],\n",
       "             'sensors': ['government'],\n",
       "             'government': ['data', 'your'],\n",
       "             'some': ['other', 'point', 'hints', 'data', 'creativity'],\n",
       "             'source': ['the', 'and', 'a', 'implementation', 'R'],\n",
       "             'finding': ['data', 'data', 'that'],\n",
       "             'figuring': ['out'],\n",
       "             'out': ['what',\n",
       "              \"what'\",\n",
       "              'of',\n",
       "              'whether',\n",
       "              'just',\n",
       "              'if',\n",
       "              'with',\n",
       "              'incrementally',\n",
       "              'how',\n",
       "              'how'],\n",
       "             'And': [\"it'\", 'that', 'as', 'this'],\n",
       "             'their': ['own',\n",
       "              'users',\n",
       "              'own',\n",
       "              'schemas',\n",
       "              'Prediction',\n",
       "              'machine',\n",
       "              'libraries',\n",
       "              'datastreams',\n",
       "              'success',\n",
       "              'path'],\n",
       "             'own': ['data', 'data', 'sales', 'story'],\n",
       "             'contributed': ['by'],\n",
       "             'increasingly': ['common', 'finding'],\n",
       "             'common': ['to'],\n",
       "             'mashup': ['data'],\n",
       "             'sources': ['.', 'of', 'all', 'in', 'to', 'and'],\n",
       "             'Mashups': ['in', 'in'],\n",
       "             \"R'\": ['analyzes', 'was'],\n",
       "             'analyzes': ['mortgage'],\n",
       "             'mortgage': ['foreclosures'],\n",
       "             'foreclosures': ['in', 'on'],\n",
       "             'Philadelphia': ['County', 'county'],\n",
       "             'County': ['by'],\n",
       "             'taking': ['a'],\n",
       "             'public': ['report', 'website', 'use', 'photos'],\n",
       "             'report': ['from', 'only'],\n",
       "             'county': [\"sheriff'\", \"sheriff'\"],\n",
       "             \"sheriff'\": ['s', 's'],\n",
       "             'office': ['extracting', '.'],\n",
       "             'extracting': ['addresses'],\n",
       "             'addresses': ['and', 'to'],\n",
       "             'Yahoo': ['to'],\n",
       "             'convert': ['the'],\n",
       "             'latitude': ['and'],\n",
       "             'longitude': ['then'],\n",
       "             'then': ['using',\n",
       "              'distributed',\n",
       "              'combined',\n",
       "              'combine',\n",
       "              'use',\n",
       "              'going',\n",
       "              'branched',\n",
       "              'looking'],\n",
       "             'geographical': ['data'],\n",
       "             'place': ['the', '.', 'to', 'where'],\n",
       "             'map': ['another'],\n",
       "             'another': ['data', 'dimension', 'essential', '.', 'piece'],\n",
       "             'group': ['them',\n",
       "              'at',\n",
       "              'together',\n",
       "              'he',\n",
       "              'at',\n",
       "              'recommendation',\n",
       "              '.'],\n",
       "             'them': ['by',\n",
       "              'are',\n",
       "              'only',\n",
       "              'open',\n",
       "              '.',\n",
       "              'inexpensively',\n",
       "              'into',\n",
       "              '.',\n",
       "              '.',\n",
       "              'the'],\n",
       "             'neighborhood': ['valuation', 'per'],\n",
       "             'valuation': ['neighborhood'],\n",
       "             'per': ['capita', 'gram', 'dollar'],\n",
       "             'capita': ['income'],\n",
       "             'income': ['and'],\n",
       "             'socio': ['economic'],\n",
       "             'economic': ['factors'],\n",
       "             'factors': ['.'],\n",
       "             'question': ['facing', 'we', 'we'],\n",
       "             'facing': ['every'],\n",
       "             'startup': ['every'],\n",
       "             'non': ['profit'],\n",
       "             'profit': ['every'],\n",
       "             'project': ['site', 'is', '.', 'that', 'that'],\n",
       "             'wants': ['to'],\n",
       "             'attract': ['a'],\n",
       "             'community': ['is'],\n",
       "             'effectively': [\"'\", 'requires', \"we'\"],\n",
       "             'but': ['all',\n",
       "              'fairly',\n",
       "              'there',\n",
       "              'to',\n",
       "              'tools',\n",
       "              \"'\",\n",
       "              'different',\n",
       "              'in',\n",
       "              'not',\n",
       "              'Hadoop',\n",
       "              'a',\n",
       "              'how',\n",
       "              'it',\n",
       "              'we',\n",
       "              'newer',\n",
       "              'also'],\n",
       "             'relevant': ['.'],\n",
       "             'Using': ['data'],\n",
       "             'requires': ['something', 'a', 'skills'],\n",
       "             'where': ['actuaries',\n",
       "              'it',\n",
       "              'it',\n",
       "              \"Moore'\",\n",
       "              \"it'\",\n",
       "              'services',\n",
       "              \"'\"],\n",
       "             'actuaries': ['in'],\n",
       "             'suits': ['perform'],\n",
       "             'perform': ['arcane', 'computations', 'a'],\n",
       "             'arcane': ['but'],\n",
       "             'fairly': ['well', 'comprehensive'],\n",
       "             'well': ['defined', 'behaved', 'you'],\n",
       "             'defined': ['kinds', 'problems'],\n",
       "             'kinds': ['of', 'of'],\n",
       "             'analysis': ['.',\n",
       "              'and',\n",
       "              'project',\n",
       "              'group',\n",
       "              'and',\n",
       "              \"we'\",\n",
       "              'is',\n",
       "              '.',\n",
       "              'has',\n",
       "              'enables',\n",
       "              '.',\n",
       "              'algorithms',\n",
       "              '.',\n",
       "              '.',\n",
       "              'over'],\n",
       "             'differentiates': ['data'],\n",
       "             'holistic': ['approach'],\n",
       "             'approach': ['.', 'which'],\n",
       "             'wild': ['and', \"data'\"],\n",
       "             'scientists': ['are', 'tend', \"'\", 'started', 'combine'],\n",
       "             'involved': ['with'],\n",
       "             'gathering': ['data'],\n",
       "             'massaging': ['it'],\n",
       "             'into': ['a',\n",
       "              'it',\n",
       "              'a',\n",
       "              'a',\n",
       "              'a',\n",
       "              'a',\n",
       "              'categories',\n",
       "              'distributed',\n",
       "              'how',\n",
       "              'smaller',\n",
       "              'a',\n",
       "              'their'],\n",
       "             'tractable': ['form', 'problem'],\n",
       "             'form': ['making'],\n",
       "             'making': ['it', 'data', 'guesses', 'sure', 'a', 'connections'],\n",
       "             'tell': ['its', 'its', 'its'],\n",
       "             'story': ['and', 'to', 'which', \"isn'\", '.', 'the'],\n",
       "             'presenting': ['that', 'data', 'results'],\n",
       "             'others': ['.', 'the'],\n",
       "             'To': ['get', 'do', 'do', 'store', 'understand'],\n",
       "             'get': ['a', '.', \"'\", 'a', 'presentable', 'a'],\n",
       "             'sense': ['for', 'of', 'of'],\n",
       "             'skills': ['are', '.', 'ranging', 'and'],\n",
       "             'required': [\"let'\"],\n",
       "             \"let'\": ['s'],\n",
       "             'look': ['at', 'at', 'at', 'up', 'for', 'like'],\n",
       "             'lifecycle': ['where'],\n",
       "             'comes': ['from', 'in', 'and', 'close', 'in'],\n",
       "             'goes': ['.', 'far'],\n",
       "             'everywhere': ['your'],\n",
       "             'partners': ['even'],\n",
       "             'even': ['your',\n",
       "              'job',\n",
       "              'richer',\n",
       "              'getting',\n",
       "              'days',\n",
       "              'an',\n",
       "              'face',\n",
       "              'have'],\n",
       "             'body': ['.', 'of', 'Or'],\n",
       "             \"aren'\": ['t', 't', 't', 't'],\n",
       "             'drowning': ['in'],\n",
       "             'sea': ['of'],\n",
       "             'almost': ['everything', 'all', 'always'],\n",
       "             'everything': ['can', 'from'],\n",
       "             'instrumented': ['.'],\n",
       "             'At': [\"O'\", 'some', \"IBM'\"],\n",
       "             'frequently': ['combine', 'missing', 'all', '.', 'called', 'the'],\n",
       "             'combine': ['publishing', 'the', 'entrepreneurship'],\n",
       "             'publishing': ['industry', 'industry'],\n",
       "             'industry': ['data', '.', '.', 'is'],\n",
       "             'Nielsen': ['BookScan'],\n",
       "             'BookScan': ['with'],\n",
       "             'our': ['own', 'analyses'],\n",
       "             'sales': ['data', 'to', 'to'],\n",
       "             'publicly': ['available'],\n",
       "             'see': [\"what'\", 'classification', 'midomi'],\n",
       "             \"what'\": ['s', 's', 's'],\n",
       "             'happening': ['in', 'with'],\n",
       "             'Sites': ['like'],\n",
       "             'like': ['Infochimps',\n",
       "              'Beautiful',\n",
       "              'a',\n",
       "              'the',\n",
       "              \"Amazon'\",\n",
       "              'an',\n",
       "              'queries',\n",
       "              'trending',\n",
       "              'Twitter',\n",
       "              'the',\n",
       "              'an',\n",
       "              '.',\n",
       "              'did',\n",
       "              'to',\n",
       "              'bit'],\n",
       "             'Infochimps': ['and'],\n",
       "             'Factual': ['provide', 'enlists'],\n",
       "             'provide': ['access', 'data', \"'\"],\n",
       "             'access': ['to'],\n",
       "             'large': ['datasets',\n",
       "              'snakes',\n",
       "              'number',\n",
       "              'problem',\n",
       "              'computing',\n",
       "              'searches',\n",
       "              'data',\n",
       "              'collection',\n",
       "              'job',\n",
       "              'problems',\n",
       "              'difficult'],\n",
       "             'datasets': ['including',\n",
       "              'which',\n",
       "              'for',\n",
       "              'effectively',\n",
       "              'present',\n",
       "              'the',\n",
       "              'using',\n",
       "              'quickly',\n",
       "              'and'],\n",
       "             'climate': ['data'],\n",
       "             'MySpace': ['activity'],\n",
       "             'activity': ['streams'],\n",
       "             'game': ['logs'],\n",
       "             'sporting': ['events'],\n",
       "             'events': ['.', 'that'],\n",
       "             'enlists': ['users'],\n",
       "             'update': ['and'],\n",
       "             'improve': ['its'],\n",
       "             'cover': ['topics'],\n",
       "             'topics': ['as', 'on', \"don'\"],\n",
       "             'diverse': ['as'],\n",
       "             'endocrinologists': ['to'],\n",
       "             'hiking': ['trails'],\n",
       "             'trails': ['.'],\n",
       "             'Much': ['of'],\n",
       "             'currently': ['work'],\n",
       "             'work': ['with',\n",
       "              '.',\n",
       "              'with',\n",
       "              'without',\n",
       "              'if',\n",
       "              '.',\n",
       "              '.',\n",
       "              'R',\n",
       "              \"That'\",\n",
       "              'with'],\n",
       "             'direct': ['consequence'],\n",
       "             'consequence': ['of'],\n",
       "             \"Moore'\": ['s', 's', 's'],\n",
       "             'Law': ['applied', 'comes'],\n",
       "             'applied': ['to', 'to'],\n",
       "             'spending': ['more'],\n",
       "             'leaving': ['a'],\n",
       "             'wherever': ['they'],\n",
       "             'go': ['.', 'beyond', 'to'],\n",
       "             'Mobile': ['applications'],\n",
       "             'richer': ['data'],\n",
       "             'since': ['many'],\n",
       "             'annotated': ['with'],\n",
       "             'geolocation': ['or', 'skills'],\n",
       "             'involve': ['video', 'cleaning'],\n",
       "             'video': ['or', 'is'],\n",
       "             'Point': ['of'],\n",
       "             'sale': ['devices'],\n",
       "             'devices': ['and'],\n",
       "             'frequent': [\"shopper'\"],\n",
       "             \"shopper'\": ['s'],\n",
       "             'cards': ['make'],\n",
       "             'make': ['it',\n",
       "              'online',\n",
       "              'a',\n",
       "              'that',\n",
       "              'it',\n",
       "              'it',\n",
       "              'it',\n",
       "              'it',\n",
       "              'from'],\n",
       "             'possible': ['to', '.', 'to', 'to'],\n",
       "             'capture': ['all'],\n",
       "             'transactions': ['not', 'that'],\n",
       "             'ones': ['you'],\n",
       "             'All': ['of'],\n",
       "             'would': ['be', 'be', 'have', 'take', 'start'],\n",
       "             'useless': ['if'],\n",
       "             'if': ['we',\n",
       "              'there',\n",
       "              \"you'\",\n",
       "              'any',\n",
       "              'you',\n",
       "              \"you'\",\n",
       "              'you',\n",
       "              'you',\n",
       "              'you',\n",
       "              'you',\n",
       "              'so',\n",
       "              'it'],\n",
       "             \"couldn'\": ['t'],\n",
       "             'store': ['it', 'it', 'huge'],\n",
       "             'Since': ['the'],\n",
       "             'early': [\"'\"],\n",
       "             '80s': ['processor'],\n",
       "             'processor': ['speed'],\n",
       "             'speed': ['has', '.', '.'],\n",
       "             'increased': ['from', 'sophistication'],\n",
       "             '10': ['MHz', '000', '000'],\n",
       "             'MHz': ['to'],\n",
       "             '3': ['.', 'Where', \"'\"],\n",
       "             '6': ['GHz'],\n",
       "             'GHz': [\"'\"],\n",
       "             'increase': ['of', 'in', 'of'],\n",
       "             '360': ['not'],\n",
       "             'counting': ['increases'],\n",
       "             'increases': ['in', 'in'],\n",
       "             'word': ['length', \"'\"],\n",
       "             'cores': ['.', 'running'],\n",
       "             'seen': ['much', 'the', 'a'],\n",
       "             'much': ['bigger', 'easier', 'of', 'more'],\n",
       "             'bigger': ['increases'],\n",
       "             'storage': ['capacity', 'has', 'is', 'capacity', 'capacity'],\n",
       "             'capacity': ['on', 'storage', 'demands', 'continues'],\n",
       "             'level': ['.', 'dataflow'],\n",
       "             'RAM': ['has'],\n",
       "             'moved': ['from'],\n",
       "             '1': ['000', '.', '010', '012', 'The'],\n",
       "             '000': ['MB', 'postings', 'cores'],\n",
       "             'MB': ['to'],\n",
       "             'roughly': ['25', '250', '0'],\n",
       "             '25': ['GB'],\n",
       "             'GB': [\"'\", 'microSD'],\n",
       "             'price': ['reduction'],\n",
       "             'reduction': ['of', 'in'],\n",
       "             '40000': ['to'],\n",
       "             'say': ['nothing'],\n",
       "             'nothing': ['of'],\n",
       "             'size': ['and', 'of'],\n",
       "             'Hitachi': ['made'],\n",
       "             'first': ['gigabyte', 'step', 'step', 'data', 'flippant'],\n",
       "             'gigabyte': ['disk'],\n",
       "             'disk': ['drives'],\n",
       "             'drives': ['in', 'are'],\n",
       "             '1982': ['weighing'],\n",
       "             'weighing': ['in'],\n",
       "             '250': ['pounds'],\n",
       "             'pounds': ['now'],\n",
       "             'now': ['terabyte', 'expect', 'ask'],\n",
       "             'terabyte': ['drives'],\n",
       "             'consumer': ['equipment', 'oriented'],\n",
       "             'equipment': ['and', 'fails'],\n",
       "             '32': ['GB'],\n",
       "             'microSD': ['card'],\n",
       "             'weighs': ['about'],\n",
       "             'half': ['a'],\n",
       "             'gram': ['.', 'bits'],\n",
       "             'bits': ['per', 'per'],\n",
       "             'dollar': ['or'],\n",
       "             'raw': ['capacity'],\n",
       "             'than': ['kept',\n",
       "              'for',\n",
       "              'sales',\n",
       "              'another',\n",
       "              'computer',\n",
       "              'tackling'],\n",
       "             'kept': ['pace'],\n",
       "             'pace': ['with'],\n",
       "             'CPU': ['speed'],\n",
       "             'importance': ['of'],\n",
       "             'law': ['as'],\n",
       "             'geek': ['pyrotechnics'],\n",
       "             'pyrotechnics': ['.'],\n",
       "             'expands': ['to'],\n",
       "             'fill': ['the'],\n",
       "             'space': ['you'],\n",
       "             'will': ['find', 'be', 'be'],\n",
       "             'exhaust': ['you'],\n",
       "             'whenever': ['you'],\n",
       "             'surf': ['the'],\n",
       "             'friend': ['someone'],\n",
       "             'someone': ['on', 'with', 'you'],\n",
       "             'purchase': ['in'],\n",
       "             'local': ['supermarket'],\n",
       "             'supermarket': ['is'],\n",
       "             'carefully': ['collected'],\n",
       "             'analyzed': ['.', 'the', 'a'],\n",
       "             'Increased': ['storage'],\n",
       "             'demands': ['increased'],\n",
       "             'sophistication': ['in'],\n",
       "             'foundation': ['of'],\n",
       "             'So': ['how'],\n",
       "             'step': ['of', 'in'],\n",
       "             'conditioning': [\"'\", 'can', 'you', 'if', 'to'],\n",
       "             'getting': ['data', 'humans', 'the'],\n",
       "             'state': ['where', 'of'],\n",
       "             'usable': ['.'],\n",
       "             'We': ['are', 'now', \"don'\"],\n",
       "             'seeing': ['more'],\n",
       "             'formats': ['that', \"that'\", 'including'],\n",
       "             'easier': ['to', 'to', 'to'],\n",
       "             'consume': ['Atom'],\n",
       "             'Atom': ['data'],\n",
       "             'feeds': ['web'],\n",
       "             'microformats': ['and'],\n",
       "             'newer': ['technologies',\n",
       "              'techniques',\n",
       "              'extensions',\n",
       "              'companies'],\n",
       "             'directly': ['machine', 'is'],\n",
       "             'machine': ['consumable',\n",
       "              'learning',\n",
       "              'learning',\n",
       "              'learning',\n",
       "              'learning',\n",
       "              'learning'],\n",
       "             'consumable': ['.'],\n",
       "             'old': ['style'],\n",
       "             'style': ['screen'],\n",
       "             'screen': ['scraping'],\n",
       "             'scraping': [\"hasn'\"],\n",
       "             \"hasn'\": ['t'],\n",
       "             'died': ['and'],\n",
       "             'going': ['to', 'to', 'to', 'back', 'to'],\n",
       "             'die': ['.'],\n",
       "             'Many': ['sources', 'of', 'of', 'Eyes'],\n",
       "             \"data'\": ['are', 'is'],\n",
       "             'extremely': ['messy', 'large', 'large', 'long'],\n",
       "             'messy': ['.', 'HTML', 'and'],\n",
       "             'behaved': ['XML', 'data'],\n",
       "             'XML': ['files', 'parsers'],\n",
       "             'files': ['with'],\n",
       "             'nicely': ['in'],\n",
       "             'foreclosure': ['data'],\n",
       "             'posted': ['on'],\n",
       "             'website': ['by'],\n",
       "             'This': ['data', 'is'],\n",
       "             'presented': ['as'],\n",
       "             'HTML': ['file', \"that'\", 'with'],\n",
       "             'file': ['that'],\n",
       "             'probably': ['generated'],\n",
       "             'generated': ['automatically', 'by', '2'],\n",
       "             'automatically': ['from'],\n",
       "             'spreadsheet': ['.'],\n",
       "             'Excel': ['you'],\n",
       "             'fun': ['to'],\n",
       "             'process': ['.', 'of', 'worked', 'that', 'it'],\n",
       "             'cleaning': ['up'],\n",
       "             'up': ['messy',\n",
       "              'more',\n",
       "              'into',\n",
       "              'that',\n",
       "              'with',\n",
       "              'in',\n",
       "              'into',\n",
       "              'that',\n",
       "              'with'],\n",
       "             'tools': ['like', 'to', 'discarded', 'like', 'for'],\n",
       "             'Beautiful': ['Soup', 'Data', 'Data'],\n",
       "             'Soup': ['natural'],\n",
       "             'natural': ['language', 'language'],\n",
       "             'language': ['processing',\n",
       "              'understanding',\n",
       "              'and',\n",
       "              'processing',\n",
       "              'called',\n",
       "              \"'\",\n",
       "              'particularly'],\n",
       "             'parse': ['plain', 'the'],\n",
       "             'plain': ['text'],\n",
       "             'text': ['in', 'for'],\n",
       "             'English': ['and', '.'],\n",
       "             'languages': ['or', 'such'],\n",
       "             'humans': ['to', '0', 'classify', 'or'],\n",
       "             'dirty': ['work'],\n",
       "             \"You'\": ['re'],\n",
       "             'likely': ['to'],\n",
       "             'dealing': ['with'],\n",
       "             'array': ['of'],\n",
       "             'forms': ['.'],\n",
       "             'It': ['would',\n",
       "              'incorporates',\n",
       "              'is',\n",
       "              \"isn'\",\n",
       "              'has',\n",
       "              'would',\n",
       "              'then',\n",
       "              'started',\n",
       "              'was'],\n",
       "             'nice': ['if'],\n",
       "             'standard': ['set', '.'],\n",
       "             'set': ['of', 'to', 'of', 'of', \"'\", 'of', 'she'],\n",
       "             'ready': ['for'],\n",
       "             'whatever': ['comes', 'interests'],\n",
       "             'willing': ['to'],\n",
       "             'ancient': ['Unix'],\n",
       "             'Unix': ['utilities'],\n",
       "             'utilities': ['such'],\n",
       "             'such': ['as', 'as', '.'],\n",
       "             'awk': ['to'],\n",
       "             'parsers': ['and', 'for'],\n",
       "             'libraries': ['.', 'available', '.'],\n",
       "             'Scripting': ['languages'],\n",
       "             'Perl': ['and'],\n",
       "             'Python': ['are', 'language', 'Elefant', 'design'],\n",
       "             'essential': ['.', 'tool', 'tool'],\n",
       "             'Once': [\"you'\", \"you'\", \"you'\"],\n",
       "             'parsed': ['the'],\n",
       "             'start': ['thinking', 'a', 'a'],\n",
       "             'thinking': ['about'],\n",
       "             'quality': ['of'],\n",
       "             'missing': ['or', 'do', 'points'],\n",
       "             'incongruous': ['.', 'do', 'data'],\n",
       "             'simply': ['ignore'],\n",
       "             'ignore': ['the', 'anomalous'],\n",
       "             'points': ['That', 'at'],\n",
       "             'That': [\"isn'\", 'joke'],\n",
       "             'always': ['possible', 'requires'],\n",
       "             'decide': ['that'],\n",
       "             'wrong': ['with'],\n",
       "             'badly': ['behaved'],\n",
       "             'after': ['all', \"you'\"],\n",
       "             'fails': ['or', 'you', '.'],\n",
       "             'telling': ['its', 'you', 'you', \"isn'\", '.'],\n",
       "             'interesting': [\"It'\", '.', 'products'],\n",
       "             'reported': ['that'],\n",
       "             'discovery': ['of'],\n",
       "             'ozone': ['layer'],\n",
       "             'layer': ['depletion'],\n",
       "             'depletion': ['was'],\n",
       "             'delayed': ['because'],\n",
       "             'automated': ['data'],\n",
       "             'collection': ['tools', 'of', 'and'],\n",
       "             'discarded': ['readings'],\n",
       "             'readings': ['that'],\n",
       "             'were': ['too', 'insufficient', 'the', \"'\"],\n",
       "             'too': ['low'],\n",
       "             'low': ['1', 'values'],\n",
       "             'usually': ['impossible'],\n",
       "             'impossible': ['to'],\n",
       "             \"better'\": ['data'],\n",
       "             'no': ['alternative'],\n",
       "             'alternative': ['but'],\n",
       "             'hand': ['.'],\n",
       "             'involves': ['human', 'making'],\n",
       "             'human': ['language', 'intelligence'],\n",
       "             'understanding': ['the', 'the', 'of', 'how'],\n",
       "             'adds': ['another'],\n",
       "             'dimension': ['to'],\n",
       "             'Roger': ['Magoulas'],\n",
       "             'Magoulas': ['who'],\n",
       "             'who': ['runs', 'dies', 'figure'],\n",
       "             'runs': ['the'],\n",
       "             'recently': ['searching'],\n",
       "             'searching': ['a'],\n",
       "             'Apple': ['job', 'industry', 'you', \"'\"],\n",
       "             'listings': ['requiring', 'and'],\n",
       "             'requiring': ['geolocation'],\n",
       "             'task': ['the', 'but', 'up', 'is', '.'],\n",
       "             'trick': ['was'],\n",
       "             'disambiguating': [\"'\"],\n",
       "             \"Apple'\": ['from'],\n",
       "             'postings': ['in', 'with'],\n",
       "             'growing': ['Apple'],\n",
       "             'need': ['to', 'to', 'to', 'to', 'to', 'some'],\n",
       "             'understand': ['the', 'what', 'what', 'it', 'it'],\n",
       "             'grammatical': ['structure'],\n",
       "             'structure': ['of'],\n",
       "             'posting': ['you'],\n",
       "             'able': ['to', 'to'],\n",
       "             'showing': ['up'],\n",
       "             'Try': ['using'],\n",
       "             'Trends': ['to'],\n",
       "             'figure': ['out', 'out', 'out'],\n",
       "             'Cassandra': ['database'],\n",
       "             'll': ['get'],\n",
       "             'indexed': ['many'],\n",
       "             'websites': ['about'],\n",
       "             'snakes': ['.'],\n",
       "             'Disambiguation': ['is'],\n",
       "             'never': ['an', 'conceived'],\n",
       "             'easy': ['task', 'to', 'to', 'to', 'to'],\n",
       "             'Natural': ['Language'],\n",
       "             'Language': ['Toolkit'],\n",
       "             'Toolkit': ['library'],\n",
       "             'library': ['can', 'is', 'CRAN'],\n",
       "             'simpler': ['.'],\n",
       "             'When': ['natural', \"you'\"],\n",
       "             'replace': ['artificial'],\n",
       "             'artificial': ['intelligence', 'intelligence'],\n",
       "             'intelligence': ['with', '.', 'problem', 'BI'],\n",
       "             'Mechanical': ['Turk', \"Turk'\", 'Turk'],\n",
       "             'Turk': ['come', 'is', 'is'],\n",
       "             'split': ['your'],\n",
       "             'subtasks': ['that', 'which'],\n",
       "             'easily': ['described', 'be'],\n",
       "             'described': ['you', 'the'],\n",
       "             \"Turk'\": ['s'],\n",
       "             'marketplace': ['for'],\n",
       "             'cheap': ['labor'],\n",
       "             'labor': ['.'],\n",
       "             'For': ['example', 'computer'],\n",
       "             'example': ['if', 'of', '.'],\n",
       "             'looking': ['at', 'at', 'at', 'up', 'for'],\n",
       "             'want': ['to', 'to'],\n",
       "             'originated': ['with'],\n",
       "             'real': ['people', 'time', 'time', 'time', 'time', '.'],\n",
       "             'classification': ['for', 'error'],\n",
       "             '01': ['each', 'to'],\n",
       "             'already': ['reduced'],\n",
       "             'reduced': ['the'],\n",
       "             'paying': ['humans', 'only'],\n",
       "             'classify': ['them', 'them', 'a'],\n",
       "             'costs': ['100', 'a'],\n",
       "             '100': ['.'],\n",
       "             'lot': ['about', 'of', 'of'],\n",
       "             'big': ['data', \"data'\", 'picture', 'problem'],\n",
       "             \"big'\": ['is', 'is'],\n",
       "             'red': ['herring'],\n",
       "             'herring': ['.'],\n",
       "             'Oil': ['companies'],\n",
       "             'telecommunications': ['companies'],\n",
       "             'centric': ['industries'],\n",
       "             'industries': ['have'],\n",
       "             'huge': ['datasets',\n",
       "              'datasets',\n",
       "              'datasets',\n",
       "              'database',\n",
       "              'mountain'],\n",
       "             'long': ['time', 'turn', 'datasets'],\n",
       "             'continues': ['to'],\n",
       "             'expand': [\"today'\"],\n",
       "             \"today'\": ['s'],\n",
       "             'certainly': [\"tomorrow'\", 'worth'],\n",
       "             \"tomorrow'\": ['s'],\n",
       "             \"medium'\": ['and'],\n",
       "             \"week'\": ['s'],\n",
       "             'small': ['.', 'simple', 'and'],\n",
       "             'meaningful': ['definition'],\n",
       "             'definition': [\"I'\"],\n",
       "             \"I'\": ['ve'],\n",
       "             'when': ['the', 'she', 'the', 'you'],\n",
       "             'becomes': ['part'],\n",
       "             'part': ['of', 'of', 'of', 'of'],\n",
       "             'discussing': ['data', 'here'],\n",
       "             'problems': ['ranging', '.', 'ranging', 'up', '.', 'to', \"'\"],\n",
       "             'ranging': ['from', 'from', 'from'],\n",
       "             'gigabytes': ['to'],\n",
       "             'petabytes': ['of'],\n",
       "             'point': ['traditional', \"it'\"],\n",
       "             'techniques': ['for', 'from'],\n",
       "             'working': ['with', 'with'],\n",
       "             'run': ['out'],\n",
       "             'steam': ['.'],\n",
       "             'trying': ['to', 'to', 'to', 'to', 'to'],\n",
       "             'According': ['to', 'to', 'to', 'to', 'to'],\n",
       "             'Jeff': ['Hammerbacher',\n",
       "              'Hammerbacher',\n",
       "              'Hammerbacher',\n",
       "              'Hammerbacher'],\n",
       "             'Hammerbacher': ['2', 'said', 'in', 'in'],\n",
       "             'hackingdata': [\"we'\"],\n",
       "             'build': ['information',\n",
       "              'clusters',\n",
       "              'interesting',\n",
       "              'the',\n",
       "              'data'],\n",
       "             'information': ['platforms', 'platform'],\n",
       "             'platforms': ['or', 'are', 'have'],\n",
       "             'dataspaces': ['.'],\n",
       "             'Information': ['platforms', 'is', 'Platforms', 'Platforms'],\n",
       "             'similar': ['to'],\n",
       "             'warehouses': ['but'],\n",
       "             'expose': ['rich'],\n",
       "             'rich': ['APIs'],\n",
       "             'APIs': ['and'],\n",
       "             'designed': ['for', 'for', 'to', 'for'],\n",
       "             'exploring': ['and'],\n",
       "             'rather': ['than', 'than', 'than'],\n",
       "             'reporting': ['.'],\n",
       "             'accept': ['all'],\n",
       "             'schemas': ['evolve'],\n",
       "             'evolve': ['as'],\n",
       "             'changes': ['.'],\n",
       "             'Most': ['of', 'data'],\n",
       "             'organizations': ['that'],\n",
       "             'found': ['it', 'a'],\n",
       "             'necessary': ['to', 'for'],\n",
       "             'beyond': ['the', 'a'],\n",
       "             'relational': ['database', 'database'],\n",
       "             'model': ['.'],\n",
       "             'Traditional': ['relational', 'data'],\n",
       "             'systems': ['stop'],\n",
       "             'stop': ['being', 'information', \"shopping'\"],\n",
       "             'being': ['effective', 'physicists'],\n",
       "             'effective': ['at', 'R'],\n",
       "             'scale': ['.'],\n",
       "             'Managing': ['sharding'],\n",
       "             'sharding': ['and'],\n",
       "             'replication': ['across'],\n",
       "             'across': ['a', 'many', 'an', 'many', 'thousands', \"LinkedIn'\"],\n",
       "             'horde': ['of'],\n",
       "             'servers': ['is'],\n",
       "             'difficult': ['and', 'problem', 'problem'],\n",
       "             'slow': ['.'],\n",
       "             'define': ['a'],\n",
       "             'schema': ['in', '.'],\n",
       "             'advance': ['conflicts'],\n",
       "             'conflicts': ['with'],\n",
       "             'reality': ['of'],\n",
       "             'multiple': ['unstructured'],\n",
       "             'unstructured': ['data'],\n",
       "             'important': ['until', 'part', 'role', 'insight', 'skill'],\n",
       "             'until': ['after'],\n",
       "             'Relational': ['databases', 'databases'],\n",
       "             'consistency': ['to', 'is', 'and'],\n",
       "             'support': ['complex', '.'],\n",
       "             'complex': ['transactions', 'set'],\n",
       "             'rolled': ['back'],\n",
       "             'one': ['of',\n",
       "              'stop',\n",
       "              'of',\n",
       "              'of',\n",
       "              'advertisement',\n",
       "              'stop',\n",
       "              'place',\n",
       "              'in'],\n",
       "             'operations': ['fails'],\n",
       "             'rock': ['solid'],\n",
       "             'solid': ['consistency'],\n",
       "             'crucial': ['to', 'to', 'to'],\n",
       "             'kind': ['of', 'of', 'of'],\n",
       "             'here': ['.', '.'],\n",
       "             'Do': ['you'],\n",
       "             '010': ['or'],\n",
       "             '012': ['Twitter'],\n",
       "             'Twitter': ['followers', '.', 'a', 'you'],\n",
       "             'followers': ['Precision', 'on'],\n",
       "             'Precision': ['has'],\n",
       "             'allure': ['but', 'is'],\n",
       "             'outside': ['of', 'the'],\n",
       "             'finance': ['that'],\n",
       "             'deceptive': ['.'],\n",
       "             'comparative': ['if'],\n",
       "             'asking': ['whether', 'the'],\n",
       "             'whether': ['sales', \"you'\", 'this'],\n",
       "             'Northern': ['Europe'],\n",
       "             'Europe': ['are', 'you'],\n",
       "             'increasing': ['faster'],\n",
       "             'faster': ['than', 'product'],\n",
       "             'Southern': ['Europe'],\n",
       "             'concerned': ['about'],\n",
       "             'difference': ['between', 'is'],\n",
       "             'between': ['5', 'developers'],\n",
       "             '5': ['.', '.'],\n",
       "             '92': ['percent'],\n",
       "             'percent': ['annual', '.', 'more'],\n",
       "             'annual': ['growth'],\n",
       "             'growth': ['and', 'of'],\n",
       "             '93': ['percent'],\n",
       "             'breed': ['of'],\n",
       "             'appear': ['.'],\n",
       "             'called': ['NoSQL', 'Pig', 'a'],\n",
       "             'NoSQL': ['databases'],\n",
       "             'Non': ['Relational'],\n",
       "             'neither': ['term'],\n",
       "             'term': ['is'],\n",
       "             'very': ['useful',\n",
       "              'flexible',\n",
       "              'effective',\n",
       "              'difficult',\n",
       "              'broadly'],\n",
       "             'dissimilar': ['products'],\n",
       "             'logical': ['descendants'],\n",
       "             'descendants': ['of'],\n",
       "             \"Google'\": ['s', 's', 's'],\n",
       "             'BigTable': ['and'],\n",
       "             'Dynamo': ['and'],\n",
       "             'distributed': ['across', 'across', 'filesystem', 'computing'],\n",
       "             'nodes': ['to'],\n",
       "             'eventual': [\"consistency'\"],\n",
       "             \"consistency'\": ['but'],\n",
       "             'absolute': ['consistency'],\n",
       "             'flexible': ['schema', 'process'],\n",
       "             'two': ['dozen'],\n",
       "             'dozen': ['or', 'or'],\n",
       "             'open': ['source', 'source', 'source'],\n",
       "             'leaders': ['have'],\n",
       "             'established': ['themselves'],\n",
       "             'themselves': ['Storing'],\n",
       "             'Storing': ['data'],\n",
       "             'building': ['a', 'a', 'statistical', 'maps'],\n",
       "             'enormous': ['datasets'],\n",
       "             'present': ['computational'],\n",
       "             'computational': ['problems'],\n",
       "             'popularized': ['the'],\n",
       "             'MapReduce': ['approach',\n",
       "              'seems',\n",
       "              'has',\n",
       "              'is',\n",
       "              'makes',\n",
       "              'implementation',\n",
       "              'make',\n",
       "              'to'],\n",
       "             'basically': ['a'],\n",
       "             'divide': ['and'],\n",
       "             'conquer': ['strategy'],\n",
       "             'strategy': ['for'],\n",
       "             'distributing': ['an'],\n",
       "             'computing': ['cluster', '.', 'power', 'skills', 'time'],\n",
       "             'cluster': ['.'],\n",
       "             \"map'\": ['stage'],\n",
       "             'stage': ['a', '.', 'of'],\n",
       "             'programming': ['task'],\n",
       "             'divided': ['into'],\n",
       "             'identical': ['subtasks'],\n",
       "             'processors': ['the', 'and', 'as'],\n",
       "             'intermediate': ['results', 'results'],\n",
       "             'results': ['are', 'into', 'in', '.', 'it', 'of'],\n",
       "             'combined': ['by'],\n",
       "             'single': ['reduce', 'set', 'tool'],\n",
       "             'reduce': ['task'],\n",
       "             'hindsight': ['MapReduce'],\n",
       "             'seems': ['like'],\n",
       "             'obvious': ['solution', 'is'],\n",
       "             'solution': ['to', 'for', '.'],\n",
       "             'biggest': ['problem'],\n",
       "             'distribute': ['a'],\n",
       "             'thousands': ['of', 'of', 'of'],\n",
       "             \"What'\": ['s'],\n",
       "             'less': ['obvious'],\n",
       "             'proven': ['to'],\n",
       "             'widely': ['applicable'],\n",
       "             'applicable': ['to'],\n",
       "             'popular': ['open', 'courses'],\n",
       "             'implementation': ['of', 'of'],\n",
       "             'Hadoop': ['project',\n",
       "              'application',\n",
       "              'developers',\n",
       "              'to',\n",
       "              'images',\n",
       "              'goes',\n",
       "              'datasets',\n",
       "              'is',\n",
       "              'has',\n",
       "              'and',\n",
       "              'is',\n",
       "              'Online',\n",
       "              'processes',\n",
       "              '.',\n",
       "              'or'],\n",
       "             \"Yahoo'\": ['s'],\n",
       "             'claim': ['that'],\n",
       "             \"world'\": ['s'],\n",
       "             'largest': ['production'],\n",
       "             'production': ['Hadoop'],\n",
       "             'running': ['Linux'],\n",
       "             'Linux': ['brought', 'machines'],\n",
       "             'brought': ['it'],\n",
       "             'onto': ['center'],\n",
       "             'center': ['stage'],\n",
       "             'key': ['Hadoop', 'component', 'to'],\n",
       "             'home': ['at'],\n",
       "             'Cloudera': ['which'],\n",
       "             'commercial': ['support', 'statistical'],\n",
       "             'Elastic': ['MapReduce', 'MapReduce'],\n",
       "             'makes': ['it'],\n",
       "             'without': ['investing'],\n",
       "             'investing': ['in'],\n",
       "             'racks': ['of'],\n",
       "             'machines': ['by'],\n",
       "             'providing': ['preconfigured', \"'\"],\n",
       "             'preconfigured': ['Hadoop'],\n",
       "             'images': ['for'],\n",
       "             'EC2': ['clusters'],\n",
       "             'clusters': ['.', 'that'],\n",
       "             'You': ['can', \"don'\", 'have', 'need'],\n",
       "             'allocate': ['and', 'processors'],\n",
       "             'de': ['allocate', 'facto'],\n",
       "             'needed': ['paying'],\n",
       "             'far': ['beyond'],\n",
       "             'several': [\"it'\"],\n",
       "             'component': ['of'],\n",
       "             'incorporates': ['HDFS', 'a'],\n",
       "             'HDFS': ['a'],\n",
       "             'filesystem': ['designed'],\n",
       "             'performance': ['and'],\n",
       "             'reliability': ['requirements'],\n",
       "             'requirements': ['of'],\n",
       "             'HBase': ['database'],\n",
       "             ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1518"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make trigrams, the transitions need to be longer\n",
    "# the starts also need to be tracked seperately but I'll leave that for later\n",
    "\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "for prev, current, next in zip(document, document[1:], document[2:]):\n",
    "    if prev == '.':\n",
    "        starts.append(current)\n",
    "    trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4145"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigram_transitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The len() explodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"We'\", 've'),\n",
       " ('ve', 'all'),\n",
       " ('all', 'heard'),\n",
       " ('heard', 'it'),\n",
       " ('it', 'according')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_transitions)[0:5] # some python searching led me to this approach. You cannot access items based on an index in a dict..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied on a webpage\n",
    "\n",
    "- I messed around with both the scraping (using 'inspect' and selecting a 'div class' with a loose name) and then the creation of a list of words and `.`\n",
    "- then the transitions is a defaultdict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like the shifting atmosphere of a distant planet Chris Clark s music is subject to violent extremes . With little warning a reassuring beat might furiously morph into an instrumental storm breaking just as suddenly into a diamond rain of twinkling synths . The British musician s delight in wrong footing expectations has been one of the few constants in a career that has swerved wildly from tricky IDM to off kilter hip hop beats and from blistering techno to hushed minimalism . At his propulsive best Clark dazzles with both the density and dynamism of his music . But in contrast to the explosive changes that have taken place from record to record Clark s work has also undergone another more subtle evolution . In recent years as he has amassed a growing body of soundtracks for film and TV he has developed an ear for emptiness one that has both heightened the drama of his music and accentuated its suggestion of three dimensional space . The architecture of Clark s production has never sounded airier or more fluid than it does on his latest record Sus Dog where he foregrounds one instrument he has largely left in the margins his voice . Executive produced by Thom Yorke Sus Dog is warm and immediately gratifying offering the musician s fragile falsetto as a graceful counterpoint to his intricate and sometimes breakneck production . Historically Clark s experiments with voice have yielded mixed results . By turns angelic and menacing the vocal accents on 2017 s Death Peak are crucial to that record s apocalyptic appeal while the garbled ultra processed growls and chanted raps on 2009 s Totems Flare have aged poorly . Here rather than slapping his voice on top of the mix Clark has learned to accommodate it . Working with a more limited palette of alternately boxy and lightspeed synths interwoven with acoustic instruments Sus Dog is an ornate but fleet footed synth pop album brimming with some of the loveliest music he s ever made . Clark glides over his beats using his high plaintive voice to nudge a song into gear before soaring on its pent up momentum . Clutch Pearlers levitates over a bed of delicate music box plucks while on Town Crank he surfs a blaring synth pulse reminiscent of Suicide at their most antagonistic his voice rising above the chaos as the track veers into the red . With the exception of Arca s mentorship with Bjrk no electronic producer has had a more reliable singing coach than Clark under the tutelage of Thom Yorke . At first you might think that Yorke himself is tearing into Town Crank but the similarity between the two men is limited to their beatific falsettos . Clark s voice while handsome lacks the lower range and piercing corroded edge that Yorke brings to Radiohead s most emotive tracks a quality that he more than makes up for with the sheer violence of his production . Apart from the bridge of Bully where he sighs an ultra Yorkean line Drift off in traffic Colonized by your phone in a particularly Yorkean way he largely forgoes replicating any of his mentor s vocal tics even when they harmonize together on Medicine .'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = soup.find('div', 'body__inner-container')\n",
    "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period\n",
    "\n",
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "    document.extend(words)\n",
    "' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7.7',\n",
       " 'By Harry Tafoya',\n",
       " 'Genre:',\n",
       " 'Electronic',\n",
       " 'Label:',\n",
       " 'Throttle',\n",
       " 'Reviewed:',\n",
       " 'May 31, 2023',\n",
       " 'Like the shifting atmosphere of a distant planet,\\xa0Chris Clarks music is subject to violent extremes. With little warning, a reassuring beat might furiously morph into an instrumental storm, breaking just as suddenly into a\\xa0diamond rain of twinkling synths. The British musicians delight in wrong-footing expectations has been one of the few constants in a career that has swerved wildlyfrom tricky IDM to off-kilter hip-hop beats, and from blistering techno to hushed minimalism. At his propulsive best, Clark dazzles with both the density and dynamism of his music. But in contrast to the explosive changes that have taken place from record to record, Clarks work has also undergone another, more subtle evolution. In recent years, as he has amassed a growing body of soundtracks for film and TV, he has developed an ear for emptiness, one that has both heightened the drama of his music and accentuated its suggestion of three-dimensional space.',\n",
       " 'The architecture of Clarks production has never sounded airier or more fluid than it does on his latest record,\\xa0Sus Dog, where he foregrounds one instrument he has largely left in the margins: his voice. Executive produced by\\xa0Thom Yorke,\\xa0Sus Dog is warm and immediately gratifying, offering the musicians fragile falsetto as a graceful counterpoint to his intricate and sometimes breakneck production.',\n",
       " 'Historically, Clarks experiments with voice have yielded mixed results. By turns angelic and menacing, the vocal accents on 2017s\\xa0Death Peak are crucial to that records apocalyptic appeal, while the garbled, ultra-processed growls and chanted raps on 2009s\\xa0Totems Flare have aged poorly. Here, rather than slapping his voice on top of the mix, Clark has learned to accommodate it. Working with a more limited palette of alternately boxy and lightspeed synths interwoven with acoustic instruments,\\xa0Sus Dog is an ornate but fleet-footed synth-pop album brimming with some of the loveliest music hes ever made. Clark glides over his beats, using his high, plaintive voice to nudge a song into gear before soaring on its pent-up momentum. Clutch Pearlers levitates over a bed of delicate music-box plucks, while on Town Crank he surfs a blaring synth pulse reminiscent of\\xa0Suicide at their most antagonistic, his voice rising above the chaos as the track veers into the red.',\n",
       " 'With the exception of\\xa0Arcas mentorship with\\xa0Bjrk, no electronic producer has had a more reliable singing coach than Clark under the tutelage of\\xa0Thom Yorke. At first you might think that Yorke himself is tearing into Town Crank, but the similarity between the two men is limited to their beatific falsettos. Clarks voice, while handsome, lacks the lower range and piercing, corroded edge that Yorke brings to\\xa0Radioheads most emotive tracks, a quality that he more than makes up for with the sheer violence of his production. Apart from the bridge of Bully, where he sighs an ultra-Yorkean lineDrift off in traffic/Colonized by your phonein a particularly Yorkean way, he largely forgoes replicating any of his mentors vocal tics, even when they harmonize together on Medicine.',\n",
       " 'The power of Clarks singing derives from the shapes that his voice makes out of air as much as the content of the songwriting itself. He opens Alyosha with a tinny a cappella refrain, repeating I want to believe in a hurt tone before his voice cleaves into separate spheres that pit mature practicality against raw adolescent distrust. Forest is almost entirely instrumental until a bright, multi-tracked chorus to rival\\xa0Fleet Foxes rises brilliantly out of the mist. In one of the records most striking moments, at the title tracks emotional nadir, he leaps an octave from a mournful croon into an aching note of despair as a detuned synth bleeds over the songs swelling acoustics like a bruise.',\n",
       " 'Because Clarks production is so finely detailed, one risk in making the leap to conventional songwriting is that his words might appear crude in comparison. But his approach as a lyricist remains resolutely off-kilter, pitched between vague but highly evocative ribbons of text and a clear-eyed sensitivity that approaches the unknowability of human behavior from odd angles. Alyosha, a song whose title might be a reference to the virtuous but passive protagonist from\\xa0The Brothers Karamazov, is full of unanswerable, emotionally naked pleas for understanding that are met in turn by one of the producers most scorching and merciless techno refrains. On Dismissive, he achieves a Zen-like clarity, recognizing cruelty for the shortcoming that it is before transforming derision into drive. And they can be as cynical and dismissive as they like he croons, In fact please carry on/Its all fuel to the fire.',\n",
       " 'For years, Clarks best work has toggled between beauty and brutality, blistering noise and otherworldly calm.\\xa0Sus Dog is also situated between those two poles, but in leveraging his voice like this, Clark has discovered not only a new way to guide listeners through his maze-like production, but also of expressing the strain of navigating such wild terrain. On the closing Ladder he sings wearily over mournful piano about living on a ladder stuck between two floors, which itself could be a bleakly beautiful metaphor for the zigzagging course of his own music. On\\xa0Sus Dog, Clark harnesses his careers wild atmospheric extremes; its as though for the first time, he truly felt the weather in his bones.',\n",
       " 'All products featured on Pitchfork are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'More From Pitchfork',\n",
       " 'Events',\n",
       " ' 2023 Cond Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. Pitchfork may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond Nast. Ad Choices',\n",
       " 'CN Entertainment']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://pitchfork.com/reviews/albums/clark-sus-dog/\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "content = [p.get_text() for p in paragraphs]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ Rating-iATjmx iUEiRd hJnYqh crvVFm\">7.7</p>,\n",
       " <p class=\"BylineWrapper-jWHrLH hOvThu byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\">By </span><a class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BaseLink-eNWuiM BylineLink-gEnFiw iUEiRd caJArb GVLVC eErqIx byline__name-link button\" href=\"/staff/harry-tafoya/\">Harry Tafoya</a></span></span></p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceKey-gHIvng iUEiRd YAAgl bWJknB\">Genre:</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceValue-tfmqg iUEiRd iTpcbq fkSlPp\">Electronic</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceKey-gHIvng iUEiRd YAAgl bWJknB\">Label:</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceValue-tfmqg iUEiRd iTpcbq fkSlPp\">Throttle</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceKey-gHIvng iUEiRd YAAgl bWJknB\">Reviewed:</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ InfoSliceValue-tfmqg iUEiRd iTpcbq fkSlPp\">May 31, 2023</p>,\n",
       " <p>Like the shifting atmosphere of a distant planet,<a href=\"https://pitchfork.com/artists/4987-clark/\">Chris Clark</a>s music is subject to violent extremes. With little warning, a reassuring beat might furiously morph into an instrumental storm, breaking just as suddenly into a<a href=\"https://www.space.com/diamond-rain-atmosphere-uranus-neptune\">diamond rain</a> of twinkling synths. The British musicians delight in wrong-footing expectations has been one of the few constants in a career that has swerved wildlyfrom tricky IDM to off-kilter hip-hop beats, and from blistering techno to hushed minimalism. At his propulsive best, Clark dazzles with both the density and dynamism of his music. But in contrast to the explosive changes that have taken place from record to record, Clarks work has also undergone another, more subtle evolution. In recent years, as he has amassed a growing body of soundtracks for film and TV, he has developed an ear for emptiness, one that has both heightened the drama of his music and accentuated its suggestion of three-dimensional space.</p>,\n",
       " <p>The architecture of Clarks production has never sounded airier or more fluid than it does on his latest record,<em>Sus Dog</em>, where he foregrounds one instrument he has largely left in the margins: his voice. Executive produced by<a href=\"https://pitchfork.com/artists/4823-thom-yorke/\">Thom Yorke</a>,<em>Sus Dog</em> is warm and immediately gratifying, offering the musicians fragile falsetto as a graceful counterpoint to his intricate and sometimes breakneck production.</p>,\n",
       " <p>Historically, Clarks experiments with voice have yielded mixed results. By turns angelic and menacing, the vocal accents on 2017s<a href=\"https://pitchfork.com/reviews/albums/23074-clark-death-peak/\"><em>Death Peak</em></a> are crucial to that records apocalyptic appeal, while the garbled, ultra-processed growls and chanted raps on 2009s<a href=\"https://pitchfork.com/reviews/albums/13268-totems-flare/\"><em>Totems Flare</em></a> have aged poorly. Here, rather than slapping his voice on top of the mix, Clark has learned to accommodate it. Working with a more limited palette of alternately boxy and lightspeed synths interwoven with acoustic instruments,<em>Sus Dog</em> is an ornate but fleet-footed synth-pop album brimming with some of the loveliest music hes ever made. Clark glides over his beats, using his high, plaintive voice to nudge a song into gear before soaring on its pent-up momentum. Clutch Pearlers levitates over a bed of delicate music-box plucks, while on Town Crank he surfs a blaring synth pulse reminiscent of<a href=\"https://pitchfork.com/artists/3974-suicide/\">Suicide</a> at their most antagonistic, his voice rising above the chaos as the track veers into the red.</p>,\n",
       " <p>With the exception of<a href=\"https://pitchfork.com/artists/30594-arca/\">Arcas</a> mentorship with<a href=\"https://pitchfork.com/artists/363-bjork/\">Bjrk</a>, no electronic producer has had a more reliable singing coach than Clark under the tutelage of<a href=\"https://pitchfork.com/artists/4823-thom-yorke/\">Thom Yorke</a>. At first you might think that Yorke himself is tearing into Town Crank, but the similarity between the two men is limited to their beatific falsettos. Clarks voice, while handsome, lacks the lower range and piercing, corroded edge that Yorke brings to<a href=\"https://pitchfork.com/artists/3512-radiohead/\">Radioheads</a> most emotive tracks, a quality that he more than makes up for with the sheer violence of his production. Apart from the bridge of Bully, where he sighs an ultra-Yorkean lineDrift off in traffic/Colonized by your phonein a particularly Yorkean way, he largely forgoes replicating any of his mentors vocal tics, even when they harmonize together on Medicine.</p>,\n",
       " <p>The power of Clarks singing derives from the shapes that his voice makes out of air as much as the content of the songwriting itself. He opens Alyosha with a tinny a cappella refrain, repeating I want to believe in a hurt tone before his voice cleaves into separate spheres that pit mature practicality against raw adolescent distrust. Forest is almost entirely instrumental until a bright, multi-tracked chorus to rival<a href=\"https://pitchfork.com/artists/5653-fleet-foxes/\">Fleet Foxes</a> rises brilliantly out of the mist. In one of the records most striking moments, at the title tracks emotional nadir, he leaps an octave from a mournful croon into an aching note of despair as a detuned synth bleeds over the songs swelling acoustics like a bruise.</p>,\n",
       " <p>Because Clarks production is so finely detailed, one risk in making the leap to conventional songwriting is that his words might appear crude in comparison. But his approach as a lyricist remains resolutely off-kilter, pitched between vague but highly evocative ribbons of text and a clear-eyed sensitivity that approaches the unknowability of human behavior from odd angles. Alyosha, a song whose title might be a reference to the virtuous but passive protagonist from<em>The Brothers Karamazov</em>, is full of unanswerable, emotionally naked pleas for understanding that are met in turn by one of the producers most scorching and merciless techno refrains. On Dismissive, he achieves a Zen-like clarity, recognizing cruelty for the shortcoming that it is before transforming derision into drive. And they can be as cynical and dismissive as they like he croons, In fact please carry on/Its all fuel to the fire.</p>,\n",
       " <p>For years, Clarks best work has toggled between beauty and brutality, blistering noise and otherworldly calm.<em>Sus Dog</em> is also situated between those two poles, but in leveraging his voice like this, Clark has discovered not only a new way to guide listeners through his maze-like production, but also of expressing the strain of navigating such wild terrain. On the closing Ladder he sings wearily over mournful piano about living on a ladder stuck between two floors, which itself could be a bleakly beautiful metaphor for the zigzagging course of his own music. On<em>Sus Dog</em>, Clark harnesses his careers wild atmospheric extremes; its as though for the first time, he truly felt the weather in his bones.</p>,\n",
       " <p class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ TextBlockText-gtrMkY iUEiRd clZnvx kQegaj\">All products featured on Pitchfork are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.</p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"BylineWrapper-jWHrLH kPZLBD byline bylines__byline\" data-testid=\"BylineWrapper\" itemprop=\"author\" itemtype=\"http://schema.org/Person\"><span class=\"BylineNamesWrapper-jbHncj fuDQVo\" itemprop=\"name\"><span class=\"BylineName-kwmrLn cYaBaU byline__name\" data-testid=\"BylineName\"><span class=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ BylinePreamble-iJolpQ iUEiRd jSeRBj gnILss byline__preamble\"> </span></span></span></p>,\n",
       " <p class=\"NavigationHeadingWrapper-befTuI kSFYXD navigation__heading\" data-testid=\"navigation__heading\"><button aria-expanded=\"false\" class=\"NavigationHeadingButton-eDVixB fiYwDK\" data-testid=\"navigation__heading-button\" type=\"button\">More From Pitchfork<span class=\"NavigationHeadingArrow-iJeTMa gGaHQm\" data-testid=\"navigation__heading-arrow\"></span></button></p>,\n",
       " <p class=\"NavigationHeadingWrapper-befTuI kSFYXD navigation__heading\" data-testid=\"navigation__heading\"><button aria-expanded=\"false\" class=\"NavigationHeadingButton-eDVixB fiYwDK\" data-testid=\"navigation__heading-button\" type=\"button\">Events<span class=\"NavigationHeadingArrow-iJeTMa gGaHQm\" data-testid=\"navigation__heading-arrow\"></span></button></p>,\n",
       " <p class=\"SiteFooterLegaleseText-bquxSW dSDMmF\" data-testid=\"dangerous-legalese-brand-text\"> <!-- -->2023<!-- --> Cond Nast. All rights reserved. Use of this site constitutes acceptance of our<!-- --> <a class=\"external-link\" data-event-click='{\"element\":\"ExternalLink\",\"outgoingURL\":\"https://www.condenast.com/user-agreement/\"}' href=\"https://www.condenast.com/user-agreement/\" rel=\"nofollow noopener\" target=\"_blank\">User Agreement</a> and <a class=\"external-link\" data-event-click='{\"element\":\"ExternalLink\",\"outgoingURL\":\"http://www.condenast.com/privacy-policy#privacypolicy\"}' href=\"http://www.condenast.com/privacy-policy#privacypolicy\" rel=\"nofollow noopener\" target=\"_blank\">Privacy Policy and Cookie Statement</a> and <a class=\"external-link\" data-event-click='{\"element\":\"ExternalLink\",\"outgoingURL\":\"http://www.condenast.com/privacy-policy#privacypolicy-california\"}' href=\"http://www.condenast.com/privacy-policy#privacypolicy-california\" rel=\"nofollow noopener\" target=\"_blank\">Your California Privacy Rights.</a> <em>Pitchfork</em> may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond Nast.<!-- --> <a class=\"external-link\" data-event-click='{\"element\":\"ExternalLink\",\"outgoingURL\":\"http://www.condenast.com/privacy-policy#privacypolicy-optout\"}' href=\"http://www.condenast.com/privacy-policy#privacypolicy-optout\" rel=\"nofollow noopener\" target=\"_blank\">Ad Choices</a></p>,\n",
       " <p class=\"SiteFooterCollection-kggXvx eJYhwU\">CN Entertainment</p>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.get_text() is a beautifulsoup method!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Like the shifting atmosphere of a distant planet,\\xa0Chris Clarks music is subject to violent extremes. With little warning, a reassuring beat might furiously morph into an instrumental storm, breaking just as suddenly into a\\xa0diamond rain of twinkling synths. The British musicians delight in wrong-footing expectations has been one of the few constants in a career that has swerved wildlyfrom tricky IDM to off-kilter hip-hop beats, and from blistering techno to hushed minimalism. At his propulsive best, Clark dazzles with both the density and dynamism of his music. But in contrast to the explosive changes that have taken place from record to record, Clarks work has also undergone another, more subtle evolution. In recent years, as he has amassed a growing body of soundtracks for film and TV, he has developed an ear for emptiness, one that has both heightened the drama of his music and accentuated its suggestion of three-dimensional space.',\n",
       " 'The architecture of Clarks production has never sounded airier or more fluid than it does on his latest record,\\xa0Sus Dog, where he foregrounds one instrument he has largely left in the margins: his voice. Executive produced by\\xa0Thom Yorke,\\xa0Sus Dog is warm and immediately gratifying, offering the musicians fragile falsetto as a graceful counterpoint to his intricate and sometimes breakneck production.',\n",
       " 'Historically, Clarks experiments with voice have yielded mixed results. By turns angelic and menacing, the vocal accents on 2017s\\xa0Death Peak are crucial to that records apocalyptic appeal, while the garbled, ultra-processed growls and chanted raps on 2009s\\xa0Totems Flare have aged poorly. Here, rather than slapping his voice on top of the mix, Clark has learned to accommodate it. Working with a more limited palette of alternately boxy and lightspeed synths interwoven with acoustic instruments,\\xa0Sus Dog is an ornate but fleet-footed synth-pop album brimming with some of the loveliest music hes ever made. Clark glides over his beats, using his high, plaintive voice to nudge a song into gear before soaring on its pent-up momentum. Clutch Pearlers levitates over a bed of delicate music-box plucks, while on Town Crank he surfs a blaring synth pulse reminiscent of\\xa0Suicide at their most antagonistic, his voice rising above the chaos as the track veers into the red.',\n",
       " 'With the exception of\\xa0Arcas mentorship with\\xa0Bjrk, no electronic producer has had a more reliable singing coach than Clark under the tutelage of\\xa0Thom Yorke. At first you might think that Yorke himself is tearing into Town Crank, but the similarity between the two men is limited to their beatific falsettos. Clarks voice, while handsome, lacks the lower range and piercing, corroded edge that Yorke brings to\\xa0Radioheads most emotive tracks, a quality that he more than makes up for with the sheer violence of his production. Apart from the bridge of Bully, where he sighs an ultra-Yorkean lineDrift off in traffic/Colonized by your phonein a particularly Yorkean way, he largely forgoes replicating any of his mentors vocal tics, even when they harmonize together on Medicine.',\n",
       " 'The power of Clarks singing derives from the shapes that his voice makes out of air as much as the content of the songwriting itself. He opens Alyosha with a tinny a cappella refrain, repeating I want to believe in a hurt tone before his voice cleaves into separate spheres that pit mature practicality against raw adolescent distrust. Forest is almost entirely instrumental until a bright, multi-tracked chorus to rival\\xa0Fleet Foxes rises brilliantly out of the mist. In one of the records most striking moments, at the title tracks emotional nadir, he leaps an octave from a mournful croon into an aching note of despair as a detuned synth bleeds over the songs swelling acoustics like a bruise.',\n",
       " 'Because Clarks production is so finely detailed, one risk in making the leap to conventional songwriting is that his words might appear crude in comparison. But his approach as a lyricist remains resolutely off-kilter, pitched between vague but highly evocative ribbons of text and a clear-eyed sensitivity that approaches the unknowability of human behavior from odd angles. Alyosha, a song whose title might be a reference to the virtuous but passive protagonist from\\xa0The Brothers Karamazov, is full of unanswerable, emotionally naked pleas for understanding that are met in turn by one of the producers most scorching and merciless techno refrains. On Dismissive, he achieves a Zen-like clarity, recognizing cruelty for the shortcoming that it is before transforming derision into drive. And they can be as cynical and dismissive as they like he croons, In fact please carry on/Its all fuel to the fire.',\n",
       " 'For years, Clarks best work has toggled between beauty and brutality, blistering noise and otherworldly calm.\\xa0Sus Dog is also situated between those two poles, but in leveraging his voice like this, Clark has discovered not only a new way to guide listeners through his maze-like production, but also of expressing the strain of navigating such wild terrain. On the closing Ladder he sings wearily over mournful piano about living on a ladder stuck between two floors, which itself could be a bleakly beautiful metaphor for the zigzagging course of his own music. On\\xa0Sus Dog, Clark harnesses his careers wild atmospheric extremes; its as though for the first time, he truly felt the weather in his bones.',\n",
       " 'All products featured on Pitchfork are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_divs = soup.select('div[class*=\"body__inner-container\"]')  # to be more selective \n",
    "\n",
    "\n",
    "content = [p.get_text() for paragraphs in content_divs for p in paragraphs('p')] # to avoid glueing paragraphs together\n",
    "# [div.get_text() for div in content_divs]\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Like',\n",
       " 'the',\n",
       " 'shifting',\n",
       " 'atmosphere',\n",
       " 'of',\n",
       " 'a',\n",
       " 'distant',\n",
       " 'planet',\n",
       " 'Chris',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'music',\n",
       " 'is',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'violent',\n",
       " 'extremes',\n",
       " '.',\n",
       " 'With',\n",
       " 'little',\n",
       " 'warning',\n",
       " 'a',\n",
       " 'reassuring',\n",
       " 'beat',\n",
       " 'might',\n",
       " 'furiously',\n",
       " 'morph',\n",
       " 'into',\n",
       " 'an',\n",
       " 'instrumental',\n",
       " 'storm',\n",
       " 'breaking',\n",
       " 'just',\n",
       " 'as',\n",
       " 'suddenly',\n",
       " 'into',\n",
       " 'a',\n",
       " 'diamond',\n",
       " 'rain',\n",
       " 'of',\n",
       " 'twinkling',\n",
       " 'synths',\n",
       " '.',\n",
       " 'The',\n",
       " 'British',\n",
       " 'musician',\n",
       " 's',\n",
       " 'delight',\n",
       " 'in',\n",
       " 'wrong',\n",
       " 'footing',\n",
       " 'expectations',\n",
       " 'has',\n",
       " 'been',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'few',\n",
       " 'constants',\n",
       " 'in',\n",
       " 'a',\n",
       " 'career',\n",
       " 'that',\n",
       " 'has',\n",
       " 'swerved',\n",
       " 'wildly',\n",
       " 'from',\n",
       " 'tricky',\n",
       " 'IDM',\n",
       " 'to',\n",
       " 'off',\n",
       " 'kilter',\n",
       " 'hip',\n",
       " 'hop',\n",
       " 'beats',\n",
       " 'and',\n",
       " 'from',\n",
       " 'blistering',\n",
       " 'techno',\n",
       " 'to',\n",
       " 'hushed',\n",
       " 'minimalism',\n",
       " '.',\n",
       " 'At',\n",
       " 'his',\n",
       " 'propulsive',\n",
       " 'best',\n",
       " 'Clark',\n",
       " 'dazzles',\n",
       " 'with',\n",
       " 'both',\n",
       " 'the',\n",
       " 'density',\n",
       " 'and',\n",
       " 'dynamism',\n",
       " 'of',\n",
       " 'his',\n",
       " 'music',\n",
       " '.',\n",
       " 'But',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'explosive',\n",
       " 'changes',\n",
       " 'that',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'place',\n",
       " 'from',\n",
       " 'record',\n",
       " 'to',\n",
       " 'record',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'work',\n",
       " 'has',\n",
       " 'also',\n",
       " 'undergone',\n",
       " 'another',\n",
       " 'more',\n",
       " 'subtle',\n",
       " 'evolution',\n",
       " '.',\n",
       " 'In',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'as',\n",
       " 'he',\n",
       " 'has',\n",
       " 'amassed',\n",
       " 'a',\n",
       " 'growing',\n",
       " 'body',\n",
       " 'of',\n",
       " 'soundtracks',\n",
       " 'for',\n",
       " 'film',\n",
       " 'and',\n",
       " 'TV',\n",
       " 'he',\n",
       " 'has',\n",
       " 'developed',\n",
       " 'an',\n",
       " 'ear',\n",
       " 'for',\n",
       " 'emptiness',\n",
       " 'one',\n",
       " 'that',\n",
       " 'has',\n",
       " 'both',\n",
       " 'heightened',\n",
       " 'the',\n",
       " 'drama',\n",
       " 'of',\n",
       " 'his',\n",
       " 'music',\n",
       " 'and',\n",
       " 'accentuated',\n",
       " 'its',\n",
       " 'suggestion',\n",
       " 'of',\n",
       " 'three',\n",
       " 'dimensional',\n",
       " 'space',\n",
       " '.',\n",
       " 'The',\n",
       " 'architecture',\n",
       " 'of',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'production',\n",
       " 'has',\n",
       " 'never',\n",
       " 'sounded',\n",
       " 'airier',\n",
       " 'or',\n",
       " 'more',\n",
       " 'fluid',\n",
       " 'than',\n",
       " 'it',\n",
       " 'does',\n",
       " 'on',\n",
       " 'his',\n",
       " 'latest',\n",
       " 'record',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " 'where',\n",
       " 'he',\n",
       " 'foregrounds',\n",
       " 'one',\n",
       " 'instrument',\n",
       " 'he',\n",
       " 'has',\n",
       " 'largely',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'margins',\n",
       " 'his',\n",
       " 'voice',\n",
       " '.',\n",
       " 'Executive',\n",
       " 'produced',\n",
       " 'by',\n",
       " 'Thom',\n",
       " 'Yorke',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " 'is',\n",
       " 'warm',\n",
       " 'and',\n",
       " 'immediately',\n",
       " 'gratifying',\n",
       " 'offering',\n",
       " 'the',\n",
       " 'musician',\n",
       " 's',\n",
       " 'fragile',\n",
       " 'falsetto',\n",
       " 'as',\n",
       " 'a',\n",
       " 'graceful',\n",
       " 'counterpoint',\n",
       " 'to',\n",
       " 'his',\n",
       " 'intricate',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'breakneck',\n",
       " 'production',\n",
       " '.',\n",
       " 'Historically',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'experiments',\n",
       " 'with',\n",
       " 'voice',\n",
       " 'have',\n",
       " 'yielded',\n",
       " 'mixed',\n",
       " 'results',\n",
       " '.',\n",
       " 'By',\n",
       " 'turns',\n",
       " 'angelic',\n",
       " 'and',\n",
       " 'menacing',\n",
       " 'the',\n",
       " 'vocal',\n",
       " 'accents',\n",
       " 'on',\n",
       " '2017',\n",
       " 's',\n",
       " 'Death',\n",
       " 'Peak',\n",
       " 'are',\n",
       " 'crucial',\n",
       " 'to',\n",
       " 'that',\n",
       " 'record',\n",
       " 's',\n",
       " 'apocalyptic',\n",
       " 'appeal',\n",
       " 'while',\n",
       " 'the',\n",
       " 'garbled',\n",
       " 'ultra',\n",
       " 'processed',\n",
       " 'growls',\n",
       " 'and',\n",
       " 'chanted',\n",
       " 'raps',\n",
       " 'on',\n",
       " '2009',\n",
       " 's',\n",
       " 'Totems',\n",
       " 'Flare',\n",
       " 'have',\n",
       " 'aged',\n",
       " 'poorly',\n",
       " '.',\n",
       " 'Here',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'slapping',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mix',\n",
       " 'Clark',\n",
       " 'has',\n",
       " 'learned',\n",
       " 'to',\n",
       " 'accommodate',\n",
       " 'it',\n",
       " '.',\n",
       " 'Working',\n",
       " 'with',\n",
       " 'a',\n",
       " 'more',\n",
       " 'limited',\n",
       " 'palette',\n",
       " 'of',\n",
       " 'alternately',\n",
       " 'boxy',\n",
       " 'and',\n",
       " 'lightspeed',\n",
       " 'synths',\n",
       " 'interwoven',\n",
       " 'with',\n",
       " 'acoustic',\n",
       " 'instruments',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " 'is',\n",
       " 'an',\n",
       " 'ornate',\n",
       " 'but',\n",
       " 'fleet',\n",
       " 'footed',\n",
       " 'synth',\n",
       " 'pop',\n",
       " 'album',\n",
       " 'brimming',\n",
       " 'with',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'loveliest',\n",
       " 'music',\n",
       " 'he',\n",
       " 's',\n",
       " 'ever',\n",
       " 'made',\n",
       " '.',\n",
       " 'Clark',\n",
       " 'glides',\n",
       " 'over',\n",
       " 'his',\n",
       " 'beats',\n",
       " 'using',\n",
       " 'his',\n",
       " 'high',\n",
       " 'plaintive',\n",
       " 'voice',\n",
       " 'to',\n",
       " 'nudge',\n",
       " 'a',\n",
       " 'song',\n",
       " 'into',\n",
       " 'gear',\n",
       " 'before',\n",
       " 'soaring',\n",
       " 'on',\n",
       " 'its',\n",
       " 'pent',\n",
       " 'up',\n",
       " 'momentum',\n",
       " '.',\n",
       " 'Clutch',\n",
       " 'Pearlers',\n",
       " 'levitates',\n",
       " 'over',\n",
       " 'a',\n",
       " 'bed',\n",
       " 'of',\n",
       " 'delicate',\n",
       " 'music',\n",
       " 'box',\n",
       " 'plucks',\n",
       " 'while',\n",
       " 'on',\n",
       " 'Town',\n",
       " 'Crank',\n",
       " 'he',\n",
       " 'surfs',\n",
       " 'a',\n",
       " 'blaring',\n",
       " 'synth',\n",
       " 'pulse',\n",
       " 'reminiscent',\n",
       " 'of',\n",
       " 'Suicide',\n",
       " 'at',\n",
       " 'their',\n",
       " 'most',\n",
       " 'antagonistic',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'rising',\n",
       " 'above',\n",
       " 'the',\n",
       " 'chaos',\n",
       " 'as',\n",
       " 'the',\n",
       " 'track',\n",
       " 'veers',\n",
       " 'into',\n",
       " 'the',\n",
       " 'red',\n",
       " '.',\n",
       " 'With',\n",
       " 'the',\n",
       " 'exception',\n",
       " 'of',\n",
       " 'Arca',\n",
       " 's',\n",
       " 'mentorship',\n",
       " 'with',\n",
       " 'Bjrk',\n",
       " 'no',\n",
       " 'electronic',\n",
       " 'producer',\n",
       " 'has',\n",
       " 'had',\n",
       " 'a',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'singing',\n",
       " 'coach',\n",
       " 'than',\n",
       " 'Clark',\n",
       " 'under',\n",
       " 'the',\n",
       " 'tutelage',\n",
       " 'of',\n",
       " 'Thom',\n",
       " 'Yorke',\n",
       " '.',\n",
       " 'At',\n",
       " 'first',\n",
       " 'you',\n",
       " 'might',\n",
       " 'think',\n",
       " 'that',\n",
       " 'Yorke',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'tearing',\n",
       " 'into',\n",
       " 'Town',\n",
       " 'Crank',\n",
       " 'but',\n",
       " 'the',\n",
       " 'similarity',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'men',\n",
       " 'is',\n",
       " 'limited',\n",
       " 'to',\n",
       " 'their',\n",
       " 'beatific',\n",
       " 'falsettos',\n",
       " '.',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'voice',\n",
       " 'while',\n",
       " 'handsome',\n",
       " 'lacks',\n",
       " 'the',\n",
       " 'lower',\n",
       " 'range',\n",
       " 'and',\n",
       " 'piercing',\n",
       " 'corroded',\n",
       " 'edge',\n",
       " 'that',\n",
       " 'Yorke',\n",
       " 'brings',\n",
       " 'to',\n",
       " 'Radiohead',\n",
       " 's',\n",
       " 'most',\n",
       " 'emotive',\n",
       " 'tracks',\n",
       " 'a',\n",
       " 'quality',\n",
       " 'that',\n",
       " 'he',\n",
       " 'more',\n",
       " 'than',\n",
       " 'makes',\n",
       " 'up',\n",
       " 'for',\n",
       " 'with',\n",
       " 'the',\n",
       " 'sheer',\n",
       " 'violence',\n",
       " 'of',\n",
       " 'his',\n",
       " 'production',\n",
       " '.',\n",
       " 'Apart',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bridge',\n",
       " 'of',\n",
       " 'Bully',\n",
       " 'where',\n",
       " 'he',\n",
       " 'sighs',\n",
       " 'an',\n",
       " 'ultra',\n",
       " 'Yorkean',\n",
       " 'line',\n",
       " 'Drift',\n",
       " 'off',\n",
       " 'in',\n",
       " 'traffic',\n",
       " 'Colonized',\n",
       " 'by',\n",
       " 'your',\n",
       " 'phone',\n",
       " 'in',\n",
       " 'a',\n",
       " 'particularly',\n",
       " 'Yorkean',\n",
       " 'way',\n",
       " 'he',\n",
       " 'largely',\n",
       " 'forgoes',\n",
       " 'replicating',\n",
       " 'any',\n",
       " 'of',\n",
       " 'his',\n",
       " 'mentor',\n",
       " 's',\n",
       " 'vocal',\n",
       " 'tics',\n",
       " 'even',\n",
       " 'when',\n",
       " 'they',\n",
       " 'harmonize',\n",
       " 'together',\n",
       " 'on',\n",
       " 'Medicine',\n",
       " '.',\n",
       " 'The',\n",
       " 'power',\n",
       " 'of',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'singing',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'the',\n",
       " 'shapes',\n",
       " 'that',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'makes',\n",
       " 'out',\n",
       " 'of',\n",
       " 'air',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'the',\n",
       " 'songwriting',\n",
       " 'itself',\n",
       " '.',\n",
       " 'He',\n",
       " 'opens',\n",
       " 'Alyosha',\n",
       " 'with',\n",
       " 'a',\n",
       " 'tinny',\n",
       " 'a',\n",
       " 'cappella',\n",
       " 'refrain',\n",
       " 'repeating',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'a',\n",
       " 'hurt',\n",
       " 'tone',\n",
       " 'before',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'cleaves',\n",
       " 'into',\n",
       " 'separate',\n",
       " 'spheres',\n",
       " 'that',\n",
       " 'pit',\n",
       " 'mature',\n",
       " 'practicality',\n",
       " 'against',\n",
       " 'raw',\n",
       " 'adolescent',\n",
       " 'distrust',\n",
       " '.',\n",
       " 'Forest',\n",
       " 'is',\n",
       " 'almost',\n",
       " 'entirely',\n",
       " 'instrumental',\n",
       " 'until',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'multi',\n",
       " 'tracked',\n",
       " 'chorus',\n",
       " 'to',\n",
       " 'rival',\n",
       " 'Fleet',\n",
       " 'Foxes',\n",
       " 'rises',\n",
       " 'brilliantly',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mist',\n",
       " '.',\n",
       " 'In',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'record',\n",
       " 's',\n",
       " 'most',\n",
       " 'striking',\n",
       " 'moments',\n",
       " 'at',\n",
       " 'the',\n",
       " 'title',\n",
       " 'track',\n",
       " 's',\n",
       " 'emotional',\n",
       " 'nadir',\n",
       " 'he',\n",
       " 'leaps',\n",
       " 'an',\n",
       " 'octave',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mournful',\n",
       " 'croon',\n",
       " 'into',\n",
       " 'an',\n",
       " 'aching',\n",
       " 'note',\n",
       " 'of',\n",
       " 'despair',\n",
       " 'as',\n",
       " 'a',\n",
       " 'detuned',\n",
       " 'synth',\n",
       " 'bleeds',\n",
       " 'over',\n",
       " 'the',\n",
       " 'song',\n",
       " 's',\n",
       " 'swelling',\n",
       " 'acoustics',\n",
       " 'like',\n",
       " 'a',\n",
       " 'bruise',\n",
       " '.',\n",
       " 'Because',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'production',\n",
       " 'is',\n",
       " 'so',\n",
       " 'finely',\n",
       " 'detailed',\n",
       " 'one',\n",
       " 'risk',\n",
       " 'in',\n",
       " 'making',\n",
       " 'the',\n",
       " 'leap',\n",
       " 'to',\n",
       " 'conventional',\n",
       " 'songwriting',\n",
       " 'is',\n",
       " 'that',\n",
       " 'his',\n",
       " 'words',\n",
       " 'might',\n",
       " 'appear',\n",
       " 'crude',\n",
       " 'in',\n",
       " 'comparison',\n",
       " '.',\n",
       " 'But',\n",
       " 'his',\n",
       " 'approach',\n",
       " 'as',\n",
       " 'a',\n",
       " 'lyricist',\n",
       " 'remains',\n",
       " 'resolutely',\n",
       " 'off',\n",
       " 'kilter',\n",
       " 'pitched',\n",
       " 'between',\n",
       " 'vague',\n",
       " 'but',\n",
       " 'highly',\n",
       " 'evocative',\n",
       " 'ribbons',\n",
       " 'of',\n",
       " 'text',\n",
       " 'and',\n",
       " 'a',\n",
       " 'clear',\n",
       " 'eyed',\n",
       " 'sensitivity',\n",
       " 'that',\n",
       " 'approaches',\n",
       " 'the',\n",
       " 'unknowability',\n",
       " 'of',\n",
       " 'human',\n",
       " 'behavior',\n",
       " 'from',\n",
       " 'odd',\n",
       " 'angles',\n",
       " '.',\n",
       " 'Alyosha',\n",
       " 'a',\n",
       " 'song',\n",
       " 'whose',\n",
       " 'title',\n",
       " 'might',\n",
       " 'be',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'to',\n",
       " 'the',\n",
       " 'virtuous',\n",
       " 'but',\n",
       " 'passive',\n",
       " 'protagonist',\n",
       " 'from',\n",
       " 'The',\n",
       " 'Brothers',\n",
       " 'Karamazov',\n",
       " 'is',\n",
       " 'full',\n",
       " 'of',\n",
       " 'unanswerable',\n",
       " 'emotionally',\n",
       " 'naked',\n",
       " 'pleas',\n",
       " 'for',\n",
       " 'understanding',\n",
       " 'that',\n",
       " 'are',\n",
       " 'met',\n",
       " 'in',\n",
       " 'turn',\n",
       " 'by',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'producer',\n",
       " 's',\n",
       " 'most',\n",
       " 'scorching',\n",
       " 'and',\n",
       " 'merciless',\n",
       " 'techno',\n",
       " 'refrains',\n",
       " '.',\n",
       " 'On',\n",
       " 'Dismissive',\n",
       " 'he',\n",
       " 'achieves',\n",
       " 'a',\n",
       " 'Zen',\n",
       " 'like',\n",
       " 'clarity',\n",
       " 'recognizing',\n",
       " 'cruelty',\n",
       " 'for',\n",
       " 'the',\n",
       " 'shortcoming',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'before',\n",
       " 'transforming',\n",
       " 'derision',\n",
       " 'into',\n",
       " 'drive',\n",
       " '.',\n",
       " 'And',\n",
       " 'they',\n",
       " 'can',\n",
       " 'be',\n",
       " 'as',\n",
       " 'cynical',\n",
       " 'and',\n",
       " 'dismissive',\n",
       " 'as',\n",
       " 'they',\n",
       " 'like',\n",
       " 'he',\n",
       " 'croons',\n",
       " 'In',\n",
       " 'fact',\n",
       " 'please',\n",
       " 'carry',\n",
       " 'on',\n",
       " 'It',\n",
       " 's',\n",
       " 'all',\n",
       " 'fuel',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fire',\n",
       " '.',\n",
       " 'For',\n",
       " 'years',\n",
       " 'Clark',\n",
       " 's',\n",
       " 'best',\n",
       " 'work',\n",
       " 'has',\n",
       " 'toggled',\n",
       " 'between',\n",
       " 'beauty',\n",
       " 'and',\n",
       " 'brutality',\n",
       " 'blistering',\n",
       " 'noise',\n",
       " 'and',\n",
       " 'otherworldly',\n",
       " 'calm',\n",
       " '.',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " 'is',\n",
       " 'also',\n",
       " 'situated',\n",
       " 'between',\n",
       " 'those',\n",
       " 'two',\n",
       " 'poles',\n",
       " 'but',\n",
       " 'in',\n",
       " 'leveraging',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'like',\n",
       " 'this',\n",
       " 'Clark',\n",
       " 'has',\n",
       " 'discovered',\n",
       " 'not',\n",
       " 'only',\n",
       " 'a',\n",
       " 'new',\n",
       " 'way',\n",
       " 'to',\n",
       " 'guide',\n",
       " 'listeners',\n",
       " 'through',\n",
       " 'his',\n",
       " 'maze',\n",
       " 'like',\n",
       " 'production',\n",
       " 'but',\n",
       " 'also',\n",
       " 'of',\n",
       " 'expressing',\n",
       " 'the',\n",
       " 'strain',\n",
       " 'of',\n",
       " 'navigating',\n",
       " 'such',\n",
       " 'wild',\n",
       " 'terrain',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'closing',\n",
       " 'Ladder',\n",
       " 'he',\n",
       " 'sings',\n",
       " 'wearily',\n",
       " 'over',\n",
       " 'mournful',\n",
       " 'piano',\n",
       " 'about',\n",
       " 'living',\n",
       " 'on',\n",
       " 'a',\n",
       " 'ladder',\n",
       " 'stuck',\n",
       " 'between',\n",
       " 'two',\n",
       " 'floors',\n",
       " 'which',\n",
       " 'itself',\n",
       " 'could',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bleakly',\n",
       " 'beautiful',\n",
       " 'metaphor',\n",
       " 'for',\n",
       " 'the',\n",
       " 'zigzagging',\n",
       " 'course',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'music',\n",
       " '.',\n",
       " 'On',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " 'Clark',\n",
       " 'harnesses',\n",
       " 'his',\n",
       " 'career',\n",
       " 's',\n",
       " 'wild',\n",
       " 'atmospheric',\n",
       " 'extremes',\n",
       " 'it',\n",
       " 's',\n",
       " 'as',\n",
       " 'though',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'he',\n",
       " 'truly',\n",
       " 'felt',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'in',\n",
       " 'his',\n",
       " 'bones',\n",
       " '.',\n",
       " 'All',\n",
       " 'products',\n",
       " 'featured',\n",
       " 'on',\n",
       " 'Pitchfork',\n",
       " 'are',\n",
       " 'independently',\n",
       " 'selected',\n",
       " 'by',\n",
       " 'our',\n",
       " 'editors',\n",
       " '.',\n",
       " 'However',\n",
       " 'when',\n",
       " 'you',\n",
       " 'buy',\n",
       " 'something',\n",
       " 'through',\n",
       " 'our',\n",
       " 'retail',\n",
       " 'links',\n",
       " 'we',\n",
       " 'may',\n",
       " 'earn',\n",
       " 'an',\n",
       " 'affiliate',\n",
       " 'commission',\n",
       " '.',\n",
       " 'Clark',\n",
       " 'Sus',\n",
       " 'Dog',\n",
       " '35',\n",
       " 'at',\n",
       " 'Rough',\n",
       " 'Trade',\n",
       " '33',\n",
       " 'at',\n",
       " 'Amazon']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = []\n",
    "\n",
    "for paragraph in content:\n",
    "    words = re.findall(regex, paragraph)\n",
    "    document.extend(words)     # note that append creates a list per paragraph. Extend stays in the same list.\n",
    "    \n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Like': ['the'],\n",
       "             'the': ['shifting',\n",
       "              'few',\n",
       "              'density',\n",
       "              'explosive',\n",
       "              'drama',\n",
       "              'margins',\n",
       "              'musician',\n",
       "              'vocal',\n",
       "              'garbled',\n",
       "              'mix',\n",
       "              'loveliest',\n",
       "              'chaos',\n",
       "              'track',\n",
       "              'red',\n",
       "              'exception',\n",
       "              'tutelage',\n",
       "              'similarity',\n",
       "              'two',\n",
       "              'lower',\n",
       "              'sheer',\n",
       "              'bridge',\n",
       "              'shapes',\n",
       "              'content',\n",
       "              'songwriting',\n",
       "              'mist',\n",
       "              'record',\n",
       "              'title',\n",
       "              'song',\n",
       "              'leap',\n",
       "              'unknowability',\n",
       "              'virtuous',\n",
       "              'producer',\n",
       "              'shortcoming',\n",
       "              'fire',\n",
       "              'strain',\n",
       "              'closing',\n",
       "              'zigzagging',\n",
       "              'first',\n",
       "              'weather'],\n",
       "             'shifting': ['atmosphere'],\n",
       "             'atmosphere': ['of'],\n",
       "             'of': ['a',\n",
       "              'twinkling',\n",
       "              'the',\n",
       "              'his',\n",
       "              'soundtracks',\n",
       "              'his',\n",
       "              'three',\n",
       "              'Clark',\n",
       "              'the',\n",
       "              'alternately',\n",
       "              'the',\n",
       "              'delicate',\n",
       "              'Suicide',\n",
       "              'Arca',\n",
       "              'Thom',\n",
       "              'his',\n",
       "              'Bully',\n",
       "              'his',\n",
       "              'Clark',\n",
       "              'air',\n",
       "              'the',\n",
       "              'the',\n",
       "              'the',\n",
       "              'despair',\n",
       "              'text',\n",
       "              'human',\n",
       "              'unanswerable',\n",
       "              'the',\n",
       "              'expressing',\n",
       "              'navigating',\n",
       "              'his'],\n",
       "             'a': ['distant',\n",
       "              'reassuring',\n",
       "              'diamond',\n",
       "              'career',\n",
       "              'growing',\n",
       "              'graceful',\n",
       "              'more',\n",
       "              'song',\n",
       "              'bed',\n",
       "              'blaring',\n",
       "              'more',\n",
       "              'quality',\n",
       "              'particularly',\n",
       "              'tinny',\n",
       "              'cappella',\n",
       "              'hurt',\n",
       "              'bright',\n",
       "              'mournful',\n",
       "              'detuned',\n",
       "              'bruise',\n",
       "              'lyricist',\n",
       "              'clear',\n",
       "              'song',\n",
       "              'reference',\n",
       "              'Zen',\n",
       "              'new',\n",
       "              'ladder',\n",
       "              'bleakly'],\n",
       "             'distant': ['planet'],\n",
       "             'planet': ['Chris'],\n",
       "             'Chris': ['Clark'],\n",
       "             'Clark': ['s',\n",
       "              'dazzles',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              'has',\n",
       "              'glides',\n",
       "              'under',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              's',\n",
       "              'has',\n",
       "              'harnesses',\n",
       "              'Sus'],\n",
       "             's': ['music',\n",
       "              'delight',\n",
       "              'work',\n",
       "              'production',\n",
       "              'fragile',\n",
       "              'experiments',\n",
       "              'Death',\n",
       "              'apocalyptic',\n",
       "              'Totems',\n",
       "              'ever',\n",
       "              'mentorship',\n",
       "              'voice',\n",
       "              'most',\n",
       "              'vocal',\n",
       "              'singing',\n",
       "              'most',\n",
       "              'emotional',\n",
       "              'swelling',\n",
       "              'production',\n",
       "              'most',\n",
       "              'all',\n",
       "              'best',\n",
       "              'wild',\n",
       "              'as'],\n",
       "             'music': ['is', '.', 'and', 'he', 'box', '.'],\n",
       "             'is': ['subject',\n",
       "              'warm',\n",
       "              'an',\n",
       "              'tearing',\n",
       "              'limited',\n",
       "              'almost',\n",
       "              'so',\n",
       "              'that',\n",
       "              'full',\n",
       "              'before',\n",
       "              'also'],\n",
       "             'subject': ['to'],\n",
       "             'to': ['violent',\n",
       "              'off',\n",
       "              'hushed',\n",
       "              'the',\n",
       "              'record',\n",
       "              'his',\n",
       "              'that',\n",
       "              'accommodate',\n",
       "              'nudge',\n",
       "              'their',\n",
       "              'Radiohead',\n",
       "              'believe',\n",
       "              'rival',\n",
       "              'conventional',\n",
       "              'the',\n",
       "              'the',\n",
       "              'guide'],\n",
       "             'violent': ['extremes'],\n",
       "             'extremes': ['.', 'it'],\n",
       "             '.': ['With',\n",
       "              'The',\n",
       "              'At',\n",
       "              'But',\n",
       "              'In',\n",
       "              'The',\n",
       "              'Executive',\n",
       "              'Historically',\n",
       "              'By',\n",
       "              'Here',\n",
       "              'Working',\n",
       "              'Clark',\n",
       "              'Clutch',\n",
       "              'With',\n",
       "              'At',\n",
       "              'Clark',\n",
       "              'Apart',\n",
       "              'The',\n",
       "              'He',\n",
       "              'Forest',\n",
       "              'In',\n",
       "              'Because',\n",
       "              'But',\n",
       "              'Alyosha',\n",
       "              'On',\n",
       "              'And',\n",
       "              'For',\n",
       "              'Sus',\n",
       "              'On',\n",
       "              'On',\n",
       "              'All',\n",
       "              'However',\n",
       "              'Clark'],\n",
       "             'With': ['little', 'the'],\n",
       "             'little': ['warning'],\n",
       "             'warning': ['a'],\n",
       "             'reassuring': ['beat'],\n",
       "             'beat': ['might'],\n",
       "             'might': ['furiously', 'think', 'appear', 'be'],\n",
       "             'furiously': ['morph'],\n",
       "             'morph': ['into'],\n",
       "             'into': ['an',\n",
       "              'a',\n",
       "              'gear',\n",
       "              'the',\n",
       "              'Town',\n",
       "              'separate',\n",
       "              'an',\n",
       "              'drive'],\n",
       "             'an': ['instrumental',\n",
       "              'ear',\n",
       "              'ornate',\n",
       "              'ultra',\n",
       "              'octave',\n",
       "              'aching',\n",
       "              'affiliate'],\n",
       "             'instrumental': ['storm', 'until'],\n",
       "             'storm': ['breaking'],\n",
       "             'breaking': ['just'],\n",
       "             'just': ['as'],\n",
       "             'as': ['suddenly',\n",
       "              'he',\n",
       "              'a',\n",
       "              'the',\n",
       "              'much',\n",
       "              'the',\n",
       "              'a',\n",
       "              'a',\n",
       "              'cynical',\n",
       "              'they',\n",
       "              'though'],\n",
       "             'suddenly': ['into'],\n",
       "             'diamond': ['rain'],\n",
       "             'rain': ['of'],\n",
       "             'twinkling': ['synths'],\n",
       "             'synths': ['.', 'interwoven'],\n",
       "             'The': ['British', 'architecture', 'power', 'Brothers'],\n",
       "             'British': ['musician'],\n",
       "             'musician': ['s', 's'],\n",
       "             'delight': ['in'],\n",
       "             'in': ['wrong',\n",
       "              'a',\n",
       "              'contrast',\n",
       "              'the',\n",
       "              'traffic',\n",
       "              'a',\n",
       "              'a',\n",
       "              'making',\n",
       "              'comparison',\n",
       "              'turn',\n",
       "              'leveraging',\n",
       "              'his'],\n",
       "             'wrong': ['footing'],\n",
       "             'footing': ['expectations'],\n",
       "             'expectations': ['has'],\n",
       "             'has': ['been',\n",
       "              'swerved',\n",
       "              'also',\n",
       "              'amassed',\n",
       "              'developed',\n",
       "              'both',\n",
       "              'never',\n",
       "              'largely',\n",
       "              'learned',\n",
       "              'had',\n",
       "              'toggled',\n",
       "              'discovered'],\n",
       "             'been': ['one'],\n",
       "             'one': ['of', 'that', 'instrument', 'of', 'risk', 'of'],\n",
       "             'few': ['constants'],\n",
       "             'constants': ['in'],\n",
       "             'career': ['that', 's'],\n",
       "             'that': ['has',\n",
       "              'have',\n",
       "              'has',\n",
       "              'record',\n",
       "              'Yorke',\n",
       "              'Yorke',\n",
       "              'he',\n",
       "              'his',\n",
       "              'pit',\n",
       "              'his',\n",
       "              'approaches',\n",
       "              'are',\n",
       "              'it'],\n",
       "             'swerved': ['wildly'],\n",
       "             'wildly': ['from'],\n",
       "             'from': ['tricky',\n",
       "              'blistering',\n",
       "              'record',\n",
       "              'the',\n",
       "              'the',\n",
       "              'a',\n",
       "              'odd',\n",
       "              'The'],\n",
       "             'tricky': ['IDM'],\n",
       "             'IDM': ['to'],\n",
       "             'off': ['kilter', 'in', 'kilter'],\n",
       "             'kilter': ['hip', 'pitched'],\n",
       "             'hip': ['hop'],\n",
       "             'hop': ['beats'],\n",
       "             'beats': ['and', 'using'],\n",
       "             'and': ['from',\n",
       "              'dynamism',\n",
       "              'TV',\n",
       "              'accentuated',\n",
       "              'immediately',\n",
       "              'sometimes',\n",
       "              'menacing',\n",
       "              'chanted',\n",
       "              'lightspeed',\n",
       "              'piercing',\n",
       "              'a',\n",
       "              'merciless',\n",
       "              'dismissive',\n",
       "              'brutality',\n",
       "              'otherworldly'],\n",
       "             'blistering': ['techno', 'noise'],\n",
       "             'techno': ['to', 'refrains'],\n",
       "             'hushed': ['minimalism'],\n",
       "             'minimalism': ['.'],\n",
       "             'At': ['his', 'first'],\n",
       "             'his': ['propulsive',\n",
       "              'music',\n",
       "              'music',\n",
       "              'latest',\n",
       "              'voice',\n",
       "              'intricate',\n",
       "              'voice',\n",
       "              'beats',\n",
       "              'high',\n",
       "              'voice',\n",
       "              'production',\n",
       "              'mentor',\n",
       "              'voice',\n",
       "              'voice',\n",
       "              'words',\n",
       "              'approach',\n",
       "              'voice',\n",
       "              'maze',\n",
       "              'own',\n",
       "              'career',\n",
       "              'bones'],\n",
       "             'propulsive': ['best'],\n",
       "             'best': ['Clark', 'work'],\n",
       "             'dazzles': ['with'],\n",
       "             'with': ['both',\n",
       "              'voice',\n",
       "              'a',\n",
       "              'acoustic',\n",
       "              'some',\n",
       "              'Bjrk',\n",
       "              'the',\n",
       "              'a'],\n",
       "             'both': ['the', 'heightened'],\n",
       "             'density': ['and'],\n",
       "             'dynamism': ['of'],\n",
       "             'But': ['in', 'his'],\n",
       "             'contrast': ['to'],\n",
       "             'explosive': ['changes'],\n",
       "             'changes': ['that'],\n",
       "             'have': ['taken', 'yielded', 'aged'],\n",
       "             'taken': ['place'],\n",
       "             'place': ['from'],\n",
       "             'record': ['to', 'Clark', 'Sus', 's', 's'],\n",
       "             'work': ['has', 'has'],\n",
       "             'also': ['undergone', 'situated', 'of'],\n",
       "             'undergone': ['another'],\n",
       "             'another': ['more'],\n",
       "             'more': ['subtle', 'fluid', 'limited', 'reliable', 'than'],\n",
       "             'subtle': ['evolution'],\n",
       "             'evolution': ['.'],\n",
       "             'In': ['recent', 'one', 'fact'],\n",
       "             'recent': ['years'],\n",
       "             'years': ['as', 'Clark'],\n",
       "             'he': ['has',\n",
       "              'has',\n",
       "              'foregrounds',\n",
       "              'has',\n",
       "              's',\n",
       "              'surfs',\n",
       "              'more',\n",
       "              'sighs',\n",
       "              'largely',\n",
       "              'leaps',\n",
       "              'achieves',\n",
       "              'croons',\n",
       "              'sings',\n",
       "              'truly'],\n",
       "             'amassed': ['a'],\n",
       "             'growing': ['body'],\n",
       "             'body': ['of'],\n",
       "             'soundtracks': ['for'],\n",
       "             'for': ['film',\n",
       "              'emptiness',\n",
       "              'with',\n",
       "              'understanding',\n",
       "              'the',\n",
       "              'the',\n",
       "              'the'],\n",
       "             'film': ['and'],\n",
       "             'TV': ['he'],\n",
       "             'developed': ['an'],\n",
       "             'ear': ['for'],\n",
       "             'emptiness': ['one'],\n",
       "             'heightened': ['the'],\n",
       "             'drama': ['of'],\n",
       "             'accentuated': ['its'],\n",
       "             'its': ['suggestion', 'pent'],\n",
       "             'suggestion': ['of'],\n",
       "             'three': ['dimensional'],\n",
       "             'dimensional': ['space'],\n",
       "             'space': ['.'],\n",
       "             'architecture': ['of'],\n",
       "             'production': ['has', '.', '.', 'is', 'but'],\n",
       "             'never': ['sounded'],\n",
       "             'sounded': ['airier'],\n",
       "             'airier': ['or'],\n",
       "             'or': ['more'],\n",
       "             'fluid': ['than'],\n",
       "             'than': ['it', 'slapping', 'Clark', 'makes'],\n",
       "             'it': ['does', '.', 'is', 's'],\n",
       "             'does': ['on'],\n",
       "             'on': ['his',\n",
       "              '2017',\n",
       "              '2009',\n",
       "              'top',\n",
       "              'its',\n",
       "              'Town',\n",
       "              'Medicine',\n",
       "              'It',\n",
       "              'a',\n",
       "              'Pitchfork'],\n",
       "             'latest': ['record'],\n",
       "             'Sus': ['Dog', 'Dog', 'Dog', 'Dog', 'Dog', 'Dog'],\n",
       "             'Dog': ['where', 'is', 'is', 'is', 'Clark', '35'],\n",
       "             'where': ['he', 'he'],\n",
       "             'foregrounds': ['one'],\n",
       "             'instrument': ['he'],\n",
       "             'largely': ['left', 'forgoes'],\n",
       "             'left': ['in'],\n",
       "             'margins': ['his'],\n",
       "             'voice': ['.',\n",
       "              'have',\n",
       "              'on',\n",
       "              'to',\n",
       "              'rising',\n",
       "              'while',\n",
       "              'makes',\n",
       "              'cleaves',\n",
       "              'like'],\n",
       "             'Executive': ['produced'],\n",
       "             'produced': ['by'],\n",
       "             'by': ['Thom', 'your', 'one', 'our'],\n",
       "             'Thom': ['Yorke', 'Yorke'],\n",
       "             'Yorke': ['Sus', '.', 'himself', 'brings'],\n",
       "             'warm': ['and'],\n",
       "             'immediately': ['gratifying'],\n",
       "             'gratifying': ['offering'],\n",
       "             'offering': ['the'],\n",
       "             'fragile': ['falsetto'],\n",
       "             'falsetto': ['as'],\n",
       "             'graceful': ['counterpoint'],\n",
       "             'counterpoint': ['to'],\n",
       "             'intricate': ['and'],\n",
       "             'sometimes': ['breakneck'],\n",
       "             'breakneck': ['production'],\n",
       "             'Historically': ['Clark'],\n",
       "             'experiments': ['with'],\n",
       "             'yielded': ['mixed'],\n",
       "             'mixed': ['results'],\n",
       "             'results': ['.'],\n",
       "             'By': ['turns'],\n",
       "             'turns': ['angelic'],\n",
       "             'angelic': ['and'],\n",
       "             'menacing': ['the'],\n",
       "             'vocal': ['accents', 'tics'],\n",
       "             'accents': ['on'],\n",
       "             '2017': ['s'],\n",
       "             'Death': ['Peak'],\n",
       "             'Peak': ['are'],\n",
       "             'are': ['crucial', 'met', 'independently'],\n",
       "             'crucial': ['to'],\n",
       "             'apocalyptic': ['appeal'],\n",
       "             'appeal': ['while'],\n",
       "             'while': ['the', 'on', 'handsome'],\n",
       "             'garbled': ['ultra'],\n",
       "             'ultra': ['processed', 'Yorkean'],\n",
       "             'processed': ['growls'],\n",
       "             'growls': ['and'],\n",
       "             'chanted': ['raps'],\n",
       "             'raps': ['on'],\n",
       "             '2009': ['s'],\n",
       "             'Totems': ['Flare'],\n",
       "             'Flare': ['have'],\n",
       "             'aged': ['poorly'],\n",
       "             'poorly': ['.'],\n",
       "             'Here': ['rather'],\n",
       "             'rather': ['than'],\n",
       "             'slapping': ['his'],\n",
       "             'top': ['of'],\n",
       "             'mix': ['Clark'],\n",
       "             'learned': ['to'],\n",
       "             'accommodate': ['it'],\n",
       "             'Working': ['with'],\n",
       "             'limited': ['palette', 'to'],\n",
       "             'palette': ['of'],\n",
       "             'alternately': ['boxy'],\n",
       "             'boxy': ['and'],\n",
       "             'lightspeed': ['synths'],\n",
       "             'interwoven': ['with'],\n",
       "             'acoustic': ['instruments'],\n",
       "             'instruments': ['Sus'],\n",
       "             'ornate': ['but'],\n",
       "             'but': ['fleet', 'the', 'highly', 'passive', 'in', 'also'],\n",
       "             'fleet': ['footed'],\n",
       "             'footed': ['synth'],\n",
       "             'synth': ['pop', 'pulse', 'bleeds'],\n",
       "             'pop': ['album'],\n",
       "             'album': ['brimming'],\n",
       "             'brimming': ['with'],\n",
       "             'some': ['of'],\n",
       "             'loveliest': ['music'],\n",
       "             'ever': ['made'],\n",
       "             'made': ['.'],\n",
       "             'glides': ['over'],\n",
       "             'over': ['his', 'a', 'the', 'mournful'],\n",
       "             'using': ['his'],\n",
       "             'high': ['plaintive'],\n",
       "             'plaintive': ['voice'],\n",
       "             'nudge': ['a'],\n",
       "             'song': ['into', 's', 'whose'],\n",
       "             'gear': ['before'],\n",
       "             'before': ['soaring', 'his', 'transforming'],\n",
       "             'soaring': ['on'],\n",
       "             'pent': ['up'],\n",
       "             'up': ['momentum', 'for'],\n",
       "             'momentum': ['.'],\n",
       "             'Clutch': ['Pearlers'],\n",
       "             'Pearlers': ['levitates'],\n",
       "             'levitates': ['over'],\n",
       "             'bed': ['of'],\n",
       "             'delicate': ['music'],\n",
       "             'box': ['plucks'],\n",
       "             'plucks': ['while'],\n",
       "             'Town': ['Crank', 'Crank'],\n",
       "             'Crank': ['he', 'but'],\n",
       "             'surfs': ['a'],\n",
       "             'blaring': ['synth'],\n",
       "             'pulse': ['reminiscent'],\n",
       "             'reminiscent': ['of'],\n",
       "             'Suicide': ['at'],\n",
       "             'at': ['their', 'the', 'Rough', 'Amazon'],\n",
       "             'their': ['most', 'beatific'],\n",
       "             'most': ['antagonistic', 'emotive', 'striking', 'scorching'],\n",
       "             'antagonistic': ['his'],\n",
       "             'rising': ['above'],\n",
       "             'above': ['the'],\n",
       "             'chaos': ['as'],\n",
       "             'track': ['veers', 's'],\n",
       "             'veers': ['into'],\n",
       "             'red': ['.'],\n",
       "             'exception': ['of'],\n",
       "             'Arca': ['s'],\n",
       "             'mentorship': ['with'],\n",
       "             'Bjrk': ['no'],\n",
       "             'no': ['electronic'],\n",
       "             'electronic': ['producer'],\n",
       "             'producer': ['has', 's'],\n",
       "             'had': ['a'],\n",
       "             'reliable': ['singing'],\n",
       "             'singing': ['coach', 'derives'],\n",
       "             'coach': ['than'],\n",
       "             'under': ['the'],\n",
       "             'tutelage': ['of'],\n",
       "             'first': ['you', 'time'],\n",
       "             'you': ['might', 'buy'],\n",
       "             'think': ['that'],\n",
       "             'himself': ['is'],\n",
       "             'tearing': ['into'],\n",
       "             'similarity': ['between'],\n",
       "             'between': ['the', 'vague', 'beauty', 'those', 'two'],\n",
       "             'two': ['men', 'poles', 'floors'],\n",
       "             'men': ['is'],\n",
       "             'beatific': ['falsettos'],\n",
       "             'falsettos': ['.'],\n",
       "             'handsome': ['lacks'],\n",
       "             'lacks': ['the'],\n",
       "             'lower': ['range'],\n",
       "             'range': ['and'],\n",
       "             'piercing': ['corroded'],\n",
       "             'corroded': ['edge'],\n",
       "             'edge': ['that'],\n",
       "             'brings': ['to'],\n",
       "             'Radiohead': ['s'],\n",
       "             'emotive': ['tracks'],\n",
       "             'tracks': ['a'],\n",
       "             'quality': ['that'],\n",
       "             'makes': ['up', 'out'],\n",
       "             'sheer': ['violence'],\n",
       "             'violence': ['of'],\n",
       "             'Apart': ['from'],\n",
       "             'bridge': ['of'],\n",
       "             'Bully': ['where'],\n",
       "             'sighs': ['an'],\n",
       "             'Yorkean': ['line', 'way'],\n",
       "             'line': ['Drift'],\n",
       "             'Drift': ['off'],\n",
       "             'traffic': ['Colonized'],\n",
       "             'Colonized': ['by'],\n",
       "             'your': ['phone'],\n",
       "             'phone': ['in'],\n",
       "             'particularly': ['Yorkean'],\n",
       "             'way': ['he', 'to'],\n",
       "             'forgoes': ['replicating'],\n",
       "             'replicating': ['any'],\n",
       "             'any': ['of'],\n",
       "             'mentor': ['s'],\n",
       "             'tics': ['even'],\n",
       "             'even': ['when'],\n",
       "             'when': ['they', 'you'],\n",
       "             'they': ['harmonize', 'can', 'like'],\n",
       "             'harmonize': ['together'],\n",
       "             'together': ['on'],\n",
       "             'Medicine': ['.'],\n",
       "             'power': ['of'],\n",
       "             'derives': ['from'],\n",
       "             'shapes': ['that'],\n",
       "             'out': ['of', 'of'],\n",
       "             'air': ['as'],\n",
       "             'much': ['as'],\n",
       "             'content': ['of'],\n",
       "             'songwriting': ['itself', 'is'],\n",
       "             'itself': ['.', 'could'],\n",
       "             'He': ['opens'],\n",
       "             'opens': ['Alyosha'],\n",
       "             'Alyosha': ['with', 'a'],\n",
       "             'tinny': ['a'],\n",
       "             'cappella': ['refrain'],\n",
       "             'refrain': ['repeating'],\n",
       "             'repeating': ['I'],\n",
       "             'I': ['want'],\n",
       "             'want': ['to'],\n",
       "             'believe': ['in'],\n",
       "             'hurt': ['tone'],\n",
       "             'tone': ['before'],\n",
       "             'cleaves': ['into'],\n",
       "             'separate': ['spheres'],\n",
       "             'spheres': ['that'],\n",
       "             'pit': ['mature'],\n",
       "             'mature': ['practicality'],\n",
       "             'practicality': ['against'],\n",
       "             'against': ['raw'],\n",
       "             'raw': ['adolescent'],\n",
       "             'adolescent': ['distrust'],\n",
       "             'distrust': ['.'],\n",
       "             'Forest': ['is'],\n",
       "             'almost': ['entirely'],\n",
       "             'entirely': ['instrumental'],\n",
       "             'until': ['a'],\n",
       "             'bright': ['multi'],\n",
       "             'multi': ['tracked'],\n",
       "             'tracked': ['chorus'],\n",
       "             'chorus': ['to'],\n",
       "             'rival': ['Fleet'],\n",
       "             'Fleet': ['Foxes'],\n",
       "             'Foxes': ['rises'],\n",
       "             'rises': ['brilliantly'],\n",
       "             'brilliantly': ['out'],\n",
       "             'mist': ['.'],\n",
       "             'striking': ['moments'],\n",
       "             'moments': ['at'],\n",
       "             'title': ['track', 'might'],\n",
       "             'emotional': ['nadir'],\n",
       "             'nadir': ['he'],\n",
       "             'leaps': ['an'],\n",
       "             'octave': ['from'],\n",
       "             'mournful': ['croon', 'piano'],\n",
       "             'croon': ['into'],\n",
       "             'aching': ['note'],\n",
       "             'note': ['of'],\n",
       "             'despair': ['as'],\n",
       "             'detuned': ['synth'],\n",
       "             'bleeds': ['over'],\n",
       "             'swelling': ['acoustics'],\n",
       "             'acoustics': ['like'],\n",
       "             'like': ['a', 'clarity', 'he', 'this', 'production'],\n",
       "             'bruise': ['.'],\n",
       "             'Because': ['Clark'],\n",
       "             'so': ['finely'],\n",
       "             'finely': ['detailed'],\n",
       "             'detailed': ['one'],\n",
       "             'risk': ['in'],\n",
       "             'making': ['the'],\n",
       "             'leap': ['to'],\n",
       "             'conventional': ['songwriting'],\n",
       "             'words': ['might'],\n",
       "             'appear': ['crude'],\n",
       "             'crude': ['in'],\n",
       "             'comparison': ['.'],\n",
       "             'approach': ['as'],\n",
       "             'lyricist': ['remains'],\n",
       "             'remains': ['resolutely'],\n",
       "             'resolutely': ['off'],\n",
       "             'pitched': ['between'],\n",
       "             'vague': ['but'],\n",
       "             'highly': ['evocative'],\n",
       "             'evocative': ['ribbons'],\n",
       "             'ribbons': ['of'],\n",
       "             'text': ['and'],\n",
       "             'clear': ['eyed'],\n",
       "             'eyed': ['sensitivity'],\n",
       "             'sensitivity': ['that'],\n",
       "             'approaches': ['the'],\n",
       "             'unknowability': ['of'],\n",
       "             'human': ['behavior'],\n",
       "             'behavior': ['from'],\n",
       "             'odd': ['angles'],\n",
       "             'angles': ['.'],\n",
       "             'whose': ['title'],\n",
       "             'be': ['a', 'as', 'a'],\n",
       "             'reference': ['to'],\n",
       "             'virtuous': ['but'],\n",
       "             'passive': ['protagonist'],\n",
       "             'protagonist': ['from'],\n",
       "             'Brothers': ['Karamazov'],\n",
       "             'Karamazov': ['is'],\n",
       "             'full': ['of'],\n",
       "             'unanswerable': ['emotionally'],\n",
       "             'emotionally': ['naked'],\n",
       "             'naked': ['pleas'],\n",
       "             'pleas': ['for'],\n",
       "             'understanding': ['that'],\n",
       "             'met': ['in'],\n",
       "             'turn': ['by'],\n",
       "             'scorching': ['and'],\n",
       "             'merciless': ['techno'],\n",
       "             'refrains': ['.'],\n",
       "             'On': ['Dismissive', 'the', 'Sus'],\n",
       "             'Dismissive': ['he'],\n",
       "             'achieves': ['a'],\n",
       "             'Zen': ['like'],\n",
       "             'clarity': ['recognizing'],\n",
       "             'recognizing': ['cruelty'],\n",
       "             'cruelty': ['for'],\n",
       "             'shortcoming': ['that'],\n",
       "             'transforming': ['derision'],\n",
       "             'derision': ['into'],\n",
       "             'drive': ['.'],\n",
       "             'And': ['they'],\n",
       "             'can': ['be'],\n",
       "             'cynical': ['and'],\n",
       "             'dismissive': ['as'],\n",
       "             'croons': ['In'],\n",
       "             'fact': ['please'],\n",
       "             'please': ['carry'],\n",
       "             'carry': ['on'],\n",
       "             'It': ['s'],\n",
       "             'all': ['fuel'],\n",
       "             'fuel': ['to'],\n",
       "             'fire': ['.'],\n",
       "             'For': ['years'],\n",
       "             'toggled': ['between'],\n",
       "             'beauty': ['and'],\n",
       "             'brutality': ['blistering'],\n",
       "             'noise': ['and'],\n",
       "             'otherworldly': ['calm'],\n",
       "             'calm': ['.'],\n",
       "             'situated': ['between'],\n",
       "             'those': ['two'],\n",
       "             'poles': ['but'],\n",
       "             'leveraging': ['his'],\n",
       "             'this': ['Clark'],\n",
       "             'discovered': ['not'],\n",
       "             'not': ['only'],\n",
       "             'only': ['a'],\n",
       "             'new': ['way'],\n",
       "             'guide': ['listeners'],\n",
       "             'listeners': ['through'],\n",
       "             'through': ['his', 'our'],\n",
       "             'maze': ['like'],\n",
       "             'expressing': ['the'],\n",
       "             'strain': ['of'],\n",
       "             'navigating': ['such'],\n",
       "             'such': ['wild'],\n",
       "             'wild': ['terrain', 'atmospheric'],\n",
       "             'terrain': ['.'],\n",
       "             'closing': ['Ladder'],\n",
       "             'Ladder': ['he'],\n",
       "             'sings': ['wearily'],\n",
       "             'wearily': ['over'],\n",
       "             'piano': ['about'],\n",
       "             'about': ['living'],\n",
       "             'living': ['on'],\n",
       "             'ladder': ['stuck'],\n",
       "             'stuck': ['between'],\n",
       "             'floors': ['which'],\n",
       "             'which': ['itself'],\n",
       "             'could': ['be'],\n",
       "             'bleakly': ['beautiful'],\n",
       "             'beautiful': ['metaphor'],\n",
       "             'metaphor': ['for'],\n",
       "             'zigzagging': ['course'],\n",
       "             'course': ['of'],\n",
       "             'own': ['music'],\n",
       "             'harnesses': ['his'],\n",
       "             'atmospheric': ['extremes'],\n",
       "             'though': ['for'],\n",
       "             'time': ['he'],\n",
       "             'truly': ['felt'],\n",
       "             'felt': ['the'],\n",
       "             'weather': ['in'],\n",
       "             'bones': ['.'],\n",
       "             'All': ['products'],\n",
       "             'products': ['featured'],\n",
       "             'featured': ['on'],\n",
       "             'Pitchfork': ['are'],\n",
       "             'independently': ['selected'],\n",
       "             'selected': ['by'],\n",
       "             'our': ['editors', 'retail'],\n",
       "             'editors': ['.'],\n",
       "             'However': ['when'],\n",
       "             'buy': ['something'],\n",
       "             'something': ['through'],\n",
       "             'retail': ['links'],\n",
       "             'links': ['we'],\n",
       "             'we': ['may'],\n",
       "             'may': ['earn'],\n",
       "             'earn': ['an'],\n",
       "             'affiliate': ['commission'],\n",
       "             'commission': ['.'],\n",
       "             '35': ['at'],\n",
       "             'Rough': ['Trade'],\n",
       "             'Trade': ['33'],\n",
       "             '33': ['at']})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = defaultdict(list)\n",
    "\n",
    "for prev, current in zip(document, document[1:]):\n",
    "    transitions[prev].append(current)\n",
    "\n",
    "transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In fact please carry on 2017 s all fuel to violent extremes it is tearing into an octave from The architecture of the virtuous but also of expressing the two floors which itself .'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram in gensim\n",
    "\n",
    "The book actually does something else than gensim's phrases. It's a Markov chain. **There are no good packages for this.**\n",
    "\n",
    "A bigram model finds set expressions and puts underscores between them:\n",
    "\n",
    "    The `Phrases` model in Gensim is a simple and efficient way to handle the transformation of individual words into bigrams (two consecutive words) or even larger n-grams, based on the frequency of word co-occurrence.\n",
    "\n",
    "    The idea behind `Phrases` is that if two words are often found together in the text, they might be a meaningful phrase and it might be useful to treat them as a single entity. For example, \"New York\" is a bigram that represents a single entity, and \"machine learning\" might be a meaningful bigram in a text about data science.\n",
    "\n",
    "    When you train a `Phrases` model on your text, it counts the frequency of each individual word and each pair of two consecutive words. If the frequency of the pair is significantly higher than would be expected based on the individual frequencies, the pair is considered to be a phrase.\n",
    "\n",
    "    After training, you can use the `Phrases` model to transform any text into a text where these phrases are joined together with an underscore, effectively treating them as single words.\n",
    "\n",
    "    For example, if you have the sentence \"I live in New York and I work in machine learning\", and \"New York\" and \"machine learning\" are recognized as phrases, the transformed sentence would be \"I live in New_York and I work in machine_learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working_class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans_culottes', 'of', 'the', 'french_revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative_way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken_up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived_from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political_philosophy', 'is', 'the', 'belief_that', 'rulers', 'are', 'unnecessary', 'and', 'should_be', 'abolished', 'although', 'there_are', 'differing_interpretations', 'of', 'what', 'this', 'means', 'anarchism', 'also', 'refers_to', 'related', 'social_movements', 'that', 'advocate', 'the', 'elimination', 'of', 'authoritarian', 'institutions', 'particularly', 'the', 'state', 'the', 'word', 'anarchy', 'as', 'most', 'anarchists', 'use', 'it', 'does_not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but_rather', 'a', 'harmonious', 'anti_authoritarian', 'society', 'in', 'place', 'of', 'what', 'are', 'regarded_as', 'authoritarian', 'political', 'structures', 'and', 'coercive', 'economic', 'institutions', 'anarchists', 'advocate', 'social', 'relations', 'based_upon', 'voluntary_association', 'of', 'autonomous', 'individuals', 'mutual_aid', 'and', 'self_governance', 'while', 'anarchism', 'is', 'most', 'easily', 'defined', 'by', 'what', 'it', 'is', 'against', 'anarchists', 'also', 'offer', 'positive', 'visions', 'of', 'what', 'they_believe', 'to', 'be', 'a', 'truly', 'free', 'society', 'however', 'ideas_about', 'how', 'an', 'anarchist_society', 'might', 'work', 'vary_considerably', 'especially', 'with_respect', 'to', 'economics', 'there', 'is', 'also', 'disagreement_about', 'how', 'a', 'free', 'society', 'might_be', 'brought_about', 'origins', 'and', 'predecessors', 'kropotkin', 'and', 'others_argue', 'that', 'before', 'recorded_history', 'human', 'society', 'was', 'organized', 'on', 'anarchist_principles', 'most', 'anthropologists', 'follow', 'kropotkin', 'and', 'engels', 'in', 'believing_that', 'hunter_gatherer', 'bands', 'were', 'egalitarian', 'and', 'lacked', 'division', 'of', 'labour', 'accumulated_wealth', 'or', 'decreed', 'law', 'and', 'had', 'equal', 'access', 'to', 'resources', 'william_godwin', 'anarchists', 'including', 'the', 'the', 'anarchy', 'organisation', 'and', 'rothbard', 'find', 'anarchist', 'attitudes', 'in', 'taoism', 'from', 'ancient', 'china', 'kropotkin', 'found', 'similar', 'ideas', 'in', 'stoic', 'zeno', 'of', 'citium', 'according_to', 'kropotkin', 'zeno', 'repudiated', 'the', 'omnipotence', 'of', 'the', 'state', 'its', 'intervention', 'and', 'regimentation', 'and', 'proclaimed', 'the', 'sovereignty', 'of', 'the', 'moral', 'law', 'of', 'the', 'individual', 'the', 'anabaptists', 'of', 'one', 'six', 'th_century', 'europe', 'are', 'sometimes_considered', 'to', 'be', 'religious', 'forerunners', 'of', 'modern', 'anarchism', 'bertrand_russell', 'in', 'his', 'history', 'of', 'western_philosophy', 'writes', 'that', 'the', 'anabaptists', 'repudiated', 'all', 'law', 'since', 'they', 'held', 'that', 'the', 'good', 'man', 'will_be', 'guided', 'at', 'every', 'moment', 'by', 'the', 'holy_spirit', 'from', 'this', 'premise', 'they', 'arrive_at', 'communism', 'the', 'diggers', 'or', 'true', 'levellers', 'were', 'an', 'early', 'communistic', 'movement', 'during', 'the', 'time', 'of', 'the', 'english_civil', 'war', 'and', 'are', 'considered', 'by', 'some', 'as', 'forerunners', 'of', 'modern', 'anarchism', 'in', 'the', 'modern_era', 'the', 'first', 'to', 'use', 'the', 'term', 'to', 'mean', 'something', 'other', 'than', 'chaos', 'was', 'louis', 'armand', 'baron_de', 'lahontan', 'in', 'his', 'nouveaux', 'voyages', 'dans_l', 'am_rique', 'septentrionale', 'one', 'seven', 'zero', 'three', 'where', 'he', 'described', 'the', 'indigenous', 'american', 'society', 'which', 'had', 'no', 'state', 'laws', 'prisons', 'priests', 'or', 'private_property', 'as', 'being', 'in', 'anarchy', 'russell', 'means', 'a', 'libertarian', 'and', 'leader', 'in', 'the', 'american', 'indian', 'movement', 'has', 'repeatedly_stated', 'that', 'he', 'is', 'an', 'anarchist', 'and', 'so', 'are', 'all', 'his', 'ancestors', 'in', 'one', 'seven', 'nine', 'three', 'in', 'the', 'thick', 'of', 'the', 'french_revolution', 'william_godwin', 'published', 'an_enquiry', 'concerning', 'political', 'justice', 'although', 'godwin', 'did_not', 'use', 'the', 'word', 'anarchism', 'many', 'later', 'anarchists', 'have', 'regarded', 'this', 'book', 'as', 'the', 'first', 'major', 'anarchist', 'text', 'and', 'godwin', 'as', 'the', 'founder', 'of', 'philosophical_anarchism', 'but', 'at', 'this', 'point', 'no', 'anarchist_movement', 'yet', 'existed', 'and', 'the', 'term', 'anarchiste', 'was', 'known', 'mainly', 'as', 'an_insult', 'hurled', 'by', 'the', 'bourgeois', 'girondins', 'at', 'more_radical', 'elements', 'in', 'the', 'french_revolution', 'the', 'first', 'self', 'labelled', 'anarchist', 'pierre_joseph', 'proudhon', 'it', 'is', 'commonly', 'held', 'that', 'it_wasn', 't', 'until', 'pierre_joseph', 'proudhon', 'published', 'what', 'is', 'property', 'in', 'one', 'eight', 'four', 'zero', 'that', 'the', 'term', 'anarchist', 'was', 'adopted', 'as', 'a', 'self', 'description', 'it', 'is', 'for', 'this_reason', 'that', 'some', 'claim', 'proudhon', 'as', 'the', 'founder', 'of', 'modern', 'anarchist', 'theory', 'in', 'what', 'is', 'property', 'proudhon', 'answers', 'with', 'the', 'famous', 'accusation', 'property', 'is', 'theft', 'in', 'this', 'work', 'he', 'opposed', 'the', 'institution', 'of', 'decreed', 'property', 'propri_t', 'where', 'owners', 'have', 'complete', 'rights', 'to', 'use', 'and', 'abuse', 'their', 'property', 'as', 'they_wish', 'such_as', 'exploiting', 'workers', 'for', 'profit', 'in', 'its', 'place', 'proudhon', 'supported', 'what', 'he', 'called', 'possession', 'individuals', 'can', 'have', 'limited', 'rights', 'to', 'use', 'resources', 'capital', 'and', 'goods', 'in_accordance', 'with', 'principles', 'of', 'equality', 'and', 'justice', 'proudhon', 's', 'vision', 'of', 'anarchy', 'which', 'he', 'called', 'mutualism', 'mutuellisme', 'involved', 'an', 'exchange', 'economy', 'where', 'individuals', 'and', 'groups', 'could', 'trade', 'the', 'products', 'of', 'their', 'labor', 'using', 'labor', 'notes', 'which', 'represented', 'the', 'amount', 'of', 'working', 'time', 'involved', 'in', 'production', 'this', 'would', 'ensure_that', 'no', 'one', 'would', 'profit', 'from', 'the', 'labor', 'of', 'others', 'workers', 'could', 'freely', 'join_together', 'in', 'co_operative', 'workshops', 'an', 'interest', 'free', 'bank', 'would_be', 'set_up', 'to', 'provide', 'everyone', 'with', 'access', 'to', 'the', 'means', 'of', 'production', 'proudhon', 's', 'ideas', 'were', 'influential', 'within', 'french', 'working_class', 'movements', 'and', 'his_followers', 'were', 'active', 'in', 'the', 'revolution', 'of', 'one', 'eight', 'four', 'eight', 'in', 'france', 'proudhon', 's', 'philosophy', 'of', 'property', 'is', 'complex', 'it', 'was', 'developed', 'in', 'a', 'number', 'of', 'works', 'over', 'his_lifetime', 'and', 'there_are', 'differing_interpretations', 'of', 'some', 'of', 'his', 'ideas', 'for', 'more_detailed', 'discussion', 'see', 'here', 'max_stirner', 's', 'egoism', 'in', 'his', 'the', 'ego', 'and', 'its_own', 'stirner', 'argued_that', 'most_commonly', 'accepted', 'social_institutions', 'including', 'the', 'notion', 'of', 'state', 'property', 'as', 'a', 'right', 'natural', 'rights', 'in', 'general', 'and', 'the', 'very', 'notion', 'of', 'society', 'were', 'mere', 'illusions', 'or', 'ghosts', 'in', 'the', 'mind', 'saying', 'of', 'society', 'that', 'the', 'individuals', 'are', 'its', 'reality', 'he', 'advocated', 'egoism', 'and', 'a', 'form', 'of', 'amoralism', 'in', 'which', 'individuals', 'would', 'unite', 'in', 'associations', 'of', 'egoists', 'only', 'when', 'it', 'was', 'in', 'their', 'self_interest', 'to', 'do_so', 'for', 'him', 'property', 'simply', 'comes', 'about', 'through', 'might', 'whoever', 'knows_how', 'to', 'take', 'to', 'defend', 'the', 'thing', 'to', 'him', 'belongs', 'property', 'and', 'what', 'i', 'have', 'in', 'my', 'power', 'that', 'is', 'my_own', 'so', 'long', 'as', 'i', 'assert', 'myself', 'as', 'holder', 'i_am', 'the', 'proprietor', 'of', 'the', 'thing', 'stirner', 'never', 'called', 'himself', 'an', 'anarchist', 'he', 'accepted', 'only', 'the', 'label', 'egoist', 'nevertheless', 'his', 'ideas', 'were', 'influential', 'on', 'many', 'individualistically', 'inclined', 'anarchists', 'although', 'interpretations', 'of', 'his', 'thought', 'are', 'diverse', 'american_individualist', 'anarchism', 'benjamin_tucker', 'in', 'one', 'eight', 'two', 'five', 'josiah_warren', 'had', 'participated', 'in', 'a', 'communitarian', 'experiment', 'headed_by', 'robert_owen', 'called', 'new', 'harmony', 'which', 'failed', 'in', 'a', 'few_years', 'amidst', 'much', 'internal_conflict', 'warren', 'blamed', 'the', 'community', 's', 'failure', 'on', 'a', 'lack', 'of', 'individual', 'sovereignty', 'and', 'a', 'lack', 'of', 'private_property', 'warren', 'proceeded', 'to', 'organise', 'experimenal', 'anarchist', 'communities', 'which', 'respected', 'what', 'he', 'called', 'the', 'sovereignty', 'of', 'the', 'individual', 'at', 'utopia', 'and', 'modern_times', 'in', 'one', 'eight', 'three', 'three', 'warren', 'wrote', 'and', 'published', 'the', 'peaceful', 'revolutionist', 'which', 'some', 'have', 'noted', 'to', 'be', 'the', 'first', 'anarchist_periodical', 'ever', 'published', 'benjamin_tucker', 'says_that', 'warren', 'was', 'the', 'first', 'man', 'to', 'expound', 'and', 'formulate', 'the', 'doctrine', 'now_known', 'as', 'anarchism', 'liberty', 'xiv', 'december', 'one', 'nine', 'zero', 'zero', 'one', 'benjamin_tucker', 'became_interested', 'in', 'anarchism', 'through', 'meeting', 'josiah_warren', 'and', 'william', 'b_greene', 'he', 'edited', 'and', 'published', 'liberty', 'from', 'august', 'one', 'eight', 'eight', 'one', 'to', 'april', 'one', 'nine', 'zero', 'eight', 'it', 'is', 'widely_considered', 'to', 'be', 'the', 'finest', 'individualist_anarchist', 'periodical', 'ever', 'issued', 'in', 'the', 'english_language', 'tucker', 's', 'conception', 'of', 'individualist_anarchism', 'incorporated', 'the', 'ideas', 'of', 'a', 'variety', 'of', 'theorists', 'greene', 's', 'ideas', 'on', 'mutual', 'banking', 'warren', 's', 'ideas', 'on', 'cost', 'as', 'the', 'limit', 'of', 'price', 'a', 'heterodox', 'variety', 'of', 'labour', 'theory', 'of', 'value', 'proudhon', 's', 'market_anarchism', 'max_stirner', 's', 'egoism', 'and', 'herbert_spencer', 's', 'law', 'of', 'equal', 'freedom', 'tucker', 'strongly_supported', 'the', 'individual', 's', 'right', 'to', 'own', 'the', 'product', 'of', 'his', 'or', 'her', 'labour', 'as', 'private_property', 'and', 'believed', 'in', 'a', 'market_economy', 'for', 'trading', 'this', 'property', 'he', 'argued_that', 'in', 'a', 'truly', 'free_market', 'system', 'without', 'the', 'state', 'the', 'abundance', 'of', 'competition', 'would', 'eliminate', 'profits', 'and', 'ensure_that', 'all', 'workers', 'received', 'the', 'full', 'value', 'of', 'their', 'labor', 'other', 'one', 'nine', 'th_century', 'individualists', 'included', 'lysander_spooner', 'stephen_pearl', 'andrews', 'and', 'victor', 'yarros', 'the', 'first', 'international', 'mikhail_bakunin', 'one', 'eight', 'one', 'four', 'one', 'eight', 'seven', 'six', 'in', 'europe', 'harsh', 'reaction', 'followed', 'the', 'revolutions', 'of', 'one', 'eight', 'four', 'eight', 'twenty_years', 'later', 'in', 'one', 'eight', 'six', 'four', 'the', 'international_workingmen', 's', 'association', 'sometimes_called', 'the', 'first', 'international', 'united', 'some', 'diverse', 'european', 'revolutionary', 'currents', 'including', 'anarchism', 'due_to', 'its', 'genuine', 'links', 'to', 'active', 'workers', 'movements', 'the', 'international', 'became', 'signficiant', 'from', 'the', 'start', 'karl_marx', 'was', 'a', 'leading_figure', 'in', 'the', 'international', 'he', 'was', 'elected', 'to', 'every', 'succeeding', 'general', 'council', 'of', 'the', 'association', 'the', 'first', 'objections', 'to', 'marx', 'came', 'from', 'the', 'mutualists', 'who', 'opposed_communism', 'and', 'statism', 'shortly_after', 'mikhail_bakunin', 'and', 'his_followers', 'joined', 'in', 'one', 'eight', 'six', 'eight', 'the', 'first', 'international', 'became', 'polarised', 'into', 'two', 'camps', 'with', 'marx', 'and', 'bakunin', 'as', 'their_respective', 'figureheads', 'the', 'clearest', 'difference_between', 'the', 'camps', 'was', 'over', 'strategy', 'the', 'anarchists', 'around', 'bakunin', 'favoured', 'in', 'kropotkin', 's', 'words', 'direct', 'economical', 'struggle_against', 'capitalism', 'without_interfering', 'in', 'the', 'political', 'parliamentary', 'agitation', 'at', 'that', 'time', 'marx', 'and', 'his_followers', 'focused_on', 'parliamentary', 'activity', 'bakunin', 'characterised', 'marx', 's', 'ideas', 'as', 'authoritarian', 'and', 'predicted', 'that', 'if', 'a', 'marxist', 'party', 'gained', 'to', 'power', 'its', 'leaders', 'would', 'end', 'up', 'as', 'bad', 'as', 'the', 'ruling_class', 'they', 'had', 'fought_against', 'in', 'one', 'eight', 'seven', 'two', 'the', 'conflict', 'climaxed', 'with', 'a', 'final', 'split_between', 'the', 'two', 'groups', 'at', 'the', 'hague', 'congress', 'this', 'is', 'often_cited', 'as', 'the', 'origin', 'of', 'the', 'conflict_between', 'anarchists', 'and', 'marxists', 'from', 'this', 'moment', 'the', 'social_democratic', 'and', 'libertarian', 'currents', 'of', 'socialism', 'had', 'distinct', 'organisations', 'including', 'rival', 'internationals', 'anarchist_communism', 'peter_kropotkin', 'proudhon', 'and', 'bakunin', 'both', 'opposed_communism', 'associating', 'it', 'with', 'statism', 'however', 'in', 'the', 'one', 'eight', 'seven', 'zero', 's', 'many', 'anarchists', 'moved_away', 'from', 'bakunin', 's', 'economic', 'thinking', 'called', 'collectivism', 'and', 'embraced', 'communist', 'concepts', 'communists', 'believed', 'the', 'means', 'of', 'production', 'should_be', 'owned', 'collectively', 'and', 'that', 'goods', 'be', 'distributed', 'by', 'need', 'not', 'labor', 'an', 'early', 'anarchist', 'communist', 'was', 'joseph', 'd', 'jacque', 'the', 'first_person', 'to', 'describe', 'himself', 'as', 'libertarian', 'unlike', 'proudhon', 'he', 'argued_that', 'it', 'is', 'not', 'the', 'product', 'of', 'his', 'or', 'her', 'labor', 'that', 'the', 'worker', 'has', 'a', 'right', 'to', 'but', 'to', 'the', 'satisfaction', 'of', 'his', 'or', 'her', 'needs', 'whatever', 'may_be', 'their', 'nature', 'he', 'announced', 'his', 'ideas', 'in', 'his', 'us', 'published', 'journal', 'le', 'libertaire', 'one', 'eight', 'five', 'eight', 'one', 'eight', 'six', 'one', 'peter_kropotkin', 'often_seen', 'as', 'the', 'most_important', 'theorist', 'outlined', 'his', 'economic', 'ideas', 'in', 'the', 'conquest', 'of', 'bread', 'and', 'fields', 'factories', 'and', 'workshops', 'he_felt', 'co_operation', 'is', 'more', 'beneficial', 'than', 'competition', 'illustrated', 'in', 'nature', 'in', 'mutual_aid', 'a', 'factor', 'of', 'evolution', 'one', 'eight', 'nine', 'seven', 'subsequent', 'anarchist_communists', 'include', 'emma_goldman', 'and', 'alexander_berkman', 'many', 'in', 'the', 'anarcho_syndicalist', 'movements', 'see_below', 'saw', 'anarchist_communism', 'as', 'their', 'objective', 'isaac', 'puente', 's', 'one', 'nine', 'three', 'two', 'comunismo', 'libertario', 'was', 'adopted', 'by', 'the', 'spanish', 'cnt', 'as', 'its', 'manifesto', 'for', 'a', 'post', 'revolutionary', 'society', 'some', 'anarchists', 'disliked', 'merging', 'communism', 'with', 'anarchism', 'several', 'individualist_anarchists', 'maintained', 'that', 'abolition', 'of', 'private_property', 'was', 'not', 'consistent_with', 'liberty', 'for_example', 'benjamin_tucker', 'whilst', 'professing', 'respect', 'for', 'kropotkin', 'and', 'publishing', 'his', 'work', 'described', 'communist', 'anarchism', 'as', 'pseudo', 'anarchism', 'propaganda', 'of', 'the', 'deed', 'johann', 'most', 'was', 'an_outspoken', 'advocate', 'of', 'violence', 'anarchists', 'have', 'often', 'been', 'portrayed', 'as', 'dangerous', 'and', 'violent', 'due', 'mainly', 'to', 'a', 'number', 'of', 'high_profile', 'violent_acts', 'including', 'riots', 'assassinations', 'insurrections', 'and', 'terrorism', 'by', 'some', 'anarchists', 'some', 'revolutionaries', 'of', 'the', 'late', 'one', 'nine', 'th_century', 'encouraged', 'acts', 'of', 'political', 'violence', 'such_as', 'bombings', 'and', 'the', 'assassinations', 'of', 'heads', 'of', 'state', 'to', 'further', 'anarchism', 'such', 'actions', 'have', 'sometimes', 'been', 'called', 'propaganda', 'by', 'the', 'deed', 'one', 'of', 'the', 'more', 'outspoken', 'advocates', 'of', 'this', 'strategy', 'was', 'johann', 'most', 'who', 'said', 'the', 'existing', 'system', 'will_be', 'quickest', 'and', 'most', 'radically', 'overthrown_by', 'the', 'annihilation', 'of', 'its', 'exponents', 'therefore', 'massacres', 'of', 'the', 'enemies', 'of', 'the', 'people', 'must_be', 'set', 'in', 'motion', 'most', 's', 'preferred_method', 'of', 'terrorism', 'dynamite', 'earned_him', 'the', 'moniker', 'dynamost', 'however', 'there', 'is', 'no_consensus', 'on', 'the', 'legitimacy', 'or', 'utility', 'of', 'violence', 'in', 'general', 'mikhail_bakunin', 'and', 'errico_malatesta', 'for_example', 'wrote', 'of', 'violence', 'as', 'a', 'necessary', 'and', 'sometimes', 'desirable', 'force', 'in', 'revolutionary', 'settings', 'but', 'at', 'the', 'same_time', 'they', 'denounced', 'acts', 'of', 'individual', 'terrorism', 'malatesta', 'in', 'on', 'violence', 'and', 'bakunin', 'when', 'he', 'refuted', 'nechaev', 'other', 'anarchists', 'sometimes', 'identified', 'as', 'pacifist', 'anarchists', 'advocated', 'complete', 'nonviolence', 'leo_tolstoy', 'whose', 'philosophy', 'is', 'often', 'viewed_as', 'a', 'form', 'of', 'christian_anarchism', 'see_below', 'was', 'a', 'notable', 'exponent', 'of', 'nonviolent_resistance', 'anarchism', 'in', 'the', 'labour_movement', 'the', 'red', 'and', 'black_flag', 'coming', 'from', 'the', 'experience', 'of', 'anarchists', 'in', 'the', 'labour_movement', 'is', 'particularly', 'associated_with', 'anarcho_syndicalism', 'anarcho_syndicalism', 'was', 'an', 'early', 'two', 'zero', 'th_century', 'working_class', 'movement', 'seeking', 'to', 'overthrow', 'capitalism', 'and', 'the', 'state', 'to', 'institute', 'a', 'worker', 'controlled', 'society', 'the', 'movement', 'pursued', 'industrial', 'actions', 'such_as', 'general_strike', 'as', 'a', 'primary', 'strategy', 'many', 'anarcho_syndicalists', 'believed', 'in', 'anarchist_communism', 'though', 'not', 'all', 'communists', 'believed', 'in', 'syndicalism', 'after', 'the', 'one', 'eight', 'seven', 'one', 'repression', 'french', 'anarchism', 'reemerged', 'influencing', 'the', 'bourses', 'de', 'travails', 'of', 'autonomous', 'workers', 'groups', 'and', 'trade_unions', 'from', 'this', 'movement', 'the', 'conf', 'd_ration', 'g', 'n_rale', 'du_travail', 'general', 'confederation', 'of', 'work', 'cgt', 'was', 'formed', 'in', 'one', 'eight', 'nine', 'five', 'as', 'the', 'first', 'major', 'anarcho_syndicalist', 'movement', 'emile', 'pataud', 'and', 'emile', 'pouget', 's', 'writing', 'for', 'the', 'cgt', 'saw', 'libertarian_communism', 'developing', 'from', 'a', 'general_strike', 'after', 'one', 'nine', 'one', 'four', 'the', 'cgt', 'moved_away', 'from', 'anarcho_syndicalism', 'due_to', 'the', 'appeal', 'of', 'bolshevism', 'french', 'style', 'syndicalism', 'was', 'a', 'significant', 'movement', 'in', 'europe', 'prior_to', 'one', 'nine', 'two', 'one', 'and', 'remained', 'a', 'significant', 'movement', 'in', 'spain', 'until', 'the', 'mid', 'one', 'nine', 'four', 'zero', 's', 'the', 'industrial_workers', 'of', 'the', 'world', 'iww', 'founded', 'in', 'one', 'nine', 'zero', 'five', 'in', 'the', 'us', 'espoused', 'unionism', 'and', 'sought', 'a', 'general_strike', 'to', 'usher', 'in', 'a', 'stateless', 'society', 'in', 'one', 'nine', 'two', 'three', 'one', 'zero', 'zero', 'zero', 'zero', 'zero', 'members', 'existed', 'with', 'the', 'support', 'of', 'up', 'to', 'three', 'zero', 'zero', 'zero', 'zero', 'zero', 'though', 'not', 'explicitly', 'anarchist', 'they', 'organized', 'by', 'rank', 'and', 'file', 'democracy', 'embodying', 'a', 'spirit', 'of', 'resistance', 'that', 'has', 'inspired', 'many', 'anglophone', 'syndicalists', 'cnt', 'propaganda', 'from', 'april', 'two', 'zero', 'zero', 'four', 'reads', 'don_t', 'let', 'the', 'politicians', 'rule', 'our_lives', 'you', 'vote', 'and', 'they', 'decide', 'don_t', 'allow', 'it', 'unity', 'action', 'self', 'management', 'spanish', 'anarchist', 'trade_union', 'federations', 'were', 'formed', 'in', 'the', 'one', 'eight', 'seven', 'zero', 's', 'one', 'nine', 'zero', 'zero', 'and', 'one', 'nine', 'one', 'zero', 'the', 'most_successful', 'was', 'the', 'confederaci_n', 'nacional_del', 'trabajo', 'national', 'confederation', 'of', 'labour', 'cnt', 'founded', 'in', 'one', 'nine', 'one', 'zero', 'prior_to', 'the', 'one', 'nine', 'four', 'zero', 's', 'the', 'cnt', 'was', 'the', 'major', 'force', 'in', 'spanish', 'working_class', 'politics', 'with', 'a', 'membership', 'of', 'one', 'five', 'eight', 'million', 'in', 'one', 'nine', 'three', 'four', 'the', 'cnt', 'played', 'a', 'major_role', 'in', 'the', 'spanish_civil', 'war', 'see_also', 'anarchism', 'in', 'spain', 'syndicalists', 'like', 'ricardo', 'flores', 'mag', 'n', 'were', 'key_figures', 'in', 'the', 'mexican_revolution', 'latin', 'american', 'anarchism', 'was', 'strongly_influenced', 'extending', 'to', 'the', 'zapatista', 'rebellion', 'and', 'the', 'factory', 'occupation', 'movements', 'in', 'argentina', 'in', 'berlin', 'in', 'one', 'nine', 'two', 'two', 'the', 'cnt', 'was', 'joined', 'with', 'the', 'international', 'workers', 'association', 'an', 'anarcho_syndicalist', 'successor', 'to', 'the', 'first', 'international', 'contemporary', 'anarcho_syndicalism', 'continues', 'as', 'a', 'minor', 'force', 'in', 'many', 'socities', 'much_smaller', 'than', 'in', 'the', 'one', 'nine', 'one', 'zero', 's', 'two', 'zero', 's', 'and', 'three', 'zero', 's', 'the', 'largest', 'organised', 'anarchist_movement', 'today', 'is', 'in', 'spain', 'in', 'the', 'form', 'of', 'the', 'confederaci_n', 'general', 'del_trabajo', 'and', 'the', 'cnt', 'the', 'cgt', 'claims', 'a', 'paid', 'up', 'membership', 'of', 'six', 'zero', 'zero', 'zero', 'zero', 'and', 'received', 'over', 'a', 'million', 'votes', 'in', 'spanish', 'syndical', 'elections', 'other', 'active', 'syndicalist', 'movements', 'include', 'the', 'us', 'workers_solidarity', 'alliance', 'and', 'the', 'uk', 'solidarity', 'federation', 'the', 'revolutionary', 'industrial', 'unionist', 'industrial_workers', 'of', 'the', 'world', 'also', 'exists', 'claiming', 'two', 'zero', 'zero', 'zero', 'paid', 'members', 'contemporary', 'critics', 'of', 'anarcho_syndicalism', 'and', 'revolutionary', 'industrial_unionism', 'claim_that', 'they', 'are', 'workerist', 'and', 'fail', 'to', 'deal_with', 'economic', 'life', 'outside', 'work', 'post', 'leftist', 'critics', 'such_as', 'bob', 'black', 'claim', 'anarcho_syndicalism', 'advocates', 'oppressive_social', 'structures', 'such_as', 'work', 'and', 'the', 'workplace', 'anarcho_syndicalists', 'in', 'general', 'uphold', 'principles', 'of', 'workers_solidarity', 'direct_action', 'and', 'self', 'management', 'the', 'russian_revolution', 'the', 'russian_revolution', 'of', 'one', 'nine', 'one', 'seven', 'was', 'a', 'seismic', 'event', 'in', 'the', 'development', 'of', 'anarchism', 'as', 'a', 'movement', 'and', 'as', 'a', 'philosophy', 'anarchists', 'participated', 'alongside', 'the', 'bolsheviks', 'in', 'both', 'february', 'and', 'october', 'revolutions', 'many', 'anarchists', 'initially', 'supporting', 'the', 'bolshevik', 'coup', 'however', 'the', 'bolsheviks', 'soon', 'turned', 'against', 'the', 'anarchists', 'and', 'other', 'left_wing', 'opposition', 'a', 'conflict', 'which_culminated', 'in', 'the', 'one', 'nine', 'one', 'eight', 'kronstadt_rebellion', 'anarchists', 'in', 'central', 'russia', 'were', 'imprisoned', 'or', 'driven_underground', 'or', 'joined', 'the', 'victorious', 'bolsheviks', 'in', 'ukraine', 'anarchists', 'fought', 'in', 'the', 'civil_war', 'against', 'both', 'whites', 'and', 'bolsheviks', 'within', 'the', 'makhnovshchina', 'peasant', 'army', 'led', 'by', 'nestor_makhno', 'expelled', 'american', 'anarchists', 'emma_goldman', 'and', 'alexander_berkman', 'before_leaving', 'russia', 'were', 'amongst', 'those', 'agitating', 'in', 'response', 'to', 'bolshevik', 'policy', 'and', 'the', 'suppression', 'of', 'the', 'kronstadt', 'uprising', 'both', 'wrote', 'classic', 'accounts', 'of', 'their', 'experiences', 'in', 'russia', 'aiming', 'to', 'expose', 'the', 'reality', 'of', 'bolshevik', 'control', 'for', 'them', 'bakunin', 's', 'predictions_about', 'the', 'consequences', 'of', 'marxist', 'rule', 'had', 'proved', 'all', 'too', 'true', 'the', 'victory', 'of', 'the', 'bolsheviks', 'in', 'the', 'october_revolution', 'and', 'the', 'resulting', 'russian_civil', 'war', 'did', 'serious_damage', 'to', 'anarchist', 'movements', 'internationally', 'many', 'workers', 'and', 'activists', 'saw', 'bolshevik', 'success', 'as', 'setting', 'an', 'example', 'communist_parties', 'grew', 'at', 'the', 'expense', 'of', 'anarchism', 'and', 'other', 'socialist_movements', 'in', 'france', 'and', 'the', 'us', 'for_example', 'the', 'major', 'syndicalist', 'movements', 'of', 'the', 'cgt', 'and', 'iww', 'began', 'to', 'realign', 'themselves', 'away_from', 'anarchism', 'and', 'towards', 'the', 'communist', 'international', 'in', 'paris', 'the', 'dielo', 'truda', 'group', 'of', 'russian', 'anarchist', 'exiles', 'which', 'included', 'nestor_makhno', 'concluded_that', 'anarchists', 'needed', 'to', 'develop', 'new', 'forms', 'of', 'organisation', 'in', 'response', 'to', 'the', 'structures', 'of', 'bolshevism', 'their', 'one', 'nine', 'two', 'six', 'manifesto', 'known_as', 'the', 'organisational', 'platform', 'of', 'the', 'libertarian_communists', 'was', 'supported_by', 'some', 'communist_anarchists', 'though', 'opposed', 'by', 'many', 'others', 'the', 'platform', 'continues', 'to', 'inspire', 'some', 'contemporary', 'anarchist', 'groups', 'who', 'believe', 'in', 'an', 'anarchist_movement', 'organised_around', 'its', 'principles', 'of', 'theoretical', 'unity', 'tactical', 'unity', 'collective_responsibility', 'and', 'federalism', 'platformist', 'groups', 'today', 'include', 'the', 'workers_solidarity', 'movement', 'in', 'ireland', 'the', 'uk', 's', 'anarchist', 'federation', 'and', 'the', 'late', 'north_eastern', 'federation', 'of', 'anarchist_communists', 'in', 'the', 'northeastern_united', 'states', 'and', 'bordering', 'canada', 'the', 'fight_against', 'fascism', 'spain', 'one', 'nine', 'three', 'six', 'members', 'of', 'the', 'cnt', 'construct', 'armoured_cars', 'to', 'fight_against', 'the', 'fascists', 'in', 'one', 'of', 'the', 'collectivised', 'factories', 'in', 'the', 'one', 'nine', 'two', 'zero', 's', 'and', 'one', 'nine', 'three', 'zero', 's', 'the', 'familiar', 'dynamics', 'of', 'anarchism', 's', 'conflict', 'with', 'the', 'state', 'were', 'transformed', 'by', 'the', 'rise', 'of', 'fascism', 'in', 'europe', 'in', 'many_cases', 'european', 'anarchists', 'faced', 'difficult', 'choices', 'should', 'they', 'join', 'in', 'popular', 'fronts', 'with', 'reformist', 'democrats', 'and', 'soviet', 'led', 'communists', 'against', 'a', 'common', 'fascist', 'enemy', 'luigi', 'fabbri', 'an', 'exile', 'from', 'italian_fascism', 'was', 'amongst', 'those', 'arguing_that', 'fascism', 'was', 'something', 'different', 'fascism', 'is', 'not', 'just', 'another', 'form', 'of', 'government', 'which', 'like', 'all', 'others', 'uses', 'violence', 'it', 'is', 'the', 'most', 'authoritarian', 'and', 'the', 'most', 'violent', 'form', 'of', 'government', 'imaginable', 'it', 'represents', 'the', 'utmost', 'glorification', 'of', 'the', 'theory', 'and', 'practice', 'of', 'the', 'principle', 'of', 'authority', 'in', 'france', 'where', 'the', 'fascists', 'came_close', 'to', 'insurrection', 'in', 'the', 'february', 'one', 'nine', 'three', 'four', 'riots', 'anarchists', 'divided', 'over', 'a', 'united', 'front', 'policy', 'in', 'spain', 'the', 'cnt', 'initially_refused', 'to', 'join', 'a', 'popular_front', 'electoral', 'alliance', 'and', 'abstention', 'by', 'cnt', 'supporters', 'led', 'to', 'a', 'right_wing', 'election_victory', 'but', 'in', 'one', 'nine', 'three', 'six', 'the', 'cnt', 'changed', 'its', 'policy', 'and', 'anarchist', 'votes', 'helped_bring', 'the', 'popular_front', 'back', 'to', 'power', 'months_later', 'the', 'ruling_class', 'responded', 'with', 'an', 'attempted_coup', 'and', 'the', 'spanish_civil', 'war', 'one', 'nine', 'three', 'six', 'three', 'nine', 'was', 'underway', 'in', 'reponse', 'to', 'the', 'army', 'rebellion', 'an', 'anarchist', 'inspired', 'movement', 'of', 'peasants', 'and', 'workers', 'supported_by', 'armed', 'militias', 'took_control', 'of', 'the', 'major', 'city', 'of', 'barcelona', 'and', 'of', 'large_areas', 'of', 'rural', 'spain', 'where', 'they', 'collectivized', 'the', 'land', 'but', 'even', 'before', 'the', 'eventual', 'fascist', 'victory', 'in', 'one', 'nine', 'three', 'nine', 'the', 'anarchists', 'were', 'losing_ground', 'in', 'a', 'bitter_struggle', 'with', 'the', 'stalinists', 'the', 'cnt', 'leadership', 'often', 'appeared', 'confused', 'and', 'divided', 'with', 'some', 'members', 'controversially', 'entering', 'the', 'government', 'stalinist', 'led', 'troops', 'suppressed', 'the', 'collectives', 'and', 'persecuted', 'both', 'dissident', 'marxists', 'and', 'anarchists', 'since', 'the', 'late', 'one', 'nine', 'seven', 'zero', 's', 'anarchists', 'have_been', 'involved', 'in', 'fighting', 'the', 'rise', 'of', 'neo_fascist', 'groups', 'in', 'germany', 'and', 'the', 'united_kingdom', 'some', 'anarchists', 'worked', 'within', 'militant', 'anti_fascist', 'groups', 'alongside', 'members', 'of', 'the', 'marxist', 'left', 'they', 'advocated', 'directly', 'combating', 'fascists', 'with', 'physical', 'force', 'rather_than', 'relying_on', 'the', 'state', 'since', 'the', 'late', 'one', 'nine', 'nine', 'zero', 's', 'a', 'similar', 'tendency', 'has', 'developed', 'within', 'us', 'anarchism', 'see_also', 'anti_racist', 'action', 'us', 'anti_fascist', 'action', 'uk', 'antifa', 'religious', 'anarchism', 'leo_tolstoy', 'one', 'eight', 'two', 'eight', 'one', 'nine', 'one', 'zero', 'most', 'anarchist', 'culture', 'tends_to', 'be', 'secular', 'if', 'not', 'outright', 'anti', 'religious', 'however', 'the', 'combination', 'of', 'religious', 'social', 'conscience', 'historical', 'religiousity', 'amongst', 'oppressed', 'social_classes', 'and', 'the', 'compatibility', 'of', 'some', 'interpretations', 'of', 'religious_traditions', 'with', 'anarchism', 'has_resulted', 'in', 'religious', 'anarchism', 'christian', 'anarchists', 'believe_that', 'there', 'is', 'no', 'higher', 'authority', 'than', 'god', 'and', 'oppose', 'earthly', 'authority', 'such_as', 'government', 'and', 'established', 'churches', 'they_believe', 'that', 'jesus_teachings', 'were', 'clearly', 'anarchistic', 'but', 'were', 'corrupted', 'when', 'christianity', 'was', 'declared', 'the', 'official', 'religion', 'of', 'rome', 'christian', 'anarchists', 'who', 'follow', 'jesus', 'directive', 'to', 'turn', 'the', 'other', 'cheek', 'are', 'strict', 'pacifists', 'the', 'most_famous', 'advocate', 'of', 'christian_anarchism', 'was', 'leo_tolstoy', 'author', 'of', 'the', 'kingdom', 'of', 'god', 'is', 'within', 'you', 'who', 'called', 'for', 'a', 'society', 'based_on', 'compassion', 'nonviolent', 'principles', 'and', 'freedom', 'christian', 'anarchists', 'tend_to', 'form', 'experimental', 'communities', 'they', 'also', 'occasionally', 'resist', 'taxation', 'many', 'christian', 'anarchists', 'are', 'vegetarian', 'or', 'vegan', 'christian', 'anarchy', 'can_be', 'said', 'to', 'have', 'roots', 'as', 'old', 'as', 'the', 'religion', 's', 'birth', 'as', 'the', 'early', 'church', 'exhibits', 'many', 'anarchistic', 'tendencies', 'such_as', 'communal', 'goods', 'and', 'wealth', 'by', 'aiming', 'to', 'obey', 'utterly', 'certain', 'of', 'the', 'bible', 's', 'teachings', 'certain', 'anabaptist_groups', 'of', 'sixteenth_century', 'europe', 'attempted', 'to', 'emulate', 'the', 'early', 'church', 's', 'social', 'economic', 'organisation', 'and', 'philosophy', 'by', 'regarding', 'it', 'as', 'the', 'only', 'social_structure', 'capable', 'of', 'true', 'obediance', 'to', 'jesus_teachings', 'and', 'utterly', 'rejected', 'in', 'theory', 'all', 'earthly', 'hierarchies', 'and', 'authority', 'and', 'indeed', 'non', 'anabaptists', 'in', 'general', 'and', 'violence', 'as', 'ungodly', 'such', 'groups', 'for_example', 'the', 'hutterites', 'typically', 'went', 'from', 'initially', 'anarchistic', 'beginnings', 'to', 'as', 'their', 'movements', 'stabalised', 'more', 'authoritarian', 'social', 'models', 'chinese', 'anarchism', 'was', 'most_influential', 'in', 'the', 'one', 'nine', 'two', 'zero', 's', 'strands', 'of', 'chinese', 'anarchism', 'included', 'tai', 'xu', 's', 'buddhist', 'anarchism', 'which', 'was', 'influenced_by', 'tolstoy', 'and', 'the', 'well', 'field', 'system', 'neopaganism', 'with', 'its', 'focus_on', 'the', 'environment', 'and', 'equality', 'along_with', 'its', 'often', 'decentralized_nature', 'has', 'lead', 'to', 'a', 'number', 'of', 'neopagan', 'anarchists', 'one', 'of', 'the', 'most_prominent', 'is', 'starhawk', 'who', 'writes', 'extensively', 'about', 'both', 'spirituality', 'and', 'activism', 'anarchism', 'and', 'feminism', 'emma_goldman', 'early', 'french', 'feminists', 'such_as', 'jenny', 'd', 'h', 'ricourt', 'and', 'juliette', 'adam', 'criticised', 'the', 'mysogyny', 'in', 'the', 'anarchism', 'of', 'proudhon', 'during', 'the', 'one', 'eight', 'five', 'zero', 's', 'anarcha_feminism', 'is', 'a', 'kind', 'of', 'radical_feminism', 'that', 'espouses', 'the', 'belief_that', 'patriarchy', 'is', 'a', 'fundamental', 'problem', 'in', 'society', 'while', 'anarchist', 'feminism', 'has', 'existed', 'for', 'more_than', 'a', 'hundred_years', 'its', 'explicit', 'formulation', 'as', 'anarcha_feminism', 'dates_back', 'to', 'the', 'early', 'seven', 'zero', 's', 'during', 'the', 'second_wave', 'feminist_movement', 'anarcha_feminism', 'views', 'patriarchy', 'as', 'the', 'first', 'manifestation', 'of', 'hierarchy', 'in', 'human', 'history', 'thus', 'the', 'first', 'form', 'of', 'oppression', 'occurred', 'in', 'the', 'dominance', 'of', 'male', 'over', 'female', 'anarcha', 'feminists', 'then', 'conclude_that', 'if', 'feminists', 'are', 'against', 'patriarchy', 'they', 'must', 'also', 'be', 'against', 'all', 'forms', 'of', 'hierarchy', 'and', 'therefore', 'must', 'reject', 'the', 'authoritarian', 'nature', 'of', 'the', 'state', 'and', 'capitalism_anarcho', 'primitivists', 'see', 'the', 'creation', 'of', 'gender_roles', 'and', 'patriarchy', 'a', 'creation', 'of', 'the', 'start', 'of', 'civilization', 'and', 'therefore', 'consider', 'primitivism', 'to', 'also', 'be', 'an', 'anarchist', 'school', 'of', 'thought', 'that', 'addresses', 'feminist', 'concerns', 'eco', 'feminism', 'is', 'often_considered', 'a', 'feminist', 'variant', 'of', 'green', 'anarchist', 'feminist', 'thought', 'anarcha_feminism', 'is', 'most', 'often_associated', 'with', 'early', 'two', 'zero', 'th_century', 'authors', 'and', 'theorists', 'such_as', 'emma_goldman', 'and', 'voltairine_de', 'cleyre', 'although', 'even', 'early', 'first', 'wave_feminist', 'mary_wollstonecraft', 'held', 'proto', 'anarchist', 'views', 'and', 'william_godwin', 'is', 'often_considered', 'a', 'feminist', 'anarchist', 'precursor', 'it', 'should_be', 'noted_that', 'goldman', 'and', 'de_cleyre', 'though', 'they', 'both', 'opposed', 'the', 'state', 'had', 'opposing', 'philosophies', 'as', 'de_cleyre', 'explains', 'miss', 'goldman', 'is', 'a', 'communist', 'i_am', 'an', 'individualist', 'she', 'wishes', 'to', 'destroy', 'the', 'right', 'of', 'property', 'i_wish', 'to', 'assert', 'it', 'i', 'make', 'my', 'war', 'upon', 'privilege', 'and', 'authority', 'whereby', 'the', 'right', 'of', 'property', 'the', 'true', 'right', 'in', 'that', 'which', 'is', 'proper', 'to', 'the', 'individual', 'is', 'annihilated', 'she_believes', 'that', 'co_operation', 'would', 'entirely', 'supplant', 'competition', 'i', 'hold', 'that', 'competition', 'in', 'one', 'form', 'or', 'another', 'will_always', 'exist', 'and', 'that', 'it', 'is', 'highly_desirable', 'it', 'should', 'in', 'the', 'spanish_civil', 'war', 'an', 'anarcha', 'feminist', 'group', 'free', 'women', 'organized', 'to', 'defend', 'both', 'anarchist', 'and', 'feminist_ideas', 'in', 'the', 'modern_day', 'anarchist_movement', 'many', 'anarchists', 'male', 'or', 'female', 'consider_themselves', 'feminists', 'and', 'anarcha', 'feminist_ideas', 'are', 'growing', 'the', 'publishing', 'of', 'quiet', 'rumors', 'an', 'anarcha', 'feminist', 'reader', 'has', 'helped', 'to', 'spread', 'various_kinds', 'of', 'anti_authoritarian', 'and', 'anarchist', 'feminist_ideas', 'to', 'the', 'broader', 'movement', 'wendy_mcelroy', 'has', 'popularized', 'an', 'individualist_anarchism', 'take', 'on', 'feminism', 'in', 'her', 'books', 'articles', 'and', 'individualist', 'feminist', 'website', 'anarcho_capitalism', 'murray_rothbard', 'one', 'nine', 'two', 'six', 'one', 'nine', 'nine', 'five', 'anarcho_capitalism', 'is', 'a', 'predominantly', 'united_states', 'based', 'theoretical', 'tradition', 'that', 'desires', 'a', 'stateless', 'society', 'with', 'the', 'economic', 'system', 'of', 'free_market', 'capitalism', 'unlike', 'other', 'branches', 'of', 'anarchism', 'it', 'does_not', 'oppose', 'profit', 'or', 'capitalism', 'consequently', 'most', 'anarchists', 'do_not', 'recognise', 'anarcho_capitalism', 'as', 'a', 'form', 'of', 'anarchism', 'murray_rothbard', 's', 'synthesis', 'of', 'classical_liberalism', 'and', 'austrian_economics', 'was', 'germinal', 'for', 'the', 'development', 'of', 'contemporary', 'anarcho_capitalist', 'theory', 'he', 'defines', 'anarcho_capitalism', 'in', 'terms', 'of', 'the', 'non_aggression', 'principle', 'based_on', 'the', 'concept', 'of', 'natural_law', 'competiting', 'theorists', 'use', 'egoism', 'utilitarianism', 'used', 'by', 'david_friedman', 'or', 'contractarianism', 'used', 'by', 'jan', 'narveson', 'some', 'minarchists', 'such_as', 'ayn_rand', 'robert_nozick', 'and', 'robert', 'a', 'heinlein', 'have', 'influenced', 'anarcho_capitalism', 'some', 'anarcho_capitalists', 'along_with', 'some', 'right_wing', 'libertarian', 'historians', 'such_as', 'david', 'hart', 'and', 'ralph', 'raico', 'considered', 'similar', 'philosophies', 'existing', 'prior_to', 'rothbard', 'to', 'be', 'anarcho_capitalist', 'such_as', 'those', 'of', 'gustave_de', 'molinari', 'and', 'auberon_herbert', 'opponents', 'of', 'anarcho_capitalists', 'dispute', 'these', 'claims', 'the', 'place', 'of', 'anarcho_capitalism', 'within', 'anarchism', 'and', 'indeed', 'whether', 'it', 'is', 'a', 'form', 'of', 'anarchism', 'at', 'all', 'is', 'highly_controversial', 'for', 'more', 'on', 'this', 'debate', 'see', 'anarchism', 'and', 'anarcho_capitalism', 'anarchism', 'and', 'the', 'environment', 'since', 'the', 'late', 'one', 'nine', 'seven', 'zero', 's', 'anarchists', 'in', 'anglophone', 'and', 'european_countries', 'have_been', 'taking', 'action', 'for', 'the', 'natural_environment', 'eco_anarchists', 'or', 'green', 'anarchists', 'believe', 'in', 'deep_ecology', 'this', 'is', 'a', 'worldview', 'that', 'embraces', 'biodiversity', 'and', 'sustainability', 'eco_anarchists', 'often', 'use', 'direct_action', 'against', 'what', 'they', 'see', 'as', 'earth', 'destroying', 'institutions', 'of', 'particular_importance', 'is', 'the', 'earth', 'first', 'movement', 'that', 'takes', 'action', 'such_as', 'tree', 'sitting', 'another', 'important_component', 'is', 'ecofeminism', 'which', 'sees', 'the', 'domination', 'of', 'nature', 'as', 'a', 'metaphor', 'for', 'the', 'domination', 'of', 'women', 'green_anarchism', 'also', 'involves', 'a', 'critique', 'of', 'industrial_capitalism', 'and', 'for', 'some', 'green', 'anarchists', 'civilization', 'itself', 'primitivism', 'is', 'a', 'predominantly', 'western_philosophy', 'that', 'advocates', 'a', 'return', 'to', 'a', 'pre_industrial', 'and', 'usually', 'pre', 'agricultural', 'society', 'it', 'develops', 'a', 'critique', 'of', 'industrial', 'civilization', 'in', 'this', 'critique', 'technology', 'and', 'development', 'have', 'alienated', 'people', 'from', 'the', 'natural', 'world', 'this', 'philosophy', 'develops', 'themes', 'present', 'in', 'the', 'political', 'action', 'of', 'the', 'luddites', 'and', 'the', 'writings', 'of', 'jean_jacques', 'rousseau', 'primitivism', 'developed', 'in', 'the', 'context', 'of', 'the', 'reclaim', 'the', 'streets', 'earth', 'first', 'and', 'the', 'earth_liberation', 'front', 'movements', 'john_zerzan', 'wrote', 'that', 'civilization', 'not', 'just', 'the', 'state', 'would', 'need', 'to', 'fall', 'for', 'anarchy', 'to', 'be_achieved', 'anarcho', 'primitivists', 'point', 'to', 'the', 'anti_authoritarian', 'nature', 'of', 'many', 'primitive', 'or', 'hunter_gatherer', 'societies', 'throughout', 'the', 'world', 's', 'history', 'as', 'examples', 'of', 'anarchist', 'societies', 'other', 'branches', 'and', 'offshoots', 'anarchism', 'generates', 'many', 'eclectic', 'and', 'syncretic', 'philosophies', 'and', 'movements', 'since', 'the', 'western', 'social', 'formet', 'in', 'the', 'one', 'nine', 'six', 'zero', 's', 'and', 'one', 'nine', 'seven', 'zero', 's', 'a', 'number', 'new', 'of', 'movements', 'and', 'schools', 'have', 'appeared', 'most', 'of', 'these', 'stances', 'are', 'limited', 'to', 'even', 'smaller', 'numbers', 'than', 'the', 'schools', 'and', 'movements', 'listed_above', 'hakim_bey', 'post', 'left_anarchy', 'post', 'left_anarchy', 'also', 'called', 'egoist', 'anarchism', 'seeks', 'to', 'distance', 'itself', 'from', 'the', 'traditional', 'left', 'communists', 'liberals', 'social_democrats', 'etc', 'and', 'to', 'escape', 'the', 'confines', 'of', 'ideology', 'in', 'general', 'post', 'leftists', 'argue_that', 'anarchism', 'has_been', 'weakened', 'by', 'its', 'long', 'attachment', 'to', 'contrary', 'leftist', 'movements', 'and', 'single', 'issue', 'causes', 'anti', 'war', 'anti', 'nuclear', 'etc', 'it', 'calls', 'for', 'a', 'synthesis', 'of', 'anarchist', 'thought', 'and', 'a', 'specifically', 'anti_authoritarian', 'revolutionary_movement', 'outside', 'of', 'the', 'leftist', 'milieu', 'it', 'often', 'focuses_on', 'the', 'individual', 'rather_than', 'speaking', 'in', 'terms', 'of', 'class', 'or', 'other', 'broad', 'generalizations', 'and', 'shuns', 'organizational', 'tendencies', 'in', 'favor', 'of', 'the', 'complete_absence', 'of', 'explicit', 'hierarchy', 'important', 'groups', 'and', 'individuals', 'associated_with', 'post', 'left_anarchy', 'include', 'crimethinc', 'the', 'magazine', 'anarchy', 'a', 'journal', 'of', 'desire', 'armed', 'and', 'its', 'editor', 'jason', 'mcquinn', 'bob', 'black', 'hakim_bey', 'and', 'others', 'for', 'more', 'information', 'see', 'infoshop_org', 's', 'anarchy', 'after', 'leftism', 'section', 'and', 'the', 'post', 'left', 'section', 'on', 'anarchism', 'ws', 'see_also', 'post', 'left_anarchy', 'post_structuralism', 'the', 'term', 'postanarchism', 'was', 'originated', 'by', 'saul', 'newman', 'first', 'receiving', 'popular', 'attention', 'in', 'his', 'book', 'from', 'bakunin', 'to', 'lacan', 'to', 'refer_to', 'a', 'theoretical', 'move_towards', 'a', 'synthesis', 'of', 'classical', 'anarchist', 'theory', 'and', 'poststructuralist', 'thought', 'subsequent', 'to', 'newman', 's', 'use', 'of', 'the', 'term', 'however', 'it', 'has', 'taken', 'on', 'a', 'life', 'of', 'its_own', 'and', 'a', 'wide_range', 'of', 'ideas', 'including', 'autonomism', 'post', 'left_anarchy', 'situationism', 'post', 'colonialism', 'and', 'zapatismo', 'by', 'its', 'very', 'nature', 'post', 'anarchism', 'rejects', 'the', 'idea', 'that', 'it', 'should_be', 'a', 'coherent', 'set', 'of', 'doctrines', 'and', 'beliefs', 'as', 'such', 'it', 'is', 'difficult', 'if', 'not', 'impossible', 'to', 'state', 'with', 'any', 'degree', 'of', 'certainty', 'who', 'should', 'or', 'shouldn_t', 'be', 'grouped_under', 'the', 'rubric', 'nonetheless', 'key', 'thinkers', 'associated_with', 'post', 'anarchism', 'include', 'saul', 'newman', 'todd', 'may', 'gilles_deleuze', 'and', 'f_lix', 'guattari', 'external', 'reference', 'postanarchism', 'clearinghouse', 'see_also', 'post', 'anarchism', 'insurrectionary', 'anarchism', 'insurrectionary', 'anarchism', 'is', 'a', 'form', 'of', 'revolutionary', 'anarchism', 'critical', 'of', 'formal', 'anarchist', 'labor_unions', 'and', 'federations', 'insurrectionary', 'anarchists', 'advocate', 'informal', 'organization', 'including', 'small', 'affinity', 'groups', 'carrying_out', 'acts', 'of', 'resistance', 'in', 'various', 'struggles', 'and', 'mass', 'organizations', 'called', 'base', 'structures', 'which', 'can', 'include', 'exploited', 'individuals_who', 'are', 'not', 'anarchists', 'proponents', 'include', 'wolfi', 'landstreicher', 'and', 'alfredo', 'm', 'bonanno', 'author', 'of', 'works', 'including', 'armed', 'joy', 'and', 'the', 'anarchist', 'tension', 'this', 'tendency', 'is', 'represented', 'in', 'the', 'us', 'in', 'magazines', 'such_as', 'willful', 'disobedience', 'and', 'killing', 'king', 'abacus', 'see_also', 'insurrectionary', 'anarchism', 'small', 'a', 'anarchism', 'small', 'a', 'anarchism', 'is', 'a', 'term', 'used', 'in', 'two', 'different', 'but', 'not', 'unconnected', 'contexts', 'dave', 'neal', 'posited', 'the', 'term', 'in', 'opposition', 'to', 'big', 'a', 'anarchism', 'in', 'the', 'article', 'anarchism', 'ideology', 'or', 'methodology', 'while', 'big', 'a', 'anarchism', 'referred_to', 'ideological', 'anarchists', 'small', 'a', 'anarchism', 'was', 'applied', 'to', 'their', 'methodological', 'counterparts', 'those_who', 'viewed', 'anarchism', 'as', 'a', 'way', 'of', 'acting', 'or', 'a', 'historical', 'tendency', 'against', 'illegitimate', 'authority', 'as', 'an', 'anti', 'ideological', 'position', 'small', 'a', 'anarchism', 'shares', 'some', 'similarities', 'with', 'post', 'left_anarchy', 'david', 'graeber', 'and', 'andrej', 'grubacic', 'offer', 'an_alternative', 'use', 'of', 'the', 'term', 'applying', 'it', 'to', 'groups', 'and', 'movements', 'organising', 'according_to', 'or', 'acting', 'in', 'a', 'manner_consistent', 'with', 'anarchist_principles', 'of', 'decentralisation', 'voluntary_association', 'mutual_aid', 'the', 'network', 'model', 'and', 'crucially', 'the', 'rejection', 'of', 'any', 'idea', 'that', 'the', 'end', 'justifies', 'the', 'means', 'let_alone', 'that', 'the', 'business', 'of', 'a', 'revolutionary', 'is', 'to', 'seize', 'state', 'power', 'and', 'then', 'begin', 'imposing', 'one', 's', 'vision', 'at', 'the', 'point', 'of', 'a', 'gun', 'other', 'issues', 'conceptions', 'of', 'an', 'anarchist_society', 'many', 'political', 'philosophers', 'justify', 'support', 'of', 'the', 'state', 'as', 'a', 'means', 'of', 'regulating', 'violence', 'so', 'that', 'the', 'destruction', 'caused_by', 'human', 'conflict', 'is', 'minimized', 'and', 'fair', 'relationships', 'are', 'established', 'anarchists', 'argue_that', 'pursuit', 'of', 'these', 'ends', 'does_not', 'justify', 'the', 'establishment', 'of', 'a', 'state', 'many', 'argue_that', 'the', 'state', 'is', 'incompatible_with', 'those', 'goals', 'and', 'the', 'cause', 'of', 'chaos', 'violence', 'and', 'war', 'anarchists', 'argue_that', 'the', 'state', 'helps', 'to', 'create', 'a', 'monopoly', 'on', 'violence', 'and', 'uses', 'violence', 'to', 'advance', 'elite', 'interests', 'much', 'effort', 'has_been', 'dedicated', 'to', 'explaining_how', 'anarchist', 'societies', 'would', 'handle', 'criminality', 'see_also', 'anarchism', 'and', 'society', 'civil_rights', 'and', 'cultural', 'sovereignty', 'black', 'anarchism', 'opposes', 'the', 'existence', 'of', 'a', 'state', 'capitalism', 'and', 'subjugation', 'and', 'domination', 'of', 'people', 'of', 'color', 'and', 'favors', 'a', 'non', 'hierarchical_organization', 'of', 'society', 'theorists', 'include', 'ashanti', 'alston', 'lorenzo', 'komboa', 'ervin', 'and', 'sam', 'mbah', 'anarchist', 'people', 'of', 'color', 'was', 'created', 'as', 'a', 'forum', 'for', 'non', 'caucasian', 'anarchists', 'to', 'express', 'their', 'thoughts', 'about', 'racial', 'issues', 'within', 'the', 'anarchist_movement', 'particularly', 'within', 'the', 'united_states', 'national', 'anarchism', 'is', 'a', 'political', 'view', 'which', 'seeks', 'to', 'unite', 'cultural', 'or', 'ethnic', 'preservation', 'with', 'anarchist', 'views', 'its_adherents', 'propose', 'that', 'those', 'preventing', 'ethnic_groups', 'or', 'races', 'from', 'living', 'in', 'separate', 'autonomous', 'groupings', 'should_be', 'resisted', 'anti_racist', 'action', 'is', 'not', 'an', 'anarchist', 'group', 'but', 'many', 'anarchists', 'are', 'involved', 'it', 'focuses_on', 'publicly', 'confronting', 'racist', 'agitators', 'the', 'zapatista', 'movement', 'of', 'chiapas_mexico', 'is', 'a', 'cultural', 'sovereignty', 'group', 'with', 'some', 'anarchist', 'proclivities', 'neocolonialism', 'and', 'globalization', 'nearly_all', 'anarchists', 'oppose', 'neocolonialism', 'as', 'an_attempt', 'to', 'use', 'economic_coercion', 'on', 'a', 'global_scale', 'carried_out', 'through', 'state', 'institutions', 'such_as', 'the', 'world_bank', 'world_trade', 'organization', 'group', 'of', 'eight', 'and', 'the', 'world', 'economic_forum', 'globalization', 'is', 'an', 'ambiguous', 'term', 'that', 'has', 'different_meanings', 'to', 'different', 'anarchist', 'factions', 'most', 'anarchists', 'use', 'the', 'term', 'to', 'mean', 'neocolonialism', 'and', 'or', 'cultural_imperialism', 'which', 'they', 'may', 'see', 'as', 'related', 'many', 'are', 'active', 'in', 'the', 'anti_globalization', 'movement', 'others', 'particularly', 'anarcho_capitalists', 'use', 'globalization', 'to', 'mean', 'the', 'worldwide', 'expansion', 'of', 'the', 'division', 'of', 'labor', 'and', 'trade', 'which', 'they', 'see', 'as', 'beneficial', 'so', 'long', 'as', 'governments', 'do_not', 'intervene', 'parallel', 'structures', 'many', 'anarchists', 'try', 'to', 'set_up', 'alternatives', 'to', 'state', 'supported', 'institutions', 'and', 'outposts', 'such_as', 'food', 'not', 'bombs', 'infoshops', 'educational', 'systems', 'such_as', 'home_schooling', 'neighborhood', 'mediation_arbitration', 'groups', 'and', 'so', 'on', 'the', 'idea', 'is', 'to', 'create', 'the', 'structures', 'for', 'a', 'new', 'anti_authoritarian', 'society', 'in', 'the', 'shell', 'of', 'the', 'old', 'authoritarian', 'one', 'technology', 'recent', 'technological_developments', 'have', 'made', 'the', 'anarchist', 'cause', 'both', 'easier', 'to', 'advance', 'and', 'more', 'conceivable', 'to', 'people', 'many', 'people', 'use', 'the', 'internet', 'to', 'form', 'on', 'line', 'communities', 'intellectual_property', 'is', 'undermined', 'and', 'a', 'gift', 'culture', 'supported_by', 'sharing', 'music', 'files', 'open_source', 'programming', 'and', 'the', 'free_software', 'movement', 'these', 'cyber', 'communities', 'include', 'the', 'gnu_linux', 'indymedia', 'and', 'wiki', 'some', 'anarchists', 'see', 'information_technology', 'as', 'the', 'best', 'weapon', 'to', 'defeat', 'authoritarianism', 'some', 'even', 'think', 'the', 'information', 'age', 'makes', 'eventual', 'anarchy', 'inevitable', 'see_also', 'crypto_anarchism', 'and', 'cypherpunk', 'pacifism', 'some', 'anarchists', 'consider', 'pacifism', 'opposition', 'to', 'war', 'to', 'be', 'inherent', 'in', 'their', 'philosophy', 'anarcho', 'pacifists', 'take', 'it', 'further', 'and', 'follow', 'leo_tolstoy', 's', 'belief', 'in', 'non_violence', 'anarchists', 'see', 'war', 'as', 'an', 'activity', 'in', 'which', 'the', 'state', 'seeks', 'to', 'gain', 'and', 'consolidate_power', 'both', 'domestically', 'and', 'in', 'foreign', 'lands', 'and', 'subscribe', 'to', 'randolph', 'bourne', 's', 'view', 'that', 'war', 'is', 'the', 'health', 'of', 'the', 'state', 'a_lot', 'of', 'anarchist', 'activity', 'has_been', 'anti', 'war', 'based', 'parliamentarianism', 'in', 'general', 'terms', 'the', 'anarchist', 'ethos', 'opposes', 'voting', 'in', 'elections', 'because', 'voting', 'amounts', 'to', 'condoning', 'the', 'state', 'voluntaryism', 'is', 'an', 'anarchist', 'school', 'of', 'thought', 'which', 'emphasizes', 'tending', 'your_own', 'garden', 'and', 'neither', 'ballots', 'nor', 'bullets', 'the', 'anarchist', 'case', 'against', 'voting', 'is', 'explained', 'in', 'the', 'ethics', 'of', 'voting', 'by', 'george_h', 'smith', 'also', 'see', 'voting', 'anarchists', 'an_oxymoron', 'or', 'what', 'by', 'joe', 'peacott', 'and', 'writings', 'by', 'fred', 'woodworth', 'sectarianism', 'most', 'anarchist', 'schools', 'of', 'thought', 'are', 'to', 'some_degree', 'sectarian', 'there', 'is', 'often', 'a', 'difference', 'of', 'opinion', 'within', 'each', 'school', 'about', 'how', 'to', 'react', 'to', 'or', 'interact_with', 'other', 'schools', 'some', 'such_as', 'panarchists', 'believe_that', 'it', 'is', 'possible', 'for', 'a', 'variety', 'of', 'modes', 'of', 'social', 'life', 'to', 'coexist', 'and', 'compete', 'some', 'anarchists', 'view', 'opposing', 'schools', 'as', 'a', 'social', 'impossibility', 'and', 'resist', 'interaction', 'others', 'see', 'opportunities_for', 'coalition', 'building', 'or', 'at_least', 'temporary', 'alliances', 'for', 'specific_purposes', 'see', 'anarchism', 'without', 'adjectives', 'criticisms', 'of', 'anarchism', 'main_article', 'criticisms', 'of', 'anarchism', 'violence', 'since', 'anarchism', 'has', 'often', 'been', 'associated_with', 'violence', 'and', 'destruction', 'some', 'people', 'have', 'seen', 'it', 'as', 'being', 'too', 'violent', 'on', 'the', 'other_hand', 'hand', 'frederick_engels', 'criticsed', 'anarchists', 'for', 'not', 'being', 'violent', 'enough', 'a', 'revolution', 'is', 'certainly', 'the', 'most', 'authoritarian', 'thing', 'there', 'is', 'it', 'is', 'the', 'act', 'whereby', 'one', 'part', 'of', 'the', 'population', 'imposes', 'its', 'will', 'upon', 'the', 'other', 'part', 'by', 'means', 'of', 'rifles', 'bayonets', 'and', 'cannon', 'authoritarian', 'means', 'if', 'such', 'there', 'be', 'at', 'all', 'and', 'if', 'the', 'victorious', 'party', 'does_not', 'want', 'to', 'have', 'fought', 'in', 'vain', 'it', 'must', 'maintain', 'this', 'rule', 'by', 'means', 'of', 'the', 'terror', 'which', 'its', 'arms', 'inspire', 'in', 'the', 'reactionists', 'would', 'the', 'paris_commune', 'have', 'lasted', 'a', 'single', 'day', 'if', 'it', 'had', 'not', 'made', 'use', 'of', 'this', 'authority', 'of', 'the', 'armed', 'people', 'against', 'the', 'bourgeois', 'utopianism', 'anarchism', 'is', 'often_criticised', 'as', 'unfeasible', 'or', 'plain', 'utopian', 'even', 'by', 'many', 'who', 'agree_that', 'it', 's', 'a', 'nice', 'idea', 'in', 'principle', 'for_example', 'carl', 'landauer', 'in', 'his', 'book', 'european', 'socialism', 'criticizes', 'anarchism', 'as', 'being', 'unrealistically', 'utopian', 'and', 'holds_that', 'government', 'is', 'a', 'lesser', 'evil', 'than', 'a', 'society', 'without', 'repressive', 'force', 'he', 'holds_that', 'the', 'belief_that', 'ill', 'intentions', 'will_cease', 'if', 'repressive', 'force', 'disappears', 'is', 'an', 'absurdity', 'however', 'it', 'must_be', 'noted_that', 'not', 'all', 'anarchists', 'have', 'such', 'a', 'utopian', 'view', 'of', 'anarchism', 'for_example', 'some', 'such_as', 'benjamin_tucker', 'advocate', 'privately_funded', 'institutions', 'that', 'defend', 'individual_liberty', 'and', 'property', 'however', 'other', 'anarchists', 'such_as', 'sir_herbert', 'read', 'proudly', 'accept', 'the', 'characterization', 'utopian', 'class', 'character', 'marxists', 'have', 'characterised', 'anarchism', 'as', 'an', 'expression', 'of', 'the', 'class', 'interests', 'of', 'the', 'petite', 'bourgeoisie', 'or', 'perhaps', 'the', 'lumpenproletariat', 'see', 'e_g', 'plekhanov', 'for', 'a', 'marxist_critique', 'of', 'one', 'eight', 'nine', 'five', 'anarchists', 'have', 'also', 'been', 'characterised', 'as', 'spoilt', 'middle_class', 'dilettantes', 'most_recently', 'in', 'relation', 'to', 'anti_capitalist', 'protesters', 'tacit', 'authoritarianism', 'in', 'recent_decades', 'anarchism', 'has_been', 'criticised', 'by', 'situationists', 'post', 'anarchists', 'and', 'others', 'of', 'preserving', 'tacitly', 'statist', 'authoritarian', 'or', 'bureaucratic', 'tendencies', 'behind', 'a', 'dogmatic', 'facade', 'hypocrisy', 'some_critics', 'point', 'to', 'the', 'sexist', 'and', 'racist_views', 'of', 'some', 'prominent', 'anarchists', 'notably', 'proudhon', 'and', 'bakunin', 'as', 'examples', 'of', 'hypocrisy', 'inherent', 'within', 'anarchism', 'while', 'many', 'anarchists', 'however', 'dismiss', 'that', 'the', 'personal', 'prejudices', 'of', 'one', 'nine', 'th_century', 'theorists', 'influence', 'the', 'beliefs', 'of', 'present_day', 'anarchists', 'others', 'criticise', 'modern', 'anarchism', 'for', 'continuing', 'to', 'be', 'eurocentric', 'and', 'reference', 'the', 'impact', 'of', 'anarchist', 'thinkers', 'like', 'proudhon', 'on', 'fascism', 'through', 'groups', 'like', 'cercle', 'proudhon', 'anarcho_capitalist', 'bryan', 'caplan', 'argues_that', 'the', 'treatment', 'of', 'fascists', 'and', 'suspected', 'fascist', 'sympathizers', 'by', 'spanish', 'anarchists', 'in', 'the', 'spanish_civil', 'war', 'was', 'a', 'form', 'of', 'illegitimate', 'coercion', 'making', 'the', 'proffessed', 'anarchists', 'ultimately', 'just', 'a', 'third', 'faction', 'of', 'totalitarians', 'alongside', 'the', 'communists', 'and', 'fascists', 'he', 'also', 'criticizes', 'the', 'willingness', 'of', 'the', 'cnt', 'to', 'join', 'the', 'statist', 'republican', 'government', 'during', 'the', 'civil_war', 'and', 'references', 'stanley', 'g', 'payne', 's', 'book', 'on', 'the', 'franco_regime', 'which', 'claims', 'that', 'the', 'cnt', 'entered', 'negotiations', 'with', 'the', 'fascist', 'government', 'six', 'years', 'after', 'the', 'war', 'cultural', 'phenomena', 'noam_chomsky', 'one', 'nine', 'two', 'eight', 'the', 'kind', 'of', 'anarchism', 'that', 'is', 'most', 'easily', 'encountered', 'in', 'popular_culture', 'is', 'represented_by', 'celebrities', 'who', 'publicly', 'identify_themselves', 'as', 'anarchists', 'although', 'some', 'anarchists', 'reject_any', 'focus_on', 'such', 'famous', 'living', 'individuals', 'as', 'inherently', 'litist', 'the', 'following', 'figures', 'are', 'examples', 'of', 'prominent', 'publicly', 'self', 'avowed', 'anarchists', 'the', 'mit', 'professor', 'of', 'linguistics', 'noam_chomsky', 'the', 'science_fiction', 'author', 'ursula_k', 'le_guin', 'the', 'social', 'historian', 'howard_zinn', 'entertainer', 'and', 'author', 'hans', 'alfredsson', 'the', 'avant_garde', 'artist', 'nicol', 's', 'rossell', 'in', 'denmark', 'the', 'freetown', 'christiania', 'was', 'created', 'in', 'downtown', 'copenhagen', 'the', 'housing', 'and', 'employment', 'crisis', 'in', 'most', 'of', 'western_europe', 'led', 'to', 'the', 'formation', 'of', 'communes', 'and', 'squatter', 'movements', 'like', 'the', 'one', 'still', 'thriving', 'in', 'barcelona', 'in', 'catalonia', 'militant', 'resistance', 'to', 'neo_nazi', 'groups', 'in', 'places', 'like', 'germany', 'and', 'the', 'uprisings', 'of', 'autonomous', 'marxism', 'situationist', 'and', 'autonomist', 'groups', 'in', 'france', 'and', 'italy', 'also', 'helped', 'to', 'give', 'popularity', 'to', 'anti_authoritarian', 'non', 'capitalist', 'ideas', 'in', 'various', 'musical_styles', 'anarchism', 'rose', 'in', 'popularity', 'most_famous', 'for', 'the', 'linking', 'of', 'anarchist_ideas', 'and', 'music', 'has_been', 'punk_rock', 'although', 'in', 'the', 'modern', 'age', 'hip_hop', 'and', 'folk_music', 'are', 'also', 'becoming', 'important', 'mediums', 'for', 'the', 'spreading', 'of', 'the', 'anarchist', 'message', 'in', 'the', 'uk', 'this', 'was', 'associated_with', 'the', 'punk_rock', 'movement', 'the', 'band_crass', 'is', 'celebrated', 'for', 'its', 'anarchist', 'and', 'pacifist', 'ideas', 'the', 'dutch', 'punk_band', 'the', 'ex', 'further', 'exemplifies', 'this', 'expression', 'for', 'further_details', 'see', 'anarcho_punk', 'see_also', 'there_are', 'many', 'concepts', 'relevant', 'to', 'the', 'topic', 'of', 'anarchism', 'this', 'is', 'a', 'brief_summary', 'there', 'is', 'also', 'a', 'more', 'extensive', 'list', 'of', 'anarchist', 'concepts', 'individualist_anarchism', 'anarcho_communism', 'anarcho_syndicalism', 'anarcho_capitalism', 'mutualism', 'christian_anarchism', 'anarcha_feminism', 'green_anarchism', 'nihilist', 'anarchism_anarcho', 'nationalism', 'black', 'anarchism', 'national', 'anarchism', 'post', 'anarchism', 'post', 'left', 'anarchism', 'libertarian_socialism', 'anarchist', 'symbolism', 'list', 'of', 'anarchism', 'links', 'list', 'of', 'anarchists', 'list', 'of', 'anarchist_organizations', 'major', 'conflicts', 'within', 'anarchist', 'thought', 'past', 'and', 'present', 'anarchist', 'communities', 'historical_events', 'paris_commune', 'one', 'eight', 'seven', 'one', 'haymarket_riot', 'one', 'eight', 'eight', 'six', 'the', 'makhnovschina', 'one', 'nine', 'one', 'seven', 'one', 'nine', 'two', 'one', 'kronstadt_rebellion', 'one', 'nine', 'two', 'one', 'spanish', 'revolution', 'one', 'nine', 'three', 'six', 'see', 'anarchism', 'in', 'spain', 'and', 'spanish', 'revolution', 'may', 'one', 'nine', 'six', 'eight', 'france', 'one', 'nine', 'six', 'eight', 'wto', 'meeting', 'in', 'seattle', 'one', 'nine', 'nine', 'nine', 'books', 'the', 'following', 'is', 'a', 'sample', 'of', 'books', 'that', 'have_been', 'referenced', 'in', 'this', 'page', 'a', 'more', 'complete_list', 'can_be', 'found', 'at', 'the', 'list', 'of', 'anarchist', 'books', 'mikhail_bakunin', 'god', 'and', 'the', 'state', 'emma_goldman', 'anarchism', 'other', 'essays', 'peter_kropotkin', 'mutual_aid', 'pierre_joseph', 'proudhon', 'what', 'is', 'property', 'rudolf_rocker', 'anarcho_syndicalism', 'murray_rothbard', 'the', 'ethics', 'of', 'liberty', 'max_stirner', 'the', 'ego', 'and', 'its_own', 'leo_tolstoy', 'the', 'kingdom', 'of', 'god', 'is', 'within', 'you', 'anarchism', 'by', 'region', 'culture', 'african', 'anarchism', 'anarchism', 'in', 'spain', 'anarchism', 'in', 'the', 'english', 'tradition', 'chinese', 'anarchism', 'references', 'these', 'notes', 'have', 'no', 'corresponding', 'reference', 'in', 'the', 'article', 'they', 'might_be', 're', 'used', 'against', 'politics', 'appleton', 'boston', 'anarchists', 'yarros', 'victor', 'liberty', 'vii', 'january', 'two', 'one', 'eight', 'nine', 'two', 'noam_chomsky', 'on', 'anarchism', 'by', 'noam_chomsky', 'external_links', 'the', 'overwhelming', 'diversity', 'and', 'number', 'of', 'links', 'relating_to', 'anarchism', 'is', 'extensively_covered', 'on', 'the', 'links', 'subpage', 'anarchoblogs', 'blogs', 'by', 'anarchists', 'anarchy', 'archives', 'extensively', 'archives', 'information', 'relating_to', 'famous', 'anarchists', 'this', 'includes', 'many', 'of', 'their', 'books', 'and', 'other', 'publications', 'hundreds', 'of', 'anarchists', 'are_listed', 'with', 'short', 'bios', 'links', 'dedicated', 'pages', 'at', 'the', 'daily', 'bleed', 's', 'anarchist', 'encyclopedia', 'infoshop_org', 'wikipedia_page', 'industrial_workers', 'of', 'the', 'world', 'anarchism', 'forms', 'of', 'government', 'political_ideology', 'entry', 'points', 'political', 'theories', 'social', 'philosophy', 'autism', 'is', 'classified_as', 'a', 'neurodevelopmental', 'disorder', 'that', 'manifests_itself', 'in', 'markedly', 'abnormal', 'social_interaction', 'communication', 'ability', 'patterns', 'of', 'interests', 'and', 'patterns', 'of', 'behavior', 'although', 'the', 'specific', 'etiology', 'of', 'autism', 'is', 'unknown', 'many', 'researchers', 'suspect', 'that', 'autism', 'results', 'from', 'genetically', 'mediated', 'vulnerabilities', 'to', 'environmental', 'triggers', 'and', 'while', 'there', 'is', 'disagreement_about', 'the', 'magnitude', 'nature', 'and', 'mechanisms', 'for', 'such', 'environmental_factors', 'researchers_have', 'found', 'at_least', 'seven', 'major', 'genes', 'prevalent_among', 'individuals', 'diagnosed', 'as', 'autistic', 'some', 'estimate', 'that', 'autism', 'occurs', 'in', 'as', 'many', 'as', 'one', 'united_states', 'child', 'in', 'one', 'six', 'six', 'however', 'the', 'national_institute', 'of', 'mental_health', 'gives', 'a', 'more', 'conservative_estimate', 'of', 'one', 'in', 'one', 'zero', 'zero', 'zero', 'for', 'families', 'that', 'already', 'have', 'one', 'autistic_child', 'the', 'odds', 'of', 'a', 'second', 'autistic_child', 'may_be', 'as', 'high', 'as', 'one', 'in', 'twenty', 'diagnosis', 'is', 'based_on', 'a', 'list', 'of', 'psychiatric', 'criteria', 'and', 'a', 'series', 'of', 'standardized', 'clinical_tests', 'may', 'also', 'be', 'used', 'autism', 'may', 'not', 'be', 'physiologically', 'obvious', 'a', 'complete', 'physical', 'and', 'neurological', 'evaluation', 'will', 'typically', 'be', 'part', 'of', 'diagnosing', 'autism', 'some', 'now', 'speculate_that', 'autism', 'is', 'not', 'a', 'single', 'condition', 'but', 'a', 'group', 'of', 'several', 'distinct', 'conditions', 'that', 'manifest', 'in', 'similar', 'ways', 'by', 'definition', 'autism', 'must', 'manifest', 'delays', 'in', 'social_interaction', 'language', 'as', 'used', 'in', 'social', 'communication', 'or', 'symbolic', 'or', 'imaginative', 'play', 'with', 'onset', 'prior_to', 'age', 'three', 'years', 'according_to', 'the', 'diagnostic', 'and', 'statistical_manual', 'of', 'mental_disorders', 'the', 'icd', 'one', 'zero', 'also', 'says_that', 'symptoms', 'must', 'manifest', 'before', 'the', 'age', 'of', 'three', 'years', 'there', 'have_been', 'large', 'increases', 'in', 'the', 'reported', 'incidence', 'of', 'autism', 'for', 'reasons', 'that', 'are', 'heavily_debated', 'by', 'researchers', 'in', 'psychology', 'and', 'related_fields', 'within', 'the', 'scientific_community', 'some', 'children', 'with', 'autism', 'have', 'improved', 'their', 'social', 'and', 'other', 'skills', 'to', 'the', 'point', 'where', 'they', 'can', 'fully', 'participate', 'in', 'mainstream', 'education', 'and', 'social', 'events', 'but', 'there_are', 'lingering', 'concerns', 'that', 'an', 'absolute', 'cure', 'from', 'autism', 'is', 'impossible', 'with', 'current', 'technology', 'however', 'many', 'autistic_children', 'and', 'adults', 'who', 'are', 'able_to', 'communicate', 'at_least', 'in', 'writing', 'are', 'opposed', 'to', 'attempts', 'to', 'cure', 'their', 'conditions', 'and', 'see', 'such', 'conditions', 'as', 'part', 'of', 'who', 'they', 'are', 'history', 'dr', 'hans_asperger', 'described', 'a', 'form', 'of', 'autism', 'in', 'the', 'one', 'nine', 'four', 'zero', 's', 'that', 'later_became', 'known_as', 'asperger_s', 'syndrome', 'the', 'word', 'autism', 'was', 'first', 'used', 'in', 'the', 'english_language', 'by', 'swiss_psychiatrist', 'eugene_bleuler', 'in', 'a', 'one', 'nine', 'one', 'two', 'number', 'of', 'the', 'american', 'journal', 'of', 'insanity', 'it', 'comes_from', 'the', 'greek_word', 'for', 'self', 'however', 'the', 'classification', 'of', 'autism', 'did_not', 'occur', 'until', 'the', 'middle', 'of', 'the', 'twentieth_century', 'when', 'in', 'one', 'nine', 'four', 'three', 'psychiatrist_dr', 'leo', 'kanner', 'of', 'the', 'johns_hopkins', 'hospital', 'in', 'baltimore', 'reported', 'on', 'one', 'one', 'child', 'patients', 'with', 'striking', 'behavioral', 'similarities', 'and', 'introduced', 'the', 'label', 'early_infantile', 'autism', 'he', 'suggested', 'autism', 'from', 'the', 'greek', 'autos', 'meaning', 'self', 'to', 'describe', 'the', 'fact_that', 'the', 'children', 'seemed', 'to', 'lack', 'interest', 'in', 'other', 'people', 'although', 'kanner', 's', 'first', 'paper', 'on', 'the', 'subject', 'was', 'published', 'in', 'a', 'now_defunct', 'journal', 'the', 'nervous', 'child', 'almost_every', 'characteristic', 'he', 'originally', 'described', 'is', 'still', 'regarded_as', 'typical', 'of', 'the', 'autistic_spectrum', 'of', 'disorders', 'at', 'the', 'same_time', 'an', 'austrian', 'scientist', 'dr', 'hans_asperger', 'described', 'a', 'different', 'form', 'of', 'autism', 'that', 'became_known', 'as', 'asperger_s', 'syndrome', 'but', 'the', 'widespread', 'recognition', 'of', 'asperger_s', 'work', 'was', 'delayed', 'by', 'world_war', 'ii', 'in', 'germany', 'and', 'by', 'the', 'fact_that', 'his', 'seminal', 'paper', 'wasn_t', 'translated_into', 'english', 'for', 'almost', 'five', 'zero', 'years', 'the', 'majority', 'of', 'his', 'work', 'wasn_t', 'widely_read', 'until', 'one', 'nine', 'nine', 'seven', 'thus', 'these', 'two', 'conditions', 'were', 'described', 'and', 'are', 'today', 'listed', 'in', 'the', 'diagnostic', 'and', 'statistical_manual', 'of', 'mental_disorders', 'dsm_iv', 'tr', 'fourth_edition', 'text', 'revision', 'one', 'as', 'two', 'of', 'the', 'five', 'pervasive_developmental', 'disorders', 'pdd', 'more', 'often_referred', 'to', 'today', 'as', 'autism_spectrum', 'disorders', 'asd', 'all', 'of', 'these', 'conditions', 'are', 'characterized_by', 'varying_degrees', 'of', 'difference', 'in', 'communication_skills', 'social_interactions', 'and', 'restricted', 'repetitive', 'and', 'stereotyped', 'patterns', 'of', 'behavior', 'few', 'clinicians', 'today', 'solely', 'use', 'the', 'dsm_iv', 'criteria_for', 'determining', 'a', 'diagnosis', 'of', 'autism', 'which', 'are', 'based_on', 'the', 'absence', 'or', 'delay', 'of', 'certain', 'developmental', 'milestones', 'many', 'clinicians', 'instead', 'use', 'an_alternate', 'means', 'or', 'a', 'combination_thereof', 'to', 'more_accurately', 'determine', 'a', 'diagnosis', 'terminology', 'when_referring', 'to', 'someone', 'diagnosed_with', 'autism', 'the', 'term', 'autistic', 'is', 'often_used', 'however', 'the', 'term', 'person', 'with', 'autism', 'can_be', 'used', 'instead', 'this', 'is', 'referred_to', 'as', 'person', 'first', 'terminology', 'the', 'autistic', 'community', 'generally', 'prefers', 'the', 'term', 'autistic', 'for', 'reasons', 'that', 'are', 'fairly', 'controversial', 'this_article', 'uses', 'the', 'term', 'autistic', 'see', 'talk_page', 'characteristics', 'dr_leo', 'kanner', 'introduced', 'the', 'label', 'early_infantile', 'autism', 'in', 'one', 'nine', 'four', 'three', 'there', 'is', 'a', 'great_diversity', 'in', 'the', 'skills', 'and', 'behaviors', 'of', 'individuals', 'diagnosed', 'as', 'autistic', 'and', 'physicians', 'will', 'often', 'arrive_at', 'different', 'conclusions_about', 'the', 'appropriate', 'diagnosis', 'much', 'of', 'this', 'is', 'due_to', 'the', 'sensory', 'system', 'of', 'an', 'autistic', 'which', 'is', 'quite_different', 'from', 'the', 'sensory', 'system', 'of', 'other', 'people', 'since', 'certain', 'stimulations', 'can_affect', 'an', 'autistic', 'differently', 'than', 'a', 'non_autistic', 'and', 'the', 'degree', 'to', 'which', 'the', 'sensory', 'system', 'is', 'affected', 'varies_wildly', 'from', 'one', 'autistic', 'person', 'to', 'another', 'nevertheless', 'professionals', 'within', 'pediatric', 'care', 'and', 'development', 'often', 'look', 'for', 'early', 'indicators', 'of', 'autism', 'in', 'order', 'to', 'initiate', 'treatment', 'as', 'early', 'as', 'possible', 'however', 'some', 'people', 'do_not', 'believe', 'in', 'treatment', 'for', 'autism', 'either', 'because_they', 'do_not', 'believe', 'autism', 'is', 'a', 'disorder', 'or', 'because_they', 'believe', 'treatment', 'can', 'do', 'more', 'harm', 'than', 'good', 'social', 'development', 'typically', 'developing', 'infants', 'are', 'social', 'beings', 'early', 'in', 'life', 'they_do', 'such', 'things', 'as', 'gaze', 'at', 'people', 'turn', 'toward', 'voices', 'grasp', 'a', 'finger', 'and', 'even', 'smile', 'in', 'contrast', 'most', 'autistic_children', 'prefer', 'objects', 'to', 'faces', 'and', 'seem', 'to', 'have', 'tremendous', 'difficulty_learning', 'to', 'engage', 'in', 'the', 'give', 'and', 'take', 'of', 'everyday', 'human_interaction', 'even', 'in', 'the', 'first', 'few_months', 'of', 'life', 'many', 'seem', 'indifferent', 'to', 'other', 'people', 'because_they', 'avoid', 'eye_contact', 'and', 'do_not', 'interact_with', 'them', 'as', 'often', 'as', 'non_autistic', 'children', 'children', 'with', 'autism', 'often', 'appear', 'to', 'prefer', 'being', 'alone', 'to', 'the', 'company', 'of', 'others', 'and', 'may', 'passively', 'accept', 'such', 'things', 'as', 'hugs', 'and', 'cuddling', 'without', 'reciprocating', 'or', 'resist', 'attention', 'altogether', 'later', 'they', 'seldom', 'seek', 'comfort', 'from', 'others', 'or', 'respond', 'to', 'parents', 'displays', 'of', 'anger', 'or', 'affection', 'in', 'a', 'typical', 'way', 'research', 'has', 'suggested_that', 'although', 'autistic_children', 'are', 'attached', 'to', 'their', 'parents', 'their', 'expression', 'of', 'this', 'attachment', 'is', 'unusual', 'and', 'difficult', 'to', 'interpret', 'parents', 'who', 'looked_forward', 'to', 'the', 'joys', 'of', 'cuddling', 'teaching', 'and', 'playing', 'with', 'their', 'child', 'may', 'feel', 'crushed', 'by', 'this', 'lack', 'of', 'expected', 'attachment', 'behavior', 'children', 'with', 'autism', 'appear', 'to', 'lack', 'theory', 'of', 'mind', 'the', 'ability', 'to', 'see', 'things', 'from', 'another_person', 's', 'perspective', 'a', 'behavior', 'cited', 'as', 'exclusive', 'to', 'human_beings', 'above', 'the', 'age', 'of', 'five', 'and', 'possibly', 'other', 'higher', 'primates', 'such_as', 'adult', 'gorillas', 'chimpanzees', 'and', 'bonobos', 'typical', 'five', 'year_olds', 'can', 'develop', 'insights_into', 'other', 'people', 's', 'different', 'knowledge', 'feelings', 'and', 'intentions', 'interpretations', 'based_upon', 'social', 'cues', 'e_g', 'gestures', 'facial_expressions', 'an_individual', 'with', 'autism', 'seems', 'to', 'lack', 'these', 'interpretation', 'skills', 'an', 'inability', 'that', 'leaves', 'them', 'unable_to', 'predict', 'or', 'understand', 'other', 'people', 's', 'actions', 'the', 'social', 'alienation', 'of', 'autistic', 'and', 'asperger_s', 'people', 'is', 'so', 'intense', 'from', 'childhood', 'that', 'many', 'of', 'them', 'have', 'imaginary_friends', 'as', 'companionship', 'however', 'having', 'an_imaginary', 'friend', 'is', 'not_necessarily', 'a', 'sign', 'of', 'autism', 'and', 'also', 'occurs', 'in', 'non_autistic', 'children', 'although', 'not', 'universal', 'it', 'is', 'common', 'for', 'autistic_people', 'to', 'not', 'regulate', 'their', 'behavior', 'this', 'can', 'take', 'the', 'form', 'of', 'crying', 'or', 'verbal', 'outbursts', 'that', 'may_seem', 'out', 'of', 'proportion', 'to', 'the', 'situation', 'individuals', 'with', 'autism', 'generally', 'prefer', 'consistent', 'routines', 'and', 'environments', 'they', 'may', 'react', 'negatively', 'to', 'changes', 'in', 'them', 'it', 'is', 'not_uncommon', 'for', 'these', 'individuals', 'to', 'exhibit', 'aggression', 'increased', 'levels', 'of', 'self', 'stimulatory', 'behavior', 'self', 'injury', 'or', 'extensive', 'withdrawal', 'in', 'overwhelming', 'situations', 'sensory', 'system', 'a', 'key', 'indicator', 'to', 'clinicians', 'making', 'a', 'proper', 'assessment', 'for', 'autism', 'would', 'include', 'looking', 'for', 'symptoms', 'much_like', 'those', 'found', 'in', 'sensory', 'integration', 'dysfunction', 'children', 'will', 'exhibit', 'problems', 'coping_with', 'the', 'normal', 'sensory_input', 'indicators', 'of', 'this', 'disorder', 'include', 'oversensitivity', 'or', 'underreactivity', 'to', 'touch', 'movement', 'sights', 'or', 'sounds', 'physical', 'clumsiness', 'or', 'carelessness', 'poor', 'body', 'awareness', 'a', 'tendency', 'to', 'be', 'easily', 'distracted', 'impulsive', 'physical', 'or', 'verbal_behavior', 'an', 'activity', 'level', 'that', 'is', 'unusually_high', 'or', 'low', 'not', 'unwinding', 'or', 'calming', 'oneself', 'difficulty_learning', 'new', 'movements', 'difficulty', 'in', 'making', 'transitions', 'from', 'one', 'situation', 'to', 'another', 'social', 'and', 'or', 'emotional_problems', 'delays', 'in', 'speech', 'language', 'or', 'motor_skills', 'specific', 'learning_difficulties', 'delays', 'in', 'academic_achievement', 'one', 'common', 'example', 'is', 'an_individual', 'with', 'autism', 'hearing', 'a', 'person', 'with', 'autism', 'may_have', 'trouble', 'hearing', 'certain', 'people', 'while', 'other', 'people', 'are', 'louder_than', 'usual', 'or', 'the', 'person', 'with', 'autism', 'may_be', 'unable_to', 'filter', 'out', 'sounds', 'in', 'certain_situations', 'such_as', 'in', 'a', 'large', 'crowd', 'of', 'people', 'see', 'cocktail', 'party', 'effect', 'however', 'this', 'is', 'perhaps', 'the', 'part', 'of', 'the', 'autism', 'that', 'tends_to', 'vary', 'the', 'most', 'from', 'person', 'to', 'person', 'so', 'these', 'examples', 'may', 'not_apply', 'to', 'every', 'autistic', 'it', 'should_be', 'noted_that', 'sensory', 'difficulties', 'although', 'reportedly', 'common', 'in', 'autistics', 'are', 'not', 'part', 'of', 'the', 'dsm_iv', 'diagnostic_criteria', 'for', 'autistic', 'disorder', 'communication', 'difficulties', 'by', 'age', 'three', 'typical', 'children', 'have', 'passed', 'predictable', 'language', 'learning', 'milestones', 'one', 'of', 'the', 'earliest', 'is', 'babbling', 'by', 'the', 'first', 'birthday', 'a', 'typical', 'toddler', 'says', 'words', 'turns', 'when', 'he', 'or', 'she', 'hears', 'his', 'or', 'her', 'name', 'points', 'when', 'he', 'or', 'she_wants', 'a', 'toy', 'and', 'when', 'offered', 'something', 'distasteful', 'makes', 'it', 'clear', 'that', 'the', 'answer', 'is', 'no', 'speech', 'development', 'in', 'people', 'with', 'autism', 'takes', 'different_paths', 'some', 'remain', 'mute', 'throughout', 'their_lives', 'while', 'being', 'fully', 'literate', 'and', 'able_to', 'communicate', 'in', 'other', 'ways', 'images', 'sign_language', 'and', 'typing', 'are', 'far_more', 'natural', 'to', 'them', 'some', 'infants', 'who', 'later', 'show_signs', 'of', 'autism', 'coo', 'and', 'babble', 'during', 'the', 'first', 'few_months', 'of', 'life', 'but', 'stop', 'soon_afterwards', 'others', 'may_be', 'delayed', 'developing', 'language', 'as', 'late', 'as', 'the', 'teenage_years', 'still', 'inability_to', 'speak', 'does_not', 'mean', 'that', 'people', 'with', 'autism', 'are', 'unintelligent', 'or', 'unaware', 'once', 'given', 'appropriate', 'accommodations', 'many', 'will', 'happily', 'converse', 'for', 'hours', 'and', 'can', 'often', 'be_found', 'in', 'online_chat', 'rooms', 'discussion_boards', 'or', 'websites', 'and', 'even', 'using', 'communication', 'devices', 'at', 'autism', 'community', 'social', 'events', 'such_as', 'autreat', 'those_who', 'do', 'speak', 'often', 'use', 'language', 'in', 'unusual', 'ways', 'retaining', 'features', 'of', 'earlier_stages', 'of', 'language', 'development', 'for', 'long_periods', 'or', 'throughout', 'their_lives', 'some', 'speak', 'only', 'single', 'words', 'while_others', 'repeat', 'the', 'same', 'phrase', 'over', 'and', 'over', 'some', 'repeat', 'what', 'they', 'hear', 'a', 'condition', 'called', 'echolalia', 'sing', 'song', 'repetitions', 'in', 'particular', 'are', 'a', 'calming', 'joyous', 'activity', 'that', 'many', 'autistic_adults', 'engage', 'in', 'many', 'people', 'with', 'autism', 'have', 'a', 'strong', 'tonal', 'sense', 'and', 'can', 'often', 'understand', 'spoken_language', 'some', 'children', 'may', 'exhibit', 'only', 'slight', 'delays', 'in', 'language', 'or', 'even', 'seem', 'to', 'have', 'precocious', 'language', 'and', 'unusually_large', 'vocabularies', 'but', 'have', 'great_difficulty', 'in', 'sustaining', 'typical', 'conversations', 'the', 'give', 'and', 'take', 'of', 'non_autistic', 'conversation', 'is', 'hard', 'for', 'them', 'although', 'they', 'often', 'carry', 'on', 'a', 'monologue', 'on', 'a', 'favorite', 'subject', 'giving', 'no', 'one', 'else', 'an_opportunity', 'to', 'comment', 'when', 'given', 'the', 'chance', 'to', 'converse', 'with', 'other', 'autistics', 'they', 'comfortably', 'do_so', 'in', 'parallel', 'monologue', 'taking_turns', 'expressing', 'views', 'and', 'information', 'just', 'as', 'neurotypicals', 'people', 'without', 'autism', 'have', 'trouble_understanding', 'autistic', 'body', 'languages', 'vocal', 'tones', 'or', 'phraseology', 'people', 'with', 'autism', 'similarly', 'have', 'trouble', 'with', 'such', 'things', 'in', 'people', 'without', 'autism', 'in', 'particular', 'autistic', 'language', 'abilities', 'tend_to', 'be', 'highly', 'literal', 'people', 'without', 'autism', 'often', 'inappropriately', 'attribute', 'hidden_meaning', 'to', 'what', 'people', 'with', 'autism', 'say', 'or', 'expect', 'the', 'person', 'with', 'autism', 'to', 'sense', 'such', 'unstated', 'meaning', 'in', 'their_own', 'words', 'the', 'body', 'language', 'of', 'people', 'with', 'autism', 'can_be', 'difficult', 'for', 'other', 'people', 'to', 'understand', 'facial_expressions', 'movements', 'and', 'gestures', 'may_be', 'easily_understood', 'by', 'some', 'other', 'people', 'with', 'autism', 'but', 'do_not', 'match', 'those', 'used', 'by', 'other', 'people', 'also', 'their', 'tone', 'of', 'voice', 'has', 'a', 'much_more', 'subtle', 'inflection', 'in', 'reflecting', 'their', 'feelings', 'and', 'the', 'auditory', 'system', 'of', 'a', 'person', 'without', 'autism', 'often', 'cannot', 'sense', 'the', 'fluctuations', 'what', 'seems', 'to', 'non_autistic', 'people', 'like', 'a', 'high_pitched', 'sing', 'song', 'or', 'flat', 'robot', 'like', 'voice', 'is', 'common', 'in', 'autistic_children', 'some', 'autistic_children', 'with', 'relatively', 'good', 'language', 'skills', 'speak', 'like', 'little', 'adults', 'rather_than', 'communicating', 'at', 'their', 'current', 'age', 'level', 'which', 'is', 'one', 'of', 'the', 'things', 'that', 'can_lead', 'to', 'problems', 'since', 'non_autistic', 'people', 'are', 'often', 'unfamiliar_with', 'the', 'autistic', 'body', 'language', 'and', 'since', 'autistic', 'natural', 'language', 'may', 'not', 'tend_towards', 'speech', 'autistic_people', 'often', 'struggle', 'to', 'let', 'other', 'people', 'know_what', 'they', 'need', 'as', 'anybody', 'might', 'do', 'in', 'such', 'a', 'situation', 'they', 'may', 'scream', 'in', 'frustration', 'or', 'resort', 'to', 'grabbing', 'what', 'they_want', 'while_waiting', 'for', 'non_autistic', 'people', 'to', 'learn', 'to', 'communicate_with', 'them', 'people', 'with', 'autism', 'do_whatever', 'they', 'can', 'to', 'get', 'through', 'to', 'them', 'communication', 'difficulties', 'may', 'contribute', 'to', 'autistic_people', 'becoming', 'socially', 'anxious', 'or', 'depressed', 'repetitive', 'behaviors', 'although', 'people', 'with', 'autism', 'usually', 'appear', 'physically', 'normal', 'and', 'have', 'good', 'muscle', 'control', 'unusual', 'repetitive', 'motions', 'known_as', 'self', 'stimulation', 'or', 'stimming', 'may', 'set', 'them', 'apart', 'these', 'behaviors', 'might_be', 'extreme', 'and', 'highly', 'apparent', 'or', 'more_subtle', 'some', 'children', 'and', 'older', 'individuals', 'spend', 'a_lot', 'of', 'time', 'repeatedly', 'flapping', 'their', 'arms', 'or', 'wiggling', 'their', 'toes', 'others', 'suddenly', 'freeze', 'in', 'position', 'as', 'children', 'they', 'might', 'spend_hours', 'lining_up', 'their', 'cars', 'and', 'trains', 'in', 'a', 'certain', 'way', 'not', 'using', 'them', 'for', 'pretend', 'play', 'if_someone', 'accidentally', 'moves', 'one', 'of', 'these', 'toys', 'the', 'child', 'may_be', 'tremendously', 'upset', 'autistic_children', 'often', 'need', 'and', 'demand', 'absolute', 'consistency', 'in', 'their', 'environment', 'a', 'slight', 'change', 'in', 'any', 'routine', 'in', 'mealtimes', 'dressing', 'taking', 'a', 'bath', 'or', 'going', 'to', 'school', 'at', 'a', 'certain', 'time', 'and', 'by', 'the', 'same', 'route', 'can_be', 'extremely', 'disturbing', 'people', 'with', 'autism', 'sometimes', 'have', 'a', 'persistent', 'intense', 'preoccupation', 'for_example', 'the', 'child', 'might_be', 'obsessed_with', 'learning', 'all', 'about', 'vacuum_cleaners', 'train', 'schedules', 'or', 'lighthouses', 'often', 'they', 'show', 'great', 'interest', 'in', 'different', 'languages', 'numbers', 'symbols', 'or', 'science', 'topics', 'repetitive', 'behaviors', 'can', 'also', 'extend', 'into', 'the', 'spoken_word', 'as_well', 'perseveration', 'of', 'a', 'single', 'word', 'or', 'phrase', 'even', 'for', 'a', 'specific', 'number', 'of', 'times', 'can', 'also', 'become', 'a', 'part', 'of', 'the', 'child', 's', 'daily_routine', 'effects', 'in', 'education', 'children', 'with', 'autism', 'are', 'affected', 'with', 'these', 'symptoms', 'every_day', 'these', 'unusual', 'characteristics', 'set', 'them', 'apart_from', 'the', 'everyday', 'normal', 'student', 'because_they', 'have', 'trouble_understanding', 'people', 's', 'thoughts', 'and', 'feelings', 'they', 'have', 'trouble_understanding', 'what', 'their', 'teacher', 'may_be', 'telling_them', 'they_do', 'not', 'understand', 'that', 'facial_expressions', 'and', 'vocal', 'variations', 'hold', 'meanings', 'and', 'may', 'misinterpret', 'what', 'emotion', 'their', 'instructor', 'is', 'displaying', 'this', 'inability_to', 'fully', 'decipher', 'the', 'world', 'around', 'them', 'makes', 'education', 'stressful', 'teachers', 'need', 'to', 'be', 'aware', 'of', 'a', 'student', 's', 'disorder', 'so', 'that', 'they', 'are', 'able_to', 'help', 'the', 'student', 'get', 'the', 'best', 'out', 'of', 'the', 'lessons', 'being', 'taught', 'some', 'students', 'learn', 'better', 'with', 'visual', 'aids', 'as', 'they', 'are', 'better', 'able_to', 'understand', 'material', 'presented', 'this', 'way', 'because', 'of', 'this', 'many', 'teachers', 'create', 'visual', 'schedules', 'for', 'their', 'autistic', 'students', 'this', 'allows', 'the', 'student', 'to', 'know_what', 'is', 'going', 'on', 'throughout', 'the', 'day', 'so', 'they', 'know_what', 'to', 'prepare', 'for', 'and', 'what', 'activity', 'they', 'will_be', 'doing', 'next', 'some', 'autistic_children', 'have', 'trouble', 'going', 'from', 'one', 'activity', 'to', 'the', 'next', 'so', 'this', 'visual', 'schedule', 'can', 'help', 'to', 'reduce', 'stress', 'research', 'has', 'shown', 'that', 'working', 'in', 'pairs', 'may_be', 'beneficial', 'to', 'autistic_children', 'autistic', 'students', 'have', 'problems', 'in', 'schools', 'not', 'only', 'with', 'language', 'and', 'communication', 'but', 'with', 'socialization', 'as_well', 'they', 'feel', 'self_conscious', 'about', 'themselves', 'and', 'many', 'feel', 'that', 'they', 'will_always', 'be', 'outcasts', 'by', 'allowing_them', 'to', 'work', 'with', 'peers', 'they', 'can', 'make', 'friends', 'which', 'in', 'turn', 'can', 'help', 'them', 'cope_with', 'the', 'problems', 'that', 'arise', 'by', 'doing_so', 'they', 'can', 'become', 'more', 'integrated_into', 'the', 'mainstream', 'environment', 'of', 'the', 'classroom', 'a', 'teacher', 's', 'aide', 'can', 'also', 'be', 'useful', 'to', 'the', 'student', 'the', 'aide', 'is', 'able_to', 'give', 'more_elaborate', 'directions', 'that', 'the', 'teacher', 'may', 'not', 'have', 'time', 'to', 'explain', 'to', 'the', 'autistic_child', 'the', 'aide', 'can', 'also', 'facilitate', 'the', 'autistic_child', 'in', 'such', 'a', 'way', 'as', 'to', 'allow_them', 'to', 'stay', 'at', 'a', 'similar', 'level', 'to', 'the', 'rest', 'of', 'the', 'class', 'this', 'allows', 'a', 'partially', 'one', 'on', 'one', 'lesson', 'structure', 'so', 'that', 'the', 'child', 'is', 'still', 'able_to', 'stay', 'in', 'a', 'normal', 'classroom', 'but', 'be', 'given', 'the', 'extra', 'help', 'that', 'they', 'need', 'there_are', 'many', 'different', 'techniques', 'that', 'teachers', 'can', 'use', 'to', 'assist', 'their', 'students', 'a', 'teacher', 'needs', 'to', 'become_familiar', 'with', 'the', 'child', 's', 'disorder', 'to', 'know_what', 'will', 'work', 'best', 'with', 'that', 'particular', 'child', 'every', 'child', 'is', 'going', 'to', 'be', 'different', 'and', 'teachers', 'have', 'to', 'be_able', 'to', 'adjust', 'with', 'every', 'one', 'of', 'them', 'students', 'with', 'autism_spectrum', 'disorders', 'typically', 'have', 'high_levels', 'of', 'anxiety', 'and', 'stress', 'particularly', 'in', 'social', 'environments', 'like', 'school', 'if', 'a', 'student', 'exhibits', 'aggressive', 'or', 'explosive', 'behavior', 'it', 'is', 'important', 'for', 'educational', 'teams', 'to', 'recognize', 'the', 'impact', 'of', 'stress', 'and', 'anxiety', 'preparing', 'students', 'for', 'new', 'situations', 'by', 'writing', 'social', 'stories', 'can', 'lower', 'anxiety', 'teaching', 'social', 'and', 'emotional', 'concepts', 'using', 'systematic', 'teaching', 'approaches', 'such_as', 'the', 'incredible', 'five', 'point', 'scale', 'or', 'other', 'cognitive_behavioral', 'strategies', 'can', 'increase', 'a', 'student', 's', 'ability', 'to', 'control', 'excessive', 'behavioral', 'reactions', 'dsm', 'definition', 'autism', 'is', 'defined', 'in', 'section', 'two', 'nine', 'nine', 'zero', 'zero', 'of', 'the', 'diagnostic', 'and', 'statistical_manual', 'of', 'mental_disorders', 'dsm_iv', 'as', 'a', 'total', 'of', 'six', 'or', 'more', 'items', 'from', 'one', 'two', 'and', 'three', 'with', 'at_least', 'two', 'from', 'one', 'and', 'one', 'each', 'from', 'two', 'and', 'three', 'qualitative', 'impairment', 'in', 'social_interaction', 'as', 'manifested', 'by', 'at_least', 'two', 'of', 'the', 'following', 'marked_impairment', 'in', 'the', 'use', 'of', 'multiple', 'nonverbal', 'behaviors', 'such_as', 'eye', 'to', 'eye', 'gaze', 'facial_expression', 'body', 'postures', 'and', 'gestures', 'to', 'regulate', 'social_interaction', 'failure', 'to', 'develop', 'peer', 'relationships', 'appropriate', 'to', 'developmental', 'level', 'a', 'lack', 'of', 'spontaneous', 'seeking', 'to', 'share', 'enjoyment', 'interests', 'or', 'achievements', 'with', 'other', 'people', 'e_g', 'by', 'a', 'lack', 'of', 'showing', 'bringing', 'or', 'pointing_out', 'objects', 'of', 'interest', 'lack', 'of', 'social', 'or', 'emotional']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim import corpora, models\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "dataset = [wd for wd in dataset]\n",
    "\n",
    "dct = corpora.Dictionary(dataset)\n",
    "corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "bigram = models.phrases.Phrases(dataset, min_count=3, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.corpus_word_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "first: gibbs sampling. \n",
    "- this underlies the logic behind splitting up text in topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "import random\n",
    "\n",
    "def roll_a_die() -> int:\n",
    "    return random.choice([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "def direct_sample() -> Tuple[int, int]:\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x: int) -> int:\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y: int) -> int:\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters: int = 100) -> Tuple[int, int]:\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]:\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gibbs_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.compare_distributions.<locals>.<lambda>()>,\n",
       "            {(5, 8): [18, 39],\n",
       "             (4, 8): [28, 23],\n",
       "             (6, 9): [31, 29],\n",
       "             (3, 8): [29, 23],\n",
       "             (3, 9): [18, 30],\n",
       "             (4, 5): [39, 28],\n",
       "             (2, 8): [30, 33],\n",
       "             (1, 7): [23, 27],\n",
       "             (2, 4): [35, 34],\n",
       "             (1, 6): [24, 27],\n",
       "             (1, 4): [23, 37],\n",
       "             (6, 11): [28, 30],\n",
       "             (5, 9): [18, 37],\n",
       "             (5, 7): [26, 28],\n",
       "             (4, 9): [22, 27],\n",
       "             (2, 6): [26, 19],\n",
       "             (4, 10): [31, 30],\n",
       "             (2, 5): [18, 31],\n",
       "             (2, 3): [35, 28],\n",
       "             (6, 10): [33, 26],\n",
       "             (1, 5): [32, 21],\n",
       "             (4, 6): [38, 23],\n",
       "             (3, 5): [31, 26],\n",
       "             (6, 7): [21, 34],\n",
       "             (5, 11): [25, 22],\n",
       "             (5, 6): [31, 35],\n",
       "             (6, 8): [26, 26],\n",
       "             (3, 6): [39, 27],\n",
       "             (4, 7): [24, 25],\n",
       "             (6, 12): [32, 29],\n",
       "             (5, 10): [21, 18],\n",
       "             (2, 7): [28, 22],\n",
       "             (1, 3): [29, 27],\n",
       "             (1, 2): [34, 26],\n",
       "             (3, 4): [35, 25],\n",
       "             (3, 7): [19, 28]})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(weights: List[float]) -> int:\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()      # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                       # return the smallest i such that\n",
    "        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Draw 1000 times and count\n",
    "draws = Counter(sample_from([0.1, 0.1, 0.8]) for _ in range(1000))\n",
    "assert 10 < draws[0] < 190   # should be ~10%, this is a really loose test\n",
    "assert 10 < draws[1] < 190   # should be ~10%, this is a really loose test\n",
    "assert 650 < draws[2] < 950  # should be ~80%, this is a really loose test\n",
    "assert draws[0] + draws[1] + draws[2] == 1000\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of topics and initialise counters\n",
    "K = 4\n",
    "\n",
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each document\n",
    "document_lengths = [len(document) for document in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so we have 36 distinct words and 15 documents'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"so we have {len(distinct_words)} distinct words and {len(documents)} documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d: int, word: str, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Given a document and a word in that document,\n",
    "    return the weight for the kth topic\n",
    "    \"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d: int, word: str) -> int:\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])\n",
    "\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for iter in tqdm.trange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Python and statistics 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "databases 3\n",
      "Big Data and programming languages 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"Python and statistics\",\n",
    "               \"databases\",\n",
    "               \"machine learning\"]\n",
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample documents\n",
    "documents = [\n",
    "    \"The sky is blue\",\n",
    "    \"Blue and bright day\",\n",
    "    \"Beautiful blue birds in the sky\",\n",
    "    \"The dog is in the garden\",\n",
    "    \"Cats and dogs are great pets\",\n",
    "    \"Flowers in the garden are beautiful\",\n",
    "    \"I love gardening\"\n",
    "]\n",
    "documents = [document.split() for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'sky', 'is', 'blue'],\n",
       " ['Blue', 'and', 'bright', 'day'],\n",
       " ['Beautiful', 'blue', 'birds', 'in', 'the', 'sky'],\n",
       " ['The', 'dog', 'is', 'in', 'the', 'garden'],\n",
       " ['Cats', 'and', 'dogs', 'are', 'great', 'pets'],\n",
       " ['Flowers', 'in', 'the', 'garden', 'are', 'beautiful'],\n",
       " ['I', 'love', 'gardening']]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so we have 24 distinct words and 7 documents'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each document\n",
    "document_lengths = [len(document) for document in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "f\"so we have {len(distinct_words)} distinct words and {len(documents)} documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:00<00:00, 4641.76it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for iter in tqdm.trange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 and 2\n",
      "0 dogs 1\n",
      "0 pets 1\n",
      "0 I 1\n",
      "0 gardening 1\n",
      "0 day 1\n",
      "1 sky 2\n",
      "1 the 2\n",
      "1 blue 1\n",
      "1 Beautiful 1\n",
      "1 garden 1\n",
      "1 birds 1\n",
      "1 Flowers 1\n",
      "1 Blue 1\n",
      "2 in 3\n",
      "2 are 2\n",
      "2 is 2\n",
      "2 The 2\n",
      "2 the 1\n",
      "2 garden 1\n",
      "2 Cats 1\n",
      "2 bright 1\n",
      "2 dog 1\n",
      "2 beautiful 1\n",
      "2 love 1\n",
      "2 great 1\n",
      "2 blue 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sky', 'is', 'blue']\n",
      "2 14\n",
      "1 5\n",
      "\n",
      "['Blue', 'and', 'bright', 'day']\n",
      "2 12\n",
      "1 7\n",
      "0 4\n",
      "\n",
      "['Beautiful', 'blue', 'birds', 'in', 'the', 'sky']\n",
      "2 22\n",
      "1 15\n",
      "\n",
      "['The', 'dog', 'is', 'in', 'the', 'garden']\n",
      "2 25\n",
      "0 4\n",
      "1 1\n",
      "\n",
      "['Cats', 'and', 'dogs', 'are', 'great', 'pets']\n",
      "2 23\n",
      "0 11\n",
      "\n",
      "['Flowers', 'in', 'the', 'garden', 'are', 'beautiful']\n",
      "2 25\n",
      "1 15\n",
      "0 1\n",
      "\n",
      "['I', 'love', 'gardening']\n",
      "2 11\n",
      "0 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topic_names = [\"Big Data and programming languages\",\n",
    "#                \"Python and statistics\",\n",
    "#                \"databases\",\n",
    "#                \"machine learning\"]\n",
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic, count)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gensim, the package for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.084*\"and\" + 0.078*\"are\" + 0.076*\"cats\" + 0.076*\"great\"'), (1, '0.152*\"the\" + 0.097*\"in\" + 0.096*\"blue\" + 0.069*\"beautiful\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Let's start with some sample documents\n",
    "documents = [\n",
    "    \"The sky is blue\",\n",
    "    \"Blue and bright day\",\n",
    "    \"Beautiful blue birds in the sky\",\n",
    "    \"The dog is in the garden\",\n",
    "    \"Cats and dogs are great pets\",\n",
    "    \"Flowers in the garden are beautiful\",\n",
    "    \"I love gardening\"\n",
    "]\n",
    "\n",
    "# Preprocessing: Tokenize the documents, remove common words as well as words that only appear once\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Convert the dictionary into a bag-of-words\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # this is a bag-of-words corpus\n",
    "\n",
    "# Train the LDA model\n",
    "lda = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)\n",
    "\n",
    "# Print the topics\n",
    "print(lda.print_topics(num_topics=2, num_words=4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is accompanied by a number, which is the weight of the word in that topic. \n",
    "\n",
    "The weight indicates how important that word is in defining the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<21 unique tokens: ['blue', 'is', 'sky', 'the', 'and']...>\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 0, 'is': 1, 'sky': 2, 'the': 3, 'and': 4, 'bright': 5, 'day': 6, 'beautiful': 7, 'birds': 8, 'in': 9, 'dog': 10, 'garden': 11, 'are': 12, 'cats': 13, 'dogs': 14, 'great': 15, 'pets': 16, 'flowers': 17, 'gardening': 18, 'i': 19, 'love': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on the content from the pitchfork review above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'above': 0,\n",
       " 'accents': 1,\n",
       " 'accentuated': 2,\n",
       " 'accommodate': 3,\n",
       " 'acoustic': 4,\n",
       " 'aged': 5,\n",
       " 'airier': 6,\n",
       " 'album': 7,\n",
       " 'also': 8,\n",
       " 'alternately': 9,\n",
       " 'amassed': 10,\n",
       " 'an': 11,\n",
       " 'and': 12,\n",
       " 'angelic': 13,\n",
       " 'another': 14,\n",
       " 'antagonistic': 15,\n",
       " 'any': 16,\n",
       " 'apart': 17,\n",
       " 'apocalyptic': 18,\n",
       " 'appeal': 19,\n",
       " 'arca': 20,\n",
       " 'architecture': 21,\n",
       " 'are': 22,\n",
       " 'as': 23,\n",
       " 'at': 24,\n",
       " 'atmosphere': 25,\n",
       " 'beat': 26,\n",
       " 'beatific': 27,\n",
       " 'beats': 28,\n",
       " 'bed': 29,\n",
       " 'been': 30,\n",
       " 'before': 31,\n",
       " 'best': 32,\n",
       " 'between': 33,\n",
       " 'bjork': 34,\n",
       " 'blaring': 35,\n",
       " 'blistering': 36,\n",
       " 'body': 37,\n",
       " 'both': 38,\n",
       " 'box': 39,\n",
       " 'boxy': 40,\n",
       " 'breaking': 41,\n",
       " 'breakneck': 42,\n",
       " 'bridge': 43,\n",
       " 'brimming': 44,\n",
       " 'brings': 45,\n",
       " 'british': 46,\n",
       " 'bully': 47,\n",
       " 'but': 48,\n",
       " 'by': 49,\n",
       " 'career': 50,\n",
       " 'changes': 51,\n",
       " 'chanted': 52,\n",
       " 'chaos': 53,\n",
       " 'chris': 54,\n",
       " 'clark': 55,\n",
       " 'clutch': 56,\n",
       " 'coach': 57,\n",
       " 'colonized': 58,\n",
       " 'constants': 59,\n",
       " 'contrast': 60,\n",
       " 'corroded': 61,\n",
       " 'counterpoint': 62,\n",
       " 'crank': 63,\n",
       " 'crucial': 64,\n",
       " 'dazzles': 65,\n",
       " 'death': 66,\n",
       " 'delicate': 67,\n",
       " 'delight': 68,\n",
       " 'density': 69,\n",
       " 'developed': 70,\n",
       " 'diamond': 71,\n",
       " 'dimensional': 72,\n",
       " 'distant': 73,\n",
       " 'does': 74,\n",
       " 'dog': 75,\n",
       " 'drama': 76,\n",
       " 'drift': 77,\n",
       " 'dynamism': 78,\n",
       " 'ear': 79,\n",
       " 'edge': 80,\n",
       " 'electronic': 81,\n",
       " 'emotive': 82,\n",
       " 'emptiness': 83,\n",
       " 'even': 84,\n",
       " 'ever': 85,\n",
       " 'evolution': 86,\n",
       " 'exception': 87,\n",
       " 'executive': 88,\n",
       " 'expectations': 89,\n",
       " 'experiments': 90,\n",
       " 'explosive': 91,\n",
       " 'extremes': 92,\n",
       " 'falsetto': 93,\n",
       " 'falsettos': 94,\n",
       " 'few': 95,\n",
       " 'film': 96,\n",
       " 'first': 97,\n",
       " 'flare': 98,\n",
       " 'fleet': 99,\n",
       " 'fluid': 100,\n",
       " 'footed': 101,\n",
       " 'footing': 102,\n",
       " 'for': 103,\n",
       " 'foregrounds': 104,\n",
       " 'forgoes': 105,\n",
       " 'fragile': 106,\n",
       " 'from': 107,\n",
       " 'furiously': 108,\n",
       " 'garbled': 109,\n",
       " 'gear': 110,\n",
       " 'glides': 111,\n",
       " 'graceful': 112,\n",
       " 'gratifying': 113,\n",
       " 'growing': 114,\n",
       " 'growls': 115,\n",
       " 'had': 116,\n",
       " 'handsome': 117,\n",
       " 'harmonize': 118,\n",
       " 'has': 119,\n",
       " 'have': 120,\n",
       " 'he': 121,\n",
       " 'heightened': 122,\n",
       " 'here': 123,\n",
       " 'high': 124,\n",
       " 'himself': 125,\n",
       " 'hip': 126,\n",
       " 'his': 127,\n",
       " 'historically': 128,\n",
       " 'hop': 129,\n",
       " 'hushed': 130,\n",
       " 'idm': 131,\n",
       " 'immediately': 132,\n",
       " 'in': 133,\n",
       " 'instrument': 134,\n",
       " 'instrumental': 135,\n",
       " 'instruments': 136,\n",
       " 'interwoven': 137,\n",
       " 'into': 138,\n",
       " 'intricate': 139,\n",
       " 'is': 140,\n",
       " 'it': 141,\n",
       " 'its': 142,\n",
       " 'just': 143,\n",
       " 'kilter': 144,\n",
       " 'lacks': 145,\n",
       " 'largely': 146,\n",
       " 'latest': 147,\n",
       " 'learned': 148,\n",
       " 'left': 149,\n",
       " 'levitates': 150,\n",
       " 'lightspeed': 151,\n",
       " 'like': 152,\n",
       " 'limited': 153,\n",
       " 'line': 154,\n",
       " 'little': 155,\n",
       " 'loveliest': 156,\n",
       " 'lower': 157,\n",
       " 'made': 158,\n",
       " 'makes': 159,\n",
       " 'margins': 160,\n",
       " 'medicine': 161,\n",
       " 'men': 162,\n",
       " 'menacing': 163,\n",
       " 'mentor': 164,\n",
       " 'mentorship': 165,\n",
       " 'might': 166,\n",
       " 'minimalism': 167,\n",
       " 'mix': 168,\n",
       " 'mixed': 169,\n",
       " 'momentum': 170,\n",
       " 'more': 171,\n",
       " 'morph': 172,\n",
       " 'most': 173,\n",
       " 'music': 174,\n",
       " 'musician': 175,\n",
       " 'never': 176,\n",
       " 'no': 177,\n",
       " 'nudge': 178,\n",
       " 'of': 179,\n",
       " 'off': 180,\n",
       " 'offering': 181,\n",
       " 'on': 182,\n",
       " 'one': 183,\n",
       " 'or': 184,\n",
       " 'ornate': 185,\n",
       " 'over': 186,\n",
       " 'palette': 187,\n",
       " 'particularly': 188,\n",
       " 'peak': 189,\n",
       " 'pearlers': 190,\n",
       " 'pent': 191,\n",
       " 'phone': 192,\n",
       " 'piercing': 193,\n",
       " 'place': 194,\n",
       " 'plaintive': 195,\n",
       " 'planet': 196,\n",
       " 'plucks': 197,\n",
       " 'poorly': 198,\n",
       " 'pop': 199,\n",
       " 'processed': 200,\n",
       " 'produced': 201,\n",
       " 'producer': 202,\n",
       " 'production': 203,\n",
       " 'propulsive': 204,\n",
       " 'pulse': 205,\n",
       " 'quality': 206,\n",
       " 'radiohead': 207,\n",
       " 'rain': 208,\n",
       " 'range': 209,\n",
       " 'raps': 210,\n",
       " 'rather': 211,\n",
       " 'reassuring': 212,\n",
       " 'recent': 213,\n",
       " 'record': 214,\n",
       " 'red': 215,\n",
       " 'reliable': 216,\n",
       " 'reminiscent': 217,\n",
       " 'replicating': 218,\n",
       " 'results': 219,\n",
       " 'rising': 220,\n",
       " 'sheer': 221,\n",
       " 'shifting': 222,\n",
       " 'sighs': 223,\n",
       " 'similarity': 224,\n",
       " 'singing': 225,\n",
       " 'slapping': 226,\n",
       " 'soaring': 227,\n",
       " 'some': 228,\n",
       " 'sometimes': 229,\n",
       " 'song': 230,\n",
       " 'sounded': 231,\n",
       " 'soundtracks': 232,\n",
       " 'space': 233,\n",
       " 'storm': 234,\n",
       " 'subject': 235,\n",
       " 'subtle': 236,\n",
       " 'suddenly': 237,\n",
       " 'suggestion': 238,\n",
       " 'suicide': 239,\n",
       " 'surfs': 240,\n",
       " 'sus': 241,\n",
       " 'swerved': 242,\n",
       " 'synth': 243,\n",
       " 'synths': 244,\n",
       " 'taken': 245,\n",
       " 'tearing': 246,\n",
       " 'techno': 247,\n",
       " 'than': 248,\n",
       " 'that': 249,\n",
       " 'the': 250,\n",
       " 'their': 251,\n",
       " 'they': 252,\n",
       " 'think': 253,\n",
       " 'thom': 254,\n",
       " 'three': 255,\n",
       " 'tics': 256,\n",
       " 'to': 257,\n",
       " 'together': 258,\n",
       " 'top': 259,\n",
       " 'totems': 260,\n",
       " 'town': 261,\n",
       " 'track': 262,\n",
       " 'tracks': 263,\n",
       " 'traffic': 264,\n",
       " 'tricky': 265,\n",
       " 'turns': 266,\n",
       " 'tutelage': 267,\n",
       " 'tv': 268,\n",
       " 'twinkling': 269,\n",
       " 'two': 270,\n",
       " 'ultra': 271,\n",
       " 'under': 272,\n",
       " 'undergone': 273,\n",
       " 'up': 274,\n",
       " 'using': 275,\n",
       " 'veers': 276,\n",
       " 'violence': 277,\n",
       " 'violent': 278,\n",
       " 'vocal': 279,\n",
       " 'voice': 280,\n",
       " 'warm': 281,\n",
       " 'warning': 282,\n",
       " 'way': 283,\n",
       " 'when': 284,\n",
       " 'where': 285,\n",
       " 'while': 286,\n",
       " 'wildly': 287,\n",
       " 'with': 288,\n",
       " 'work': 289,\n",
       " 'working': 290,\n",
       " 'wrong': 291,\n",
       " 'years': 292,\n",
       " 'yielded': 293,\n",
       " 'yorke': 294,\n",
       " 'yorkean': 295,\n",
       " 'you': 296,\n",
       " 'your': 297,\n",
       " 'about': 298,\n",
       " 'achieves': 299,\n",
       " 'aching': 300,\n",
       " 'acoustics': 301,\n",
       " 'adolescent': 302,\n",
       " 'affiliate': 303,\n",
       " 'against': 304,\n",
       " 'air': 305,\n",
       " 'all': 306,\n",
       " 'almost': 307,\n",
       " 'alyosha': 308,\n",
       " 'amazon': 309,\n",
       " 'angles': 310,\n",
       " 'appear': 311,\n",
       " 'approach': 312,\n",
       " 'approaches': 313,\n",
       " 'atmospheric': 314,\n",
       " 'be': 315,\n",
       " 'beautiful': 316,\n",
       " 'beauty': 317,\n",
       " 'because': 318,\n",
       " 'behavior': 319,\n",
       " 'believe': 320,\n",
       " 'bleakly': 321,\n",
       " 'bleeds': 322,\n",
       " 'bones': 323,\n",
       " 'bright': 324,\n",
       " 'brilliantly': 325,\n",
       " 'brothers': 326,\n",
       " 'bruise': 327,\n",
       " 'brutality': 328,\n",
       " 'buy': 329,\n",
       " 'calm': 330,\n",
       " 'can': 331,\n",
       " 'cappella': 332,\n",
       " 'carry': 333,\n",
       " 'chorus': 334,\n",
       " 'clarity': 335,\n",
       " 'clear': 336,\n",
       " 'cleaves': 337,\n",
       " 'closing': 338,\n",
       " 'commission': 339,\n",
       " 'comparison': 340,\n",
       " 'content': 341,\n",
       " 'conventional': 342,\n",
       " 'could': 343,\n",
       " 'course': 344,\n",
       " 'croon': 345,\n",
       " 'croons': 346,\n",
       " 'crude': 347,\n",
       " 'cruelty': 348,\n",
       " 'cynical': 349,\n",
       " 'derision': 350,\n",
       " 'derives': 351,\n",
       " 'despair': 352,\n",
       " 'detailed': 353,\n",
       " 'detuned': 354,\n",
       " 'discovered': 355,\n",
       " 'dismissive': 356,\n",
       " 'distrust': 357,\n",
       " 'drive': 358,\n",
       " 'earn': 359,\n",
       " 'editors': 360,\n",
       " 'emotional': 361,\n",
       " 'emotionally': 362,\n",
       " 'entirely': 363,\n",
       " 'evocative': 364,\n",
       " 'expressing': 365,\n",
       " 'eyed': 366,\n",
       " 'fact': 367,\n",
       " 'featured': 368,\n",
       " 'felt': 369,\n",
       " 'finely': 370,\n",
       " 'fire': 371,\n",
       " 'floors': 372,\n",
       " 'forest': 373,\n",
       " 'foxes': 374,\n",
       " 'fuel': 375,\n",
       " 'full': 376,\n",
       " 'guide': 377,\n",
       " 'harnesses': 378,\n",
       " 'highly': 379,\n",
       " 'however': 380,\n",
       " 'human': 381,\n",
       " 'hurt': 382,\n",
       " 'independently': 383,\n",
       " 'itself': 384,\n",
       " 'karamazov': 385,\n",
       " 'ladder': 386,\n",
       " 'leap': 387,\n",
       " 'leaps': 388,\n",
       " 'leveraging': 389,\n",
       " 'links': 390,\n",
       " 'listeners': 391,\n",
       " 'living': 392,\n",
       " 'lyricist': 393,\n",
       " 'making': 394,\n",
       " 'mature': 395,\n",
       " 'may': 396,\n",
       " 'maze': 397,\n",
       " 'merciless': 398,\n",
       " 'met': 399,\n",
       " 'metaphor': 400,\n",
       " 'mist': 401,\n",
       " 'moments': 402,\n",
       " 'mournful': 403,\n",
       " 'much': 404,\n",
       " 'multi': 405,\n",
       " 'nadir': 406,\n",
       " 'naked': 407,\n",
       " 'navigating': 408,\n",
       " 'new': 409,\n",
       " 'noise': 410,\n",
       " 'not': 411,\n",
       " 'note': 412,\n",
       " 'octave': 413,\n",
       " 'odd': 414,\n",
       " 'only': 415,\n",
       " 'opens': 416,\n",
       " 'otherworldly': 417,\n",
       " 'our': 418,\n",
       " 'out': 419,\n",
       " 'own': 420,\n",
       " 'passive': 421,\n",
       " 'piano': 422,\n",
       " 'pit': 423,\n",
       " 'pitched': 424,\n",
       " 'pitchfork': 425,\n",
       " 'pleas': 426,\n",
       " 'please': 427,\n",
       " 'poles': 428,\n",
       " 'power': 429,\n",
       " 'practicality': 430,\n",
       " 'products': 431,\n",
       " 'protagonist': 432,\n",
       " 'raw': 433,\n",
       " 'recognizing': 434,\n",
       " 'reference': 435,\n",
       " 'refrain': 436,\n",
       " 'refrains': 437,\n",
       " 'remains': 438,\n",
       " 'repeating': 439,\n",
       " 'resolutely': 440,\n",
       " 'retail': 441,\n",
       " 'ribbons': 442,\n",
       " 'rises': 443,\n",
       " 'risk': 444,\n",
       " 'rival': 445,\n",
       " 'rough': 446,\n",
       " 'scorching': 447,\n",
       " 'selected': 448,\n",
       " 'sensitivity': 449,\n",
       " 'separate': 450,\n",
       " 'shapes': 451,\n",
       " 'shortcoming': 452,\n",
       " 'sings': 453,\n",
       " 'situated': 454,\n",
       " 'so': 455,\n",
       " 'something': 456,\n",
       " 'songwriting': 457,\n",
       " 'spheres': 458,\n",
       " 'strain': 459,\n",
       " 'striking': 460,\n",
       " 'stuck': 461,\n",
       " 'such': 462,\n",
       " 'swelling': 463,\n",
       " 'terrain': 464,\n",
       " 'text': 465,\n",
       " 'this': 466,\n",
       " 'those': 467,\n",
       " 'though': 468,\n",
       " 'through': 469,\n",
       " 'time': 470,\n",
       " 'tinny': 471,\n",
       " 'title': 472,\n",
       " 'toggled': 473,\n",
       " 'tone': 474,\n",
       " 'tracked': 475,\n",
       " 'trade': 476,\n",
       " 'transforming': 477,\n",
       " 'truly': 478,\n",
       " 'turn': 479,\n",
       " 'unanswerable': 480,\n",
       " 'understanding': 481,\n",
       " 'unknowability': 482,\n",
       " 'until': 483,\n",
       " 'vague': 484,\n",
       " 'virtuous': 485,\n",
       " 'want': 486,\n",
       " 'we': 487,\n",
       " 'wearily': 488,\n",
       " 'weather': 489,\n",
       " 'which': 490,\n",
       " 'whose': 491,\n",
       " 'wild': 492,\n",
       " 'words': 493,\n",
       " 'zen': 494,\n",
       " 'zigzagging': 495}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# using the content from the pitchfork review above\n",
    "preprocessed = [simple_preprocess(line, deacc=True) for line in content] \n",
    "\n",
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(preprocessed)\n",
    "\n",
    "# Token to Id map\n",
    "simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 4),\n",
       "  (12, 10),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 4),\n",
       "  (24, 3),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 2),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 2),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 3),\n",
       "  (49, 3),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 9),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 2),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 3),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 1),\n",
       "  (103, 3),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 4),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 1),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 10),\n",
       "  (120, 3),\n",
       "  (121, 9),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 12),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (132, 1),\n",
       "  (133, 7),\n",
       "  (134, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 5),\n",
       "  (139, 1),\n",
       "  (140, 5),\n",
       "  (141, 2),\n",
       "  (142, 2),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 2),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 1),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1),\n",
       "  (153, 2),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 2),\n",
       "  (167, 1),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 5),\n",
       "  (172, 1),\n",
       "  (173, 2),\n",
       "  (174, 5),\n",
       "  (175, 2),\n",
       "  (176, 1),\n",
       "  (177, 1),\n",
       "  (178, 1),\n",
       "  (179, 18),\n",
       "  (180, 2),\n",
       "  (181, 1),\n",
       "  (182, 7),\n",
       "  (183, 3),\n",
       "  (184, 1),\n",
       "  (185, 1),\n",
       "  (186, 2),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 1),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 3),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 1),\n",
       "  (213, 1),\n",
       "  (214, 4),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 1),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 1),\n",
       "  (221, 1),\n",
       "  (222, 1),\n",
       "  (223, 1),\n",
       "  (224, 1),\n",
       "  (225, 1),\n",
       "  (226, 1),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 1),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 1),\n",
       "  (239, 1),\n",
       "  (240, 1),\n",
       "  (241, 3),\n",
       "  (242, 1),\n",
       "  (243, 2),\n",
       "  (244, 2),\n",
       "  (245, 1),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 4),\n",
       "  (249, 7),\n",
       "  (250, 23),\n",
       "  (251, 2),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 2),\n",
       "  (255, 1),\n",
       "  (256, 1),\n",
       "  (257, 11),\n",
       "  (258, 1),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 2),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 1),\n",
       "  (266, 1),\n",
       "  (267, 1),\n",
       "  (268, 1),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 2),\n",
       "  (272, 1),\n",
       "  (273, 1),\n",
       "  (274, 2),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (277, 1),\n",
       "  (278, 1),\n",
       "  (279, 2),\n",
       "  (280, 6),\n",
       "  (281, 1),\n",
       "  (282, 1),\n",
       "  (283, 1),\n",
       "  (284, 1),\n",
       "  (285, 2),\n",
       "  (286, 3),\n",
       "  (287, 1),\n",
       "  (288, 9),\n",
       "  (289, 1),\n",
       "  (290, 1),\n",
       "  (291, 1),\n",
       "  (292, 1),\n",
       "  (293, 1),\n",
       "  (294, 4),\n",
       "  (295, 2),\n",
       "  (296, 1),\n",
       "  (297, 1)],\n",
       " [(8, 2),\n",
       "  (11, 3),\n",
       "  (12, 6),\n",
       "  (22, 2),\n",
       "  (23, 7),\n",
       "  (24, 3),\n",
       "  (31, 2),\n",
       "  (32, 1),\n",
       "  (33, 4),\n",
       "  (36, 1),\n",
       "  (48, 5),\n",
       "  (49, 2),\n",
       "  (50, 1),\n",
       "  (55, 6),\n",
       "  (75, 3),\n",
       "  (92, 1),\n",
       "  (97, 1),\n",
       "  (99, 1),\n",
       "  (103, 5),\n",
       "  (107, 4),\n",
       "  (119, 2),\n",
       "  (121, 6),\n",
       "  (127, 9),\n",
       "  (133, 8),\n",
       "  (135, 1),\n",
       "  (138, 3),\n",
       "  (140, 6),\n",
       "  (141, 3),\n",
       "  (144, 1),\n",
       "  (152, 5),\n",
       "  (159, 1),\n",
       "  (166, 2),\n",
       "  (173, 2),\n",
       "  (174, 1),\n",
       "  (179, 13),\n",
       "  (180, 1),\n",
       "  (182, 6),\n",
       "  (183, 3),\n",
       "  (186, 2),\n",
       "  (202, 1),\n",
       "  (203, 2),\n",
       "  (214, 1),\n",
       "  (225, 1),\n",
       "  (230, 2),\n",
       "  (241, 3),\n",
       "  (243, 1),\n",
       "  (247, 1),\n",
       "  (249, 6),\n",
       "  (250, 20),\n",
       "  (252, 2),\n",
       "  (257, 6),\n",
       "  (262, 1),\n",
       "  (270, 2),\n",
       "  (280, 3),\n",
       "  (283, 1),\n",
       "  (284, 1),\n",
       "  (288, 1),\n",
       "  (289, 1),\n",
       "  (292, 1),\n",
       "  (296, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 1),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 2),\n",
       "  (307, 1),\n",
       "  (308, 2),\n",
       "  (309, 1),\n",
       "  (310, 1),\n",
       "  (311, 1),\n",
       "  (312, 1),\n",
       "  (313, 1),\n",
       "  (314, 1),\n",
       "  (315, 3),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 1),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 1),\n",
       "  (325, 1),\n",
       "  (326, 1),\n",
       "  (327, 1),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 1),\n",
       "  (337, 1),\n",
       "  (338, 1),\n",
       "  (339, 1),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 1),\n",
       "  (347, 1),\n",
       "  (348, 1),\n",
       "  (349, 1),\n",
       "  (350, 1),\n",
       "  (351, 1),\n",
       "  (352, 1),\n",
       "  (353, 1),\n",
       "  (354, 1),\n",
       "  (355, 1),\n",
       "  (356, 2),\n",
       "  (357, 1),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (362, 1),\n",
       "  (363, 1),\n",
       "  (364, 1),\n",
       "  (365, 1),\n",
       "  (366, 1),\n",
       "  (367, 1),\n",
       "  (368, 1),\n",
       "  (369, 1),\n",
       "  (370, 1),\n",
       "  (371, 1),\n",
       "  (372, 1),\n",
       "  (373, 1),\n",
       "  (374, 1),\n",
       "  (375, 1),\n",
       "  (376, 1),\n",
       "  (377, 1),\n",
       "  (378, 1),\n",
       "  (379, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (382, 1),\n",
       "  (383, 1),\n",
       "  (384, 2),\n",
       "  (385, 1),\n",
       "  (386, 2),\n",
       "  (387, 1),\n",
       "  (388, 1),\n",
       "  (389, 1),\n",
       "  (390, 1),\n",
       "  (391, 1),\n",
       "  (392, 1),\n",
       "  (393, 1),\n",
       "  (394, 1),\n",
       "  (395, 1),\n",
       "  (396, 1),\n",
       "  (397, 1),\n",
       "  (398, 1),\n",
       "  (399, 1),\n",
       "  (400, 1),\n",
       "  (401, 1),\n",
       "  (402, 1),\n",
       "  (403, 2),\n",
       "  (404, 1),\n",
       "  (405, 1),\n",
       "  (406, 1),\n",
       "  (407, 1),\n",
       "  (408, 1),\n",
       "  (409, 1),\n",
       "  (410, 1),\n",
       "  (411, 1),\n",
       "  (412, 1),\n",
       "  (413, 1),\n",
       "  (414, 1),\n",
       "  (415, 1),\n",
       "  (416, 1),\n",
       "  (417, 1),\n",
       "  (418, 2),\n",
       "  (419, 2),\n",
       "  (420, 1),\n",
       "  (421, 1),\n",
       "  (422, 1),\n",
       "  (423, 1),\n",
       "  (424, 1),\n",
       "  (425, 1),\n",
       "  (426, 1),\n",
       "  (427, 1),\n",
       "  (428, 1),\n",
       "  (429, 1),\n",
       "  (430, 1),\n",
       "  (431, 1),\n",
       "  (432, 1),\n",
       "  (433, 1),\n",
       "  (434, 1),\n",
       "  (435, 1),\n",
       "  (436, 1),\n",
       "  (437, 1),\n",
       "  (438, 1),\n",
       "  (439, 1),\n",
       "  (440, 1),\n",
       "  (441, 1),\n",
       "  (442, 1),\n",
       "  (443, 1),\n",
       "  (444, 1),\n",
       "  (445, 1),\n",
       "  (446, 1),\n",
       "  (447, 1),\n",
       "  (448, 1),\n",
       "  (449, 1),\n",
       "  (450, 1),\n",
       "  (451, 1),\n",
       "  (452, 1),\n",
       "  (453, 1),\n",
       "  (454, 1),\n",
       "  (455, 1),\n",
       "  (456, 1),\n",
       "  (457, 2),\n",
       "  (458, 1),\n",
       "  (459, 1),\n",
       "  (460, 1),\n",
       "  (461, 1),\n",
       "  (462, 1),\n",
       "  (463, 1),\n",
       "  (464, 1),\n",
       "  (465, 1),\n",
       "  (466, 1),\n",
       "  (467, 1),\n",
       "  (468, 1),\n",
       "  (469, 2),\n",
       "  (470, 1),\n",
       "  (471, 1),\n",
       "  (472, 2),\n",
       "  (473, 1),\n",
       "  (474, 1),\n",
       "  (475, 1),\n",
       "  (476, 1),\n",
       "  (477, 1),\n",
       "  (478, 1),\n",
       "  (479, 1),\n",
       "  (480, 1),\n",
       "  (481, 1),\n",
       "  (482, 1),\n",
       "  (483, 1),\n",
       "  (484, 1),\n",
       "  (485, 1),\n",
       "  (486, 1),\n",
       "  (487, 1),\n",
       "  (488, 1),\n",
       "  (489, 1),\n",
       "  (490, 1),\n",
       "  (491, 1),\n",
       "  (492, 2),\n",
       "  (493, 1),\n",
       "  (494, 1),\n",
       "  (495, 1)]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus = [dictionary.doc2bow(doc) for doc in preprocessed]\n",
    "mycorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('above', 1),\n",
       "  ('accents', 1),\n",
       "  ('accentuated', 1),\n",
       "  ('accommodate', 1),\n",
       "  ('acoustic', 1),\n",
       "  ('aged', 1),\n",
       "  ('airier', 1),\n",
       "  ('album', 1),\n",
       "  ('also', 1),\n",
       "  ('alternately', 1),\n",
       "  ('amassed', 1),\n",
       "  ('an', 4),\n",
       "  ('and', 10),\n",
       "  ('angelic', 1),\n",
       "  ('another', 1),\n",
       "  ('antagonistic', 1),\n",
       "  ('any', 1),\n",
       "  ('apart', 1),\n",
       "  ('apocalyptic', 1),\n",
       "  ('appeal', 1),\n",
       "  ('arca', 1),\n",
       "  ('architecture', 1),\n",
       "  ('are', 1),\n",
       "  ('as', 4),\n",
       "  ('at', 3),\n",
       "  ('atmosphere', 1),\n",
       "  ('beat', 1),\n",
       "  ('beatific', 1),\n",
       "  ('beats', 2),\n",
       "  ('bed', 1),\n",
       "  ('been', 1),\n",
       "  ('before', 1),\n",
       "  ('best', 1),\n",
       "  ('between', 1),\n",
       "  ('bjork', 1),\n",
       "  ('blaring', 1),\n",
       "  ('blistering', 1),\n",
       "  ('body', 1),\n",
       "  ('both', 2),\n",
       "  ('box', 1),\n",
       "  ('boxy', 1),\n",
       "  ('breaking', 1),\n",
       "  ('breakneck', 1),\n",
       "  ('bridge', 1),\n",
       "  ('brimming', 1),\n",
       "  ('brings', 1),\n",
       "  ('british', 1),\n",
       "  ('bully', 1),\n",
       "  ('but', 3),\n",
       "  ('by', 3),\n",
       "  ('career', 1),\n",
       "  ('changes', 1),\n",
       "  ('chanted', 1),\n",
       "  ('chaos', 1),\n",
       "  ('chris', 1),\n",
       "  ('clark', 9),\n",
       "  ('clutch', 1),\n",
       "  ('coach', 1),\n",
       "  ('colonized', 1),\n",
       "  ('constants', 1),\n",
       "  ('contrast', 1),\n",
       "  ('corroded', 1),\n",
       "  ('counterpoint', 1),\n",
       "  ('crank', 2),\n",
       "  ('crucial', 1),\n",
       "  ('dazzles', 1),\n",
       "  ('death', 1),\n",
       "  ('delicate', 1),\n",
       "  ('delight', 1),\n",
       "  ('density', 1),\n",
       "  ('developed', 1),\n",
       "  ('diamond', 1),\n",
       "  ('dimensional', 1),\n",
       "  ('distant', 1),\n",
       "  ('does', 1),\n",
       "  ('dog', 3),\n",
       "  ('drama', 1),\n",
       "  ('drift', 1),\n",
       "  ('dynamism', 1),\n",
       "  ('ear', 1),\n",
       "  ('edge', 1),\n",
       "  ('electronic', 1),\n",
       "  ('emotive', 1),\n",
       "  ('emptiness', 1),\n",
       "  ('even', 1),\n",
       "  ('ever', 1),\n",
       "  ('evolution', 1),\n",
       "  ('exception', 1),\n",
       "  ('executive', 1),\n",
       "  ('expectations', 1),\n",
       "  ('experiments', 1),\n",
       "  ('explosive', 1),\n",
       "  ('extremes', 1),\n",
       "  ('falsetto', 1),\n",
       "  ('falsettos', 1),\n",
       "  ('few', 1),\n",
       "  ('film', 1),\n",
       "  ('first', 1),\n",
       "  ('flare', 1),\n",
       "  ('fleet', 1),\n",
       "  ('fluid', 1),\n",
       "  ('footed', 1),\n",
       "  ('footing', 1),\n",
       "  ('for', 3),\n",
       "  ('foregrounds', 1),\n",
       "  ('forgoes', 1),\n",
       "  ('fragile', 1),\n",
       "  ('from', 4),\n",
       "  ('furiously', 1),\n",
       "  ('garbled', 1),\n",
       "  ('gear', 1),\n",
       "  ('glides', 1),\n",
       "  ('graceful', 1),\n",
       "  ('gratifying', 1),\n",
       "  ('growing', 1),\n",
       "  ('growls', 1),\n",
       "  ('had', 1),\n",
       "  ('handsome', 1),\n",
       "  ('harmonize', 1),\n",
       "  ('has', 10),\n",
       "  ('have', 3),\n",
       "  ('he', 9),\n",
       "  ('heightened', 1),\n",
       "  ('here', 1),\n",
       "  ('high', 1),\n",
       "  ('himself', 1),\n",
       "  ('hip', 1),\n",
       "  ('his', 12),\n",
       "  ('historically', 1),\n",
       "  ('hop', 1),\n",
       "  ('hushed', 1),\n",
       "  ('idm', 1),\n",
       "  ('immediately', 1),\n",
       "  ('in', 7),\n",
       "  ('instrument', 1),\n",
       "  ('instrumental', 1),\n",
       "  ('instruments', 1),\n",
       "  ('interwoven', 1),\n",
       "  ('into', 5),\n",
       "  ('intricate', 1),\n",
       "  ('is', 5),\n",
       "  ('it', 2),\n",
       "  ('its', 2),\n",
       "  ('just', 1),\n",
       "  ('kilter', 1),\n",
       "  ('lacks', 1),\n",
       "  ('largely', 2),\n",
       "  ('latest', 1),\n",
       "  ('learned', 1),\n",
       "  ('left', 1),\n",
       "  ('levitates', 1),\n",
       "  ('lightspeed', 1),\n",
       "  ('like', 1),\n",
       "  ('limited', 2),\n",
       "  ('line', 1),\n",
       "  ('little', 1),\n",
       "  ('loveliest', 1),\n",
       "  ('lower', 1),\n",
       "  ('made', 1),\n",
       "  ('makes', 1),\n",
       "  ('margins', 1),\n",
       "  ('medicine', 1),\n",
       "  ('men', 1),\n",
       "  ('menacing', 1),\n",
       "  ('mentor', 1),\n",
       "  ('mentorship', 1),\n",
       "  ('might', 2),\n",
       "  ('minimalism', 1),\n",
       "  ('mix', 1),\n",
       "  ('mixed', 1),\n",
       "  ('momentum', 1),\n",
       "  ('more', 5),\n",
       "  ('morph', 1),\n",
       "  ('most', 2),\n",
       "  ('music', 5),\n",
       "  ('musician', 2),\n",
       "  ('never', 1),\n",
       "  ('no', 1),\n",
       "  ('nudge', 1),\n",
       "  ('of', 18),\n",
       "  ('off', 2),\n",
       "  ('offering', 1),\n",
       "  ('on', 7),\n",
       "  ('one', 3),\n",
       "  ('or', 1),\n",
       "  ('ornate', 1),\n",
       "  ('over', 2),\n",
       "  ('palette', 1),\n",
       "  ('particularly', 1),\n",
       "  ('peak', 1),\n",
       "  ('pearlers', 1),\n",
       "  ('pent', 1),\n",
       "  ('phone', 1),\n",
       "  ('piercing', 1),\n",
       "  ('place', 1),\n",
       "  ('plaintive', 1),\n",
       "  ('planet', 1),\n",
       "  ('plucks', 1),\n",
       "  ('poorly', 1),\n",
       "  ('pop', 1),\n",
       "  ('processed', 1),\n",
       "  ('produced', 1),\n",
       "  ('producer', 1),\n",
       "  ('production', 3),\n",
       "  ('propulsive', 1),\n",
       "  ('pulse', 1),\n",
       "  ('quality', 1),\n",
       "  ('radiohead', 1),\n",
       "  ('rain', 1),\n",
       "  ('range', 1),\n",
       "  ('raps', 1),\n",
       "  ('rather', 1),\n",
       "  ('reassuring', 1),\n",
       "  ('recent', 1),\n",
       "  ('record', 4),\n",
       "  ('red', 1),\n",
       "  ('reliable', 1),\n",
       "  ('reminiscent', 1),\n",
       "  ('replicating', 1),\n",
       "  ('results', 1),\n",
       "  ('rising', 1),\n",
       "  ('sheer', 1),\n",
       "  ('shifting', 1),\n",
       "  ('sighs', 1),\n",
       "  ('similarity', 1),\n",
       "  ('singing', 1),\n",
       "  ('slapping', 1),\n",
       "  ('soaring', 1),\n",
       "  ('some', 1),\n",
       "  ('sometimes', 1),\n",
       "  ('song', 1),\n",
       "  ('sounded', 1),\n",
       "  ('soundtracks', 1),\n",
       "  ('space', 1),\n",
       "  ('storm', 1),\n",
       "  ('subject', 1),\n",
       "  ('subtle', 1),\n",
       "  ('suddenly', 1),\n",
       "  ('suggestion', 1),\n",
       "  ('suicide', 1),\n",
       "  ('surfs', 1),\n",
       "  ('sus', 3),\n",
       "  ('swerved', 1),\n",
       "  ('synth', 2),\n",
       "  ('synths', 2),\n",
       "  ('taken', 1),\n",
       "  ('tearing', 1),\n",
       "  ('techno', 1),\n",
       "  ('than', 4),\n",
       "  ('that', 7),\n",
       "  ('the', 23),\n",
       "  ('their', 2),\n",
       "  ('they', 1),\n",
       "  ('think', 1),\n",
       "  ('thom', 2),\n",
       "  ('three', 1),\n",
       "  ('tics', 1),\n",
       "  ('to', 11),\n",
       "  ('together', 1),\n",
       "  ('top', 1),\n",
       "  ('totems', 1),\n",
       "  ('town', 2),\n",
       "  ('track', 1),\n",
       "  ('tracks', 1),\n",
       "  ('traffic', 1),\n",
       "  ('tricky', 1),\n",
       "  ('turns', 1),\n",
       "  ('tutelage', 1),\n",
       "  ('tv', 1),\n",
       "  ('twinkling', 1),\n",
       "  ('two', 1),\n",
       "  ('ultra', 2),\n",
       "  ('under', 1),\n",
       "  ('undergone', 1),\n",
       "  ('up', 2),\n",
       "  ('using', 1),\n",
       "  ('veers', 1),\n",
       "  ('violence', 1),\n",
       "  ('violent', 1),\n",
       "  ('vocal', 2),\n",
       "  ('voice', 6),\n",
       "  ('warm', 1),\n",
       "  ('warning', 1),\n",
       "  ('way', 1),\n",
       "  ('when', 1),\n",
       "  ('where', 2),\n",
       "  ('while', 3),\n",
       "  ('wildly', 1),\n",
       "  ('with', 9),\n",
       "  ('work', 1),\n",
       "  ('working', 1),\n",
       "  ('wrong', 1),\n",
       "  ('years', 1),\n",
       "  ('yielded', 1),\n",
       "  ('yorke', 4),\n",
       "  ('yorkean', 2),\n",
       "  ('you', 1),\n",
       "  ('your', 1)],\n",
       " [('also', 2),\n",
       "  ('an', 3),\n",
       "  ('and', 6),\n",
       "  ('are', 2),\n",
       "  ('as', 7),\n",
       "  ('at', 3),\n",
       "  ('before', 2),\n",
       "  ('best', 1),\n",
       "  ('between', 4),\n",
       "  ('blistering', 1),\n",
       "  ('but', 5),\n",
       "  ('by', 2),\n",
       "  ('career', 1),\n",
       "  ('clark', 6),\n",
       "  ('dog', 3),\n",
       "  ('extremes', 1),\n",
       "  ('first', 1),\n",
       "  ('fleet', 1),\n",
       "  ('for', 5),\n",
       "  ('from', 4),\n",
       "  ('has', 2),\n",
       "  ('he', 6),\n",
       "  ('his', 9),\n",
       "  ('in', 8),\n",
       "  ('instrumental', 1),\n",
       "  ('into', 3),\n",
       "  ('is', 6),\n",
       "  ('it', 3),\n",
       "  ('kilter', 1),\n",
       "  ('like', 5),\n",
       "  ('makes', 1),\n",
       "  ('might', 2),\n",
       "  ('most', 2),\n",
       "  ('music', 1),\n",
       "  ('of', 13),\n",
       "  ('off', 1),\n",
       "  ('on', 6),\n",
       "  ('one', 3),\n",
       "  ('over', 2),\n",
       "  ('producer', 1),\n",
       "  ('production', 2),\n",
       "  ('record', 1),\n",
       "  ('singing', 1),\n",
       "  ('song', 2),\n",
       "  ('sus', 3),\n",
       "  ('synth', 1),\n",
       "  ('techno', 1),\n",
       "  ('that', 6),\n",
       "  ('the', 20),\n",
       "  ('they', 2),\n",
       "  ('to', 6),\n",
       "  ('track', 1),\n",
       "  ('two', 2),\n",
       "  ('voice', 3),\n",
       "  ('way', 1),\n",
       "  ('when', 1),\n",
       "  ('with', 1),\n",
       "  ('work', 1),\n",
       "  ('years', 1),\n",
       "  ('you', 1),\n",
       "  ('about', 1),\n",
       "  ('achieves', 1),\n",
       "  ('aching', 1),\n",
       "  ('acoustics', 1),\n",
       "  ('adolescent', 1),\n",
       "  ('affiliate', 1),\n",
       "  ('against', 1),\n",
       "  ('air', 1),\n",
       "  ('all', 2),\n",
       "  ('almost', 1),\n",
       "  ('alyosha', 2),\n",
       "  ('amazon', 1),\n",
       "  ('angles', 1),\n",
       "  ('appear', 1),\n",
       "  ('approach', 1),\n",
       "  ('approaches', 1),\n",
       "  ('atmospheric', 1),\n",
       "  ('be', 3),\n",
       "  ('beautiful', 1),\n",
       "  ('beauty', 1),\n",
       "  ('because', 1),\n",
       "  ('behavior', 1),\n",
       "  ('believe', 1),\n",
       "  ('bleakly', 1),\n",
       "  ('bleeds', 1),\n",
       "  ('bones', 1),\n",
       "  ('bright', 1),\n",
       "  ('brilliantly', 1),\n",
       "  ('brothers', 1),\n",
       "  ('bruise', 1),\n",
       "  ('brutality', 1),\n",
       "  ('buy', 1),\n",
       "  ('calm', 1),\n",
       "  ('can', 1),\n",
       "  ('cappella', 1),\n",
       "  ('carry', 1),\n",
       "  ('chorus', 1),\n",
       "  ('clarity', 1),\n",
       "  ('clear', 1),\n",
       "  ('cleaves', 1),\n",
       "  ('closing', 1),\n",
       "  ('commission', 1),\n",
       "  ('comparison', 1),\n",
       "  ('content', 1),\n",
       "  ('conventional', 1),\n",
       "  ('could', 1),\n",
       "  ('course', 1),\n",
       "  ('croon', 1),\n",
       "  ('croons', 1),\n",
       "  ('crude', 1),\n",
       "  ('cruelty', 1),\n",
       "  ('cynical', 1),\n",
       "  ('derision', 1),\n",
       "  ('derives', 1),\n",
       "  ('despair', 1),\n",
       "  ('detailed', 1),\n",
       "  ('detuned', 1),\n",
       "  ('discovered', 1),\n",
       "  ('dismissive', 2),\n",
       "  ('distrust', 1),\n",
       "  ('drive', 1),\n",
       "  ('earn', 1),\n",
       "  ('editors', 1),\n",
       "  ('emotional', 1),\n",
       "  ('emotionally', 1),\n",
       "  ('entirely', 1),\n",
       "  ('evocative', 1),\n",
       "  ('expressing', 1),\n",
       "  ('eyed', 1),\n",
       "  ('fact', 1),\n",
       "  ('featured', 1),\n",
       "  ('felt', 1),\n",
       "  ('finely', 1),\n",
       "  ('fire', 1),\n",
       "  ('floors', 1),\n",
       "  ('forest', 1),\n",
       "  ('foxes', 1),\n",
       "  ('fuel', 1),\n",
       "  ('full', 1),\n",
       "  ('guide', 1),\n",
       "  ('harnesses', 1),\n",
       "  ('highly', 1),\n",
       "  ('however', 1),\n",
       "  ('human', 1),\n",
       "  ('hurt', 1),\n",
       "  ('independently', 1),\n",
       "  ('itself', 2),\n",
       "  ('karamazov', 1),\n",
       "  ('ladder', 2),\n",
       "  ('leap', 1),\n",
       "  ('leaps', 1),\n",
       "  ('leveraging', 1),\n",
       "  ('links', 1),\n",
       "  ('listeners', 1),\n",
       "  ('living', 1),\n",
       "  ('lyricist', 1),\n",
       "  ('making', 1),\n",
       "  ('mature', 1),\n",
       "  ('may', 1),\n",
       "  ('maze', 1),\n",
       "  ('merciless', 1),\n",
       "  ('met', 1),\n",
       "  ('metaphor', 1),\n",
       "  ('mist', 1),\n",
       "  ('moments', 1),\n",
       "  ('mournful', 2),\n",
       "  ('much', 1),\n",
       "  ('multi', 1),\n",
       "  ('nadir', 1),\n",
       "  ('naked', 1),\n",
       "  ('navigating', 1),\n",
       "  ('new', 1),\n",
       "  ('noise', 1),\n",
       "  ('not', 1),\n",
       "  ('note', 1),\n",
       "  ('octave', 1),\n",
       "  ('odd', 1),\n",
       "  ('only', 1),\n",
       "  ('opens', 1),\n",
       "  ('otherworldly', 1),\n",
       "  ('our', 2),\n",
       "  ('out', 2),\n",
       "  ('own', 1),\n",
       "  ('passive', 1),\n",
       "  ('piano', 1),\n",
       "  ('pit', 1),\n",
       "  ('pitched', 1),\n",
       "  ('pitchfork', 1),\n",
       "  ('pleas', 1),\n",
       "  ('please', 1),\n",
       "  ('poles', 1),\n",
       "  ('power', 1),\n",
       "  ('practicality', 1),\n",
       "  ('products', 1),\n",
       "  ('protagonist', 1),\n",
       "  ('raw', 1),\n",
       "  ('recognizing', 1),\n",
       "  ('reference', 1),\n",
       "  ('refrain', 1),\n",
       "  ('refrains', 1),\n",
       "  ('remains', 1),\n",
       "  ('repeating', 1),\n",
       "  ('resolutely', 1),\n",
       "  ('retail', 1),\n",
       "  ('ribbons', 1),\n",
       "  ('rises', 1),\n",
       "  ('risk', 1),\n",
       "  ('rival', 1),\n",
       "  ('rough', 1),\n",
       "  ('scorching', 1),\n",
       "  ('selected', 1),\n",
       "  ('sensitivity', 1),\n",
       "  ('separate', 1),\n",
       "  ('shapes', 1),\n",
       "  ('shortcoming', 1),\n",
       "  ('sings', 1),\n",
       "  ('situated', 1),\n",
       "  ('so', 1),\n",
       "  ('something', 1),\n",
       "  ('songwriting', 2),\n",
       "  ('spheres', 1),\n",
       "  ('strain', 1),\n",
       "  ('striking', 1),\n",
       "  ('stuck', 1),\n",
       "  ('such', 1),\n",
       "  ('swelling', 1),\n",
       "  ('terrain', 1),\n",
       "  ('text', 1),\n",
       "  ('this', 1),\n",
       "  ('those', 1),\n",
       "  ('though', 1),\n",
       "  ('through', 2),\n",
       "  ('time', 1),\n",
       "  ('tinny', 1),\n",
       "  ('title', 2),\n",
       "  ('toggled', 1),\n",
       "  ('tone', 1),\n",
       "  ('tracked', 1),\n",
       "  ('trade', 1),\n",
       "  ('transforming', 1),\n",
       "  ('truly', 1),\n",
       "  ('turn', 1),\n",
       "  ('unanswerable', 1),\n",
       "  ('understanding', 1),\n",
       "  ('unknowability', 1),\n",
       "  ('until', 1),\n",
       "  ('vague', 1),\n",
       "  ('virtuous', 1),\n",
       "  ('want', 1),\n",
       "  ('we', 1),\n",
       "  ('wearily', 1),\n",
       "  ('weather', 1),\n",
       "  ('which', 1),\n",
       "  ('whose', 1),\n",
       "  ('wild', 2),\n",
       "  ('words', 1),\n",
       "  ('zen', 1),\n",
       "  ('zigzagging', 1)]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make it slightly more readable\n",
    "[[(dictionary[id], count) for id, count in line] for line in mycorpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of stop words use `[word for word if not in stop_words]` if it's a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to lemmatize words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "#> bat\n",
    "\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "#> are\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "#> foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The striped bat hanging foot best\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize list of words, remove stop_words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list if w not in stop_words])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'shifting', 'atmosphere', 'distant', 'planet']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 2: Prepare Data (Remove stopwords and lemmatize)\n",
    "data_processed = []\n",
    "\n",
    "for i, doc in enumerate(preprocessed[0:2][:20]):\n",
    "    doc_out = []\n",
    "    for wd in doc:\n",
    "        if wd not in stop_words:  # remove stopwords\n",
    "            lemmatized_word = lemmatizer.lemmatize(wd)  # lemmatize\n",
    "            if lemmatized_word:\n",
    "                doc_out = doc_out + [lemmatized_word]\n",
    "        else:\n",
    "            continue\n",
    "    data_processed.append(doc_out)\n",
    "    \n",
    "print(data_processed[0][:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create the Inputs of LDA model: Dictionary and Corpus\n",
    "dct = corpora.Dictionary(data_processed)\n",
    "corpus = [dct.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"clark\" + 0.012*\"voice\" + 0.011*\"music\" + 0.009*\"yorke\" + 0.008*\"record\" + 0.007*\"one\" + 0.007*\"production\" + 0.007*\"beat\" + 0.007*\"dog\" + 0.007*\"sus\"'),\n",
       " (1,\n",
       "  '0.003*\"alternately\" + 0.003*\"constant\" + 0.003*\"may\" + 0.003*\"ultra\" + 0.003*\"reference\" + 0.003*\"pearler\" + 0.003*\"planet\" + 0.003*\"bright\" + 0.003*\"hip\" + 0.003*\"earn\"'),\n",
       " (2,\n",
       "  '0.003*\"developed\" + 0.003*\"vocal\" + 0.003*\"morph\" + 0.003*\"acoustic\" + 0.003*\"situated\" + 0.003*\"range\" + 0.003*\"constant\" + 0.003*\"eyed\" + 0.003*\"appear\" + 0.003*\"might\"'),\n",
       " (3,\n",
       "  '0.014*\"clark\" + 0.011*\"like\" + 0.008*\"voice\" + 0.007*\"sus\" + 0.007*\"dog\" + 0.007*\"one\" + 0.005*\"refrain\" + 0.005*\"production\" + 0.005*\"wild\" + 0.005*\"ladder\"'),\n",
       " (4,\n",
       "  '0.003*\"virtuous\" + 0.003*\"emotional\" + 0.003*\"scorching\" + 0.003*\"clark\" + 0.003*\"morph\" + 0.003*\"two\" + 0.003*\"striking\" + 0.003*\"like\" + 0.003*\"crucial\" + 0.003*\"loveliest\"'),\n",
       " (5,\n",
       "  '0.003*\"dog\" + 0.003*\"beautiful\" + 0.003*\"recognizing\" + 0.003*\"pop\" + 0.003*\"clark\" + 0.003*\"one\" + 0.003*\"way\" + 0.003*\"like\" + 0.003*\"title\" + 0.003*\"instrumental\"'),\n",
       " (6,\n",
       "  '0.003*\"two\" + 0.003*\"like\" + 0.003*\"appear\" + 0.003*\"adolescent\" + 0.003*\"cleaves\" + 0.003*\"rise\" + 0.003*\"dismissive\" + 0.003*\"beauty\" + 0.003*\"clark\" + 0.003*\"eyed\"')]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the LDA model\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1) # -1 is to rank it from 1 up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.9974514)],\n",
       " [(0, [0]),\n",
       "  (1, [0]),\n",
       "  (2, [0]),\n",
       "  (3, [0]),\n",
       "  (4, [0]),\n",
       "  (5, [0]),\n",
       "  (6, [0]),\n",
       "  (7, [0]),\n",
       "  (8, [0]),\n",
       "  (9, [0]),\n",
       "  (10, [0]),\n",
       "  (11, [0]),\n",
       "  (12, [0]),\n",
       "  (13, [0]),\n",
       "  (14, [0]),\n",
       "  (15, [0]),\n",
       "  (16, [0]),\n",
       "  (17, [0]),\n",
       "  (18, [0]),\n",
       "  (19, [0]),\n",
       "  (20, [0]),\n",
       "  (21, [0]),\n",
       "  (22, [0]),\n",
       "  (23, [0]),\n",
       "  (24, [0]),\n",
       "  (25, [0]),\n",
       "  (26, [0]),\n",
       "  (27, [0]),\n",
       "  (28, [0]),\n",
       "  (29, [0]),\n",
       "  (30, [0]),\n",
       "  (31, [0]),\n",
       "  (32, [0]),\n",
       "  (33, [0]),\n",
       "  (34, [0]),\n",
       "  (35, [0]),\n",
       "  (36, [0]),\n",
       "  (37, [0]),\n",
       "  (38, [0]),\n",
       "  (39, [0]),\n",
       "  (40, [0]),\n",
       "  (41, [0]),\n",
       "  (42, [0]),\n",
       "  (43, [0]),\n",
       "  (44, [0]),\n",
       "  (45, [0]),\n",
       "  (46, [0]),\n",
       "  (47, [0]),\n",
       "  (48, [0]),\n",
       "  (49, [0]),\n",
       "  (50, [0]),\n",
       "  (51, [0]),\n",
       "  (52, [0]),\n",
       "  (53, [0]),\n",
       "  (54, [0]),\n",
       "  (55, [0]),\n",
       "  (56, [0]),\n",
       "  (57, [0]),\n",
       "  (58, [0]),\n",
       "  (59, [0]),\n",
       "  (60, [0]),\n",
       "  (61, [0]),\n",
       "  (62, [0]),\n",
       "  (63, [0]),\n",
       "  (64, [0]),\n",
       "  (65, [0]),\n",
       "  (66, [0]),\n",
       "  (67, [0]),\n",
       "  (68, [0]),\n",
       "  (69, [0]),\n",
       "  (70, [0]),\n",
       "  (71, [0]),\n",
       "  (72, [0]),\n",
       "  (73, [0]),\n",
       "  (74, [0]),\n",
       "  (75, [0]),\n",
       "  (76, [0]),\n",
       "  (77, [0]),\n",
       "  (78, [0]),\n",
       "  (79, [0]),\n",
       "  (80, [0]),\n",
       "  (81, [0]),\n",
       "  (82, [0]),\n",
       "  (83, [0]),\n",
       "  (84, [0]),\n",
       "  (85, [0]),\n",
       "  (86, [0]),\n",
       "  (87, [0]),\n",
       "  (88, [0]),\n",
       "  (89, [0]),\n",
       "  (90, [0]),\n",
       "  (91, [0]),\n",
       "  (92, [0]),\n",
       "  (93, [0]),\n",
       "  (94, [0]),\n",
       "  (95, [0]),\n",
       "  (96, [0]),\n",
       "  (97, [0]),\n",
       "  (98, [0]),\n",
       "  (99, [0]),\n",
       "  (100, [0]),\n",
       "  (101, [0]),\n",
       "  (102, [0]),\n",
       "  (103, [0]),\n",
       "  (104, [0]),\n",
       "  (105, [0]),\n",
       "  (106, [0]),\n",
       "  (107, [0]),\n",
       "  (108, [0]),\n",
       "  (109, [0]),\n",
       "  (110, [0]),\n",
       "  (111, [0]),\n",
       "  (112, [0]),\n",
       "  (113, [0]),\n",
       "  (114, [0]),\n",
       "  (115, [0]),\n",
       "  (116, [0]),\n",
       "  (117, [0]),\n",
       "  (118, [0]),\n",
       "  (119, [0]),\n",
       "  (120, [0]),\n",
       "  (121, [0]),\n",
       "  (122, [0]),\n",
       "  (123, [0]),\n",
       "  (124, [0]),\n",
       "  (125, [0]),\n",
       "  (126, [0]),\n",
       "  (127, [0]),\n",
       "  (128, [0]),\n",
       "  (129, [0]),\n",
       "  (130, [0]),\n",
       "  (131, [0]),\n",
       "  (132, [0]),\n",
       "  (133, [0]),\n",
       "  (134, [0]),\n",
       "  (135, [0]),\n",
       "  (136, [0]),\n",
       "  (137, [0]),\n",
       "  (138, [0]),\n",
       "  (139, [0]),\n",
       "  (140, [0]),\n",
       "  (141, [0]),\n",
       "  (142, [0]),\n",
       "  (143, [0]),\n",
       "  (144, [0]),\n",
       "  (145, [0]),\n",
       "  (146, [0]),\n",
       "  (147, [0]),\n",
       "  (148, [0]),\n",
       "  (149, [0]),\n",
       "  (150, [0]),\n",
       "  (151, [0]),\n",
       "  (152, [0]),\n",
       "  (153, [0]),\n",
       "  (154, [0]),\n",
       "  (155, [0]),\n",
       "  (156, [0]),\n",
       "  (157, [0]),\n",
       "  (158, [0]),\n",
       "  (159, [0]),\n",
       "  (160, [0]),\n",
       "  (161, [0]),\n",
       "  (162, [0]),\n",
       "  (163, [0]),\n",
       "  (164, [0]),\n",
       "  (165, [0]),\n",
       "  (166, [0]),\n",
       "  (167, [0]),\n",
       "  (168, [0]),\n",
       "  (169, [0]),\n",
       "  (170, [0]),\n",
       "  (171, [0]),\n",
       "  (172, [0]),\n",
       "  (173, [0]),\n",
       "  (174, [0]),\n",
       "  (175, [0]),\n",
       "  (176, [0]),\n",
       "  (177, [0]),\n",
       "  (178, [0]),\n",
       "  (179, [0]),\n",
       "  (180, [0]),\n",
       "  (181, [0]),\n",
       "  (182, [0]),\n",
       "  (183, [0]),\n",
       "  (184, [0]),\n",
       "  (185, [0]),\n",
       "  (186, [0]),\n",
       "  (187, [0]),\n",
       "  (188, [0]),\n",
       "  (189, [0]),\n",
       "  (190, [0]),\n",
       "  (191, [0]),\n",
       "  (192, [0]),\n",
       "  (193, [0]),\n",
       "  (194, [0]),\n",
       "  (195, [0]),\n",
       "  (196, [0]),\n",
       "  (197, [0]),\n",
       "  (198, [0]),\n",
       "  (199, [0]),\n",
       "  (200, [0]),\n",
       "  (201, [0]),\n",
       "  (202, [0]),\n",
       "  (203, [0]),\n",
       "  (204, [0]),\n",
       "  (205, [0]),\n",
       "  (206, [0]),\n",
       "  (207, [0]),\n",
       "  (208, [0]),\n",
       "  (209, [0]),\n",
       "  (210, [0]),\n",
       "  (211, [0]),\n",
       "  (212, [0]),\n",
       "  (213, [0]),\n",
       "  (214, [0]),\n",
       "  (215, [0]),\n",
       "  (216, [0]),\n",
       "  (217, [0]),\n",
       "  (218, [0]),\n",
       "  (219, [0]),\n",
       "  (220, [0]),\n",
       "  (221, [0]),\n",
       "  (222, [0]),\n",
       "  (223, [0]),\n",
       "  (224, [0]),\n",
       "  (225, [0]),\n",
       "  (226, [0]),\n",
       "  (227, [0]),\n",
       "  (228, [0]),\n",
       "  (229, [0]),\n",
       "  (230, [0]),\n",
       "  (231, [0]),\n",
       "  (232, [0]),\n",
       "  (233, [0]),\n",
       "  (234, [0]),\n",
       "  (235, [0]),\n",
       "  (236, [0]),\n",
       "  (237, [0]),\n",
       "  (238, [0]),\n",
       "  (239, [0]),\n",
       "  (240, [0])],\n",
       " [(0, [(0, 0.9999238)]),\n",
       "  (1, [(0, 0.99992895)]),\n",
       "  (2, [(0, 0.99993217)]),\n",
       "  (3, [(0, 0.999929)]),\n",
       "  (4, [(0, 0.9999285)]),\n",
       "  (5, [(0, 0.9999294)]),\n",
       "  (6, [(0, 0.9999284)]),\n",
       "  (7, [(0, 0.9999289)]),\n",
       "  (8, [(0, 0.9999286)]),\n",
       "  (9, [(0, 0.99992883)]),\n",
       "  (10, [(0, 0.9999261)]),\n",
       "  (11, [(0, 0.99992543)]),\n",
       "  (12, [(0, 0.9999335)]),\n",
       "  (13, [(0, 0.9999319)]),\n",
       "  (14, [(0, 0.99993)]),\n",
       "  (15, [(0, 0.99993116)]),\n",
       "  (16, [(0, 0.9999282)]),\n",
       "  (17, [(0, 0.999928)]),\n",
       "  (18, [(0, 0.99992675)]),\n",
       "  (19, [(0, 2.9999301)]),\n",
       "  (20, [(0, 0.9999316)]),\n",
       "  (21, [(0, 0.9999245)]),\n",
       "  (22, [(0, 0.99992615)]),\n",
       "  (23, [(0, 0.9999265)]),\n",
       "  (24, [(0, 0.99993265)]),\n",
       "  (25, [(0, 0.9999236)]),\n",
       "  (26, [(0, 0.9999241)]),\n",
       "  (27, [(0, 0.9999329)]),\n",
       "  (28, [(0, 0.9999242)]),\n",
       "  (29, [(0, 0.99992704)]),\n",
       "  (30, [(0, 0.99992913)]),\n",
       "  (31, [(0, 0.9999298)]),\n",
       "  (32, [(0, 0.9999295)]),\n",
       "  (33, [(0, 0.99992585)]),\n",
       "  (34, [(0, 0.9999288)]),\n",
       "  (35, [(0, 0.9999279)]),\n",
       "  (36, [(0, 0.99992853)]),\n",
       "  (37, [(0, 0.99993056)]),\n",
       "  (38, [(0, 0.999925)]),\n",
       "  (39, [(0, 0.9999294)]),\n",
       "  (40, [(0, 0.99993443)]),\n",
       "  (41, [(0, 8.999925)]),\n",
       "  (42, [(0, 0.9999299)]),\n",
       "  (43, [(0, 0.99992615)]),\n",
       "  (44, [(0, 0.99992335)]),\n",
       "  (45, [(0, 0.9999288)]),\n",
       "  (46, [(0, 0.99993044)]),\n",
       "  (47, [(0, 0.999928)]),\n",
       "  (48, [(0, 0.9999262)]),\n",
       "  (49, [(0, 1.9999261)]),\n",
       "  (50, [(0, 0.9999345)]),\n",
       "  (51, [(0, 0.99992776)]),\n",
       "  (52, [(0, 0.99992776)]),\n",
       "  (53, [(0, 0.99992865)]),\n",
       "  (54, [(0, 0.9999221)]),\n",
       "  (55, [(0, 0.9999257)]),\n",
       "  (56, [(0, 0.99992746)]),\n",
       "  (57, [(0, 0.99992764)]),\n",
       "  (58, [(0, 0.99992853)]),\n",
       "  (59, [(0, 0.99992883)]),\n",
       "  (60, [(0, 2.9999275)]),\n",
       "  (61, [(0, 0.9999263)]),\n",
       "  (62, [(0, 0.9999268)]),\n",
       "  (63, [(0, 0.99992585)]),\n",
       "  (64, [(0, 0.99993265)]),\n",
       "  (65, [(0, 0.9999302)]),\n",
       "  (66, [(0, 0.99992406)]),\n",
       "  (67, [(0, 0.99993557)]),\n",
       "  (68, [(0, 0.9999287)]),\n",
       "  (69, [(0, 0.9999238)]),\n",
       "  (70, [(0, 0.99993074)]),\n",
       "  (71, [(0, 0.9999311)]),\n",
       "  (72, [(0, 0.9999319)]),\n",
       "  (73, [(0, 0.9999311)]),\n",
       "  (74, [(0, 0.99992573)]),\n",
       "  (75, [(0, 0.9999278)]),\n",
       "  (76, [(0, 0.9999255)]),\n",
       "  (77, [(0, 0.99992925)]),\n",
       "  (78, [(0, 1.9999292)]),\n",
       "  (79, [(0, 0.99992996)]),\n",
       "  (80, [(0, 0.9999299)]),\n",
       "  (81, [(0, 0.9999256)]),\n",
       "  (82, [(0, 0.9999249)]),\n",
       "  (83, [(0, 0.99992955)]),\n",
       "  (84, [(0, 0.99993557)]),\n",
       "  (85, [(0, 0.9999283)]),\n",
       "  (86, [(0, 0.9999285)]),\n",
       "  (87, [(0, 0.9999255)]),\n",
       "  (88, [(0, 0.99993235)]),\n",
       "  (89, [(0, 0.9999283)]),\n",
       "  (90, [(0, 0.9999281)]),\n",
       "  (91, [(0, 0.99992704)]),\n",
       "  (92, [(0, 0.99993026)]),\n",
       "  (93, [(0, 0.9999243)]),\n",
       "  (94, [(0, 0.9999288)]),\n",
       "  (95, [(0, 0.99993175)]),\n",
       "  (96, [(0, 0.9999299)]),\n",
       "  (97, [(0, 0.9999249)]),\n",
       "  (98, [(0, 0.9999289)]),\n",
       "  (99, [(0, 0.9999343)]),\n",
       "  (100, [(0, 0.99992675)]),\n",
       "  (101, [(0, 0.9999228)]),\n",
       "  (102, [(0, 0.9999281)]),\n",
       "  (103, [(0, 0.99992776)]),\n",
       "  (104, [(0, 0.99993026)]),\n",
       "  (105, [(0, 0.9999266)]),\n",
       "  (106, [(0, 0.9999273)]),\n",
       "  (107, [(0, 1.999927)]),\n",
       "  (108, [(0, 0.99993414)]),\n",
       "  (109, [(0, 0.99993044)]),\n",
       "  (110, [(0, 0.9999273)]),\n",
       "  (111, [(0, 0.99992865)]),\n",
       "  (112, [(0, 0.99992764)]),\n",
       "  (113, [(0, 1.9999309)]),\n",
       "  (114, [(0, 0.9999313)]),\n",
       "  (115, [(0, 0.9999265)]),\n",
       "  (116, [(0, 0.9999324)]),\n",
       "  (117, [(0, 0.99992484)]),\n",
       "  (118, [(0, 0.9999306)]),\n",
       "  (119, [(0, 0.99992466)]),\n",
       "  (120, [(0, 1.9999304)]),\n",
       "  (121, [(0, 0.99992394)]),\n",
       "  (122, [(0, 0.99993)]),\n",
       "  (123, [(0, 0.99992806)]),\n",
       "  (124, [(0, 0.99992585)]),\n",
       "  (125, [(0, 0.9999382)]),\n",
       "  (126, [(0, 0.9999268)]),\n",
       "  (127, [(0, 0.99992347)]),\n",
       "  (128, [(0, 0.999932)]),\n",
       "  (129, [(0, 0.99992603)]),\n",
       "  (130, [(0, 0.99992555)]),\n",
       "  (131, [(0, 0.9999255)]),\n",
       "  (132, [(0, 0.99992716)]),\n",
       "  (133, [(0, 1.9999276)]),\n",
       "  (134, [(0, 0.9999226)]),\n",
       "  (135, [(0, 0.9999282)]),\n",
       "  (136, [(0, 0.999926)]),\n",
       "  (137, [(0, 0.99992806)]),\n",
       "  (138, [(0, 0.99993104)]),\n",
       "  (139, [(0, 4.9999294)]),\n",
       "  (140, [(0, 1.9999318)]),\n",
       "  (141, [(0, 0.9999278)]),\n",
       "  (142, [(0, 0.9999319)]),\n",
       "  (143, [(0, 0.9999291)]),\n",
       "  (144, [(0, 2.9999297)]),\n",
       "  (145, [(0, 0.99992496)]),\n",
       "  (146, [(0, 0.9999303)]),\n",
       "  (147, [(0, 0.99992305)]),\n",
       "  (148, [(0, 0.999933)]),\n",
       "  (149, [(0, 0.9999257)]),\n",
       "  (150, [(0, 0.9999284)]),\n",
       "  (151, [(0, 0.99992794)]),\n",
       "  (152, [(0, 0.9999281)]),\n",
       "  (153, [(0, 0.9999324)]),\n",
       "  (154, [(0, 0.99993145)]),\n",
       "  (155, [(0, 0.99993)]),\n",
       "  (156, [(0, 0.99992865)]),\n",
       "  (157, [(0, 0.9999297)]),\n",
       "  (158, [(0, 0.99993426)]),\n",
       "  (159, [(0, 0.9999307)]),\n",
       "  (160, [(0, 0.9999285)]),\n",
       "  (161, [(0, 0.9999292)]),\n",
       "  (162, [(0, 2.9999285)]),\n",
       "  (163, [(0, 0.9999254)]),\n",
       "  (164, [(0, 0.99992985)]),\n",
       "  (165, [(0, 0.99992627)]),\n",
       "  (166, [(0, 0.999922)]),\n",
       "  (167, [(0, 0.99992895)]),\n",
       "  (168, [(0, 0.99993616)]),\n",
       "  (169, [(0, 0.9999382)]),\n",
       "  (170, [(0, 0.999933)]),\n",
       "  (171, [(0, 0.99993163)]),\n",
       "  (172, [(0, 0.99993044)]),\n",
       "  (173, [(0, 3.999928)]),\n",
       "  (174, [(0, 0.9999301)]),\n",
       "  (175, [(0, 0.9999312)]),\n",
       "  (176, [(0, 0.9999291)]),\n",
       "  (177, [(0, 0.99993324)]),\n",
       "  (178, [(0, 0.9999319)]),\n",
       "  (179, [(0, 0.99992853)]),\n",
       "  (180, [(0, 0.9999317)]),\n",
       "  (181, [(0, 0.9999272)]),\n",
       "  (182, [(0, 0.9999305)]),\n",
       "  (183, [(0, 0.99992967)]),\n",
       "  (184, [(0, 0.9999275)]),\n",
       "  (185, [(0, 0.9999273)]),\n",
       "  (186, [(0, 0.99992555)]),\n",
       "  (187, [(0, 0.99992335)]),\n",
       "  (188, [(0, 0.99993587)]),\n",
       "  (189, [(0, 0.99992317)]),\n",
       "  (190, [(0, 0.9999311)]),\n",
       "  (191, [(0, 0.99993175)]),\n",
       "  (192, [(0, 0.9999285)]),\n",
       "  (193, [(0, 0.9999305)]),\n",
       "  (194, [(0, 0.9999254)]),\n",
       "  (195, [(0, 0.9999295)]),\n",
       "  (196, [(0, 0.9999312)]),\n",
       "  (197, [(0, 0.99992824)]),\n",
       "  (198, [(0, 0.99992824)]),\n",
       "  (199, [(0, 2.999926)]),\n",
       "  (200, [(0, 0.99992913)]),\n",
       "  (201, [(0, 1.9999279)]),\n",
       "  (202, [(0, 1.9999307)]),\n",
       "  (203, [(0, 0.99993235)]),\n",
       "  (204, [(0, 0.99992216)]),\n",
       "  (205, [(0, 0.9999275)]),\n",
       "  (206, [(0, 0.99992245)]),\n",
       "  (207, [(0, 1.9999278)]),\n",
       "  (208, [(0, 0.9999264)]),\n",
       "  (209, [(0, 0.99993664)]),\n",
       "  (210, [(0, 0.99993056)]),\n",
       "  (211, [(0, 0.999925)]),\n",
       "  (212, [(0, 0.99992186)]),\n",
       "  (213, [(0, 1.9999292)]),\n",
       "  (214, [(0, 1.9999261)]),\n",
       "  (215, [(0, 0.9999314)]),\n",
       "  (216, [(0, 0.9999284)]),\n",
       "  (217, [(0, 0.99992746)]),\n",
       "  (218, [(0, 0.99992275)]),\n",
       "  (219, [(0, 0.9999298)]),\n",
       "  (220, [(0, 0.9999266)]),\n",
       "  (221, [(0, 0.99992424)]),\n",
       "  (222, [(0, 1.9999257)]),\n",
       "  (223, [(0, 0.99992853)]),\n",
       "  (224, [(0, 0.9999231)]),\n",
       "  (225, [(0, 0.99992913)]),\n",
       "  (226, [(0, 0.99993306)]),\n",
       "  (227, [(0, 0.9999273)]),\n",
       "  (228, [(0, 1.9999303)]),\n",
       "  (229, [(0, 5.999927)]),\n",
       "  (230, [(0, 0.99992514)]),\n",
       "  (231, [(0, 0.9999321)]),\n",
       "  (232, [(0, 0.9999256)]),\n",
       "  (233, [(0, 0.99992967)]),\n",
       "  (234, [(0, 0.9999283)]),\n",
       "  (235, [(0, 0.9999271)]),\n",
       "  (236, [(0, 0.9999307)]),\n",
       "  (237, [(0, 0.99992156)]),\n",
       "  (238, [(0, 0.99992675)]),\n",
       "  (239, [(0, 3.9999294)]),\n",
       "  (240, [(0, 1.9999313)])])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[corpus[0]] # to show the phi values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not super interesting: Phi value is the probability of the word belonging to that particular topic. And the sum of phi values for a given word adds up to the number of times that word occurred in that document. https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "## Alternative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"clark\" + 0.012*\"music\" + 0.011*\"voice\" + 0.009*\"one\" + 0.008*\"yorke\" + 0.008*\"production\" + 0.008*\"dog\" + 0.007*\"sus\" + 0.007*\"record\" + 0.007*\"beat\"'),\n",
       " (1,\n",
       "  '0.018*\"clark\" + 0.010*\"voice\" + 0.009*\"music\" + 0.007*\"sus\" + 0.007*\"dog\" + 0.007*\"one\" + 0.007*\"production\" + 0.006*\"like\" + 0.006*\"yorke\" + 0.005*\"record\"'),\n",
       " (2,\n",
       "  '0.013*\"clark\" + 0.009*\"voice\" + 0.007*\"sus\" + 0.007*\"like\" + 0.006*\"record\" + 0.006*\"one\" + 0.006*\"music\" + 0.006*\"might\" + 0.006*\"production\" + 0.005*\"dog\"'),\n",
       " (3,\n",
       "  '0.021*\"clark\" + 0.016*\"voice\" + 0.009*\"record\" + 0.009*\"sus\" + 0.008*\"music\" + 0.008*\"yorke\" + 0.007*\"production\" + 0.007*\"dog\" + 0.006*\"one\" + 0.006*\"like\"'),\n",
       " (4,\n",
       "  '0.015*\"clark\" + 0.009*\"like\" + 0.007*\"voice\" + 0.007*\"one\" + 0.006*\"dog\" + 0.006*\"sus\" + 0.006*\"music\" + 0.005*\"production\" + 0.005*\"two\" + 0.005*\"might\"'),\n",
       " (5,\n",
       "  '0.014*\"clark\" + 0.010*\"like\" + 0.010*\"dog\" + 0.008*\"one\" + 0.007*\"sus\" + 0.007*\"voice\" + 0.006*\"production\" + 0.005*\"music\" + 0.005*\"title\" + 0.005*\"might\"'),\n",
       " (6,\n",
       "  '0.018*\"clark\" + 0.013*\"voice\" + 0.010*\"like\" + 0.008*\"dog\" + 0.008*\"sus\" + 0.008*\"one\" + 0.007*\"might\" + 0.006*\"record\" + 0.006*\"production\" + 0.006*\"two\"')]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda2 = LdaModel(corpus=corpus,\n",
    "                id2word=dct,\n",
    "                random_state=100,\n",
    "                num_topics=7)\n",
    "\n",
    "lda2.print_topics(-1) # -1 is to rank it from 1 up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.446*\"clark\" + 0.276*\"voice\" + 0.198*\"music\" + 0.170*\"dog\" + 0.170*\"sus\" + '\n",
      "  '0.170*\"one\" + 0.162*\"record\" + 0.149*\"production\" + 0.143*\"like\" + '\n",
      "  '0.141*\"yorke\"'),\n",
      " (1,\n",
      "  '-0.237*\"like\" + 0.132*\"yorke\" + 0.111*\"music\" + -0.108*\"approach\" + '\n",
      "  '-0.108*\"dismissive\" + -0.108*\"refrain\" + -0.108*\"mournful\" + -0.108*\"leap\" '\n",
      "  '+ -0.108*\"ladder\" + -0.108*\"songwriting\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "import pprint\n",
    "\n",
    "# Build the LSI Model\n",
    "lsi_model = LsiModel(corpus=corpus, id2word=dct, num_topics=7, decay=0.5)\n",
    "\n",
    "# View Topics\n",
    "pprint.pprint(lsi_model.print_topics(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Gensims Word2Vec implementation lets you train your own word embedding model for a given corpus.\n",
    "\n",
    "A word embedding model is a model that can provide numerical vectors for a given word. Using the Gensims downloader API, you can download pre-built word embedding models like word2vec, fasttext, GloVe and ConceptNet. These are built on large corpuses of commonly occurring text data such as wikipedia, google news etc.\n",
    "\n",
    "\n",
    "Start [here - the official tutorials](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9205213 ,  0.27776983, -0.05177611,  0.46860605, -0.8798394 ,\n",
       "       -1.3324406 , -1.435962  ,  0.32900223,  0.5565165 ,  0.26718137,\n",
       "        0.11481123,  0.89353824, -1.1467557 , -1.0474638 , -0.99056137,\n",
       "        1.1820714 , -1.2716994 , -0.51987416,  1.2844187 ,  0.4771293 ,\n",
       "        0.25608808,  0.9423585 , -0.2810102 ,  0.41437182, -0.43062216,\n",
       "        0.25769383, -0.09235616, -0.21599683, -0.09595884,  0.10432298,\n",
       "        1.2329164 , -0.14473377, -0.90032524, -1.2211845 ,  0.0586639 ,\n",
       "       -0.20559865, -1.729614  , -0.9255956 ,  0.12454402, -0.5193946 ,\n",
       "       -0.05347501, -0.6232367 , -0.16193298, -0.6031882 ,  0.22658087,\n",
       "        0.4826208 , -0.48977122,  0.57894325, -0.75426376,  0.9157322 ,\n",
       "        0.25455776, -0.44881257, -0.17192224, -0.45753956, -0.6101924 ,\n",
       "       -0.35173488, -0.06816396, -0.3276552 , -1.3871515 ,  1.0855129 ,\n",
       "        0.78473115,  0.7800539 ,  0.13769488,  0.0279958 , -0.68402493,\n",
       "       -0.6093991 ,  0.83312124, -0.2895124 , -1.0896566 , -0.0491749 ,\n",
       "        0.01589174,  0.38084778,  0.42404464,  0.32114476,  0.9868341 ,\n",
       "       -0.30203986,  0.6870049 , -0.21836227, -0.2946445 , -0.47034597,\n",
       "        0.1504458 , -0.2334175 , -0.83183634,  0.22761011,  0.42970327,\n",
       "        0.72524196, -1.439875  , -0.37321475,  1.3520558 , -0.5240843 ,\n",
       "       -0.7560214 ,  0.1729432 ,  0.10696999,  0.31782538,  0.4401239 ,\n",
       "        0.6204558 ,  0.8641746 , -0.16985387,  0.39187995,  0.64232343],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the word vector for given word\n",
    "model.wv['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interpretation', 0.7315137982368469),\n",
       " ('discussion', 0.6999590992927551),\n",
       " ('discourse', 0.6952456831932068),\n",
       " ('characterization', 0.6894254684448242),\n",
       " ('consensus', 0.685240626335144),\n",
       " ('debate', 0.6841682195663452),\n",
       " ('explanation', 0.6839592456817627),\n",
       " ('premise', 0.6786676049232483),\n",
       " ('focus', 0.670373797416687),\n",
       " ('speculation', 0.6699302196502686)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.1\n"
     ]
    }
   ],
   "source": [
    "# so many tutorials don't work - good to check: version!\n",
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[286], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize, keywords\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[0;32m      4\u001b[0m summarize(content)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "from pprint import pprint\n",
    "\n",
    "summarize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join((line for line in content))\n",
    "\n",
    "# Summarize the paragraph\n",
    "pprint(summarize(text, word_count=20))\n",
    "#> ('the PLA Rocket Force national defense science and technology experts panel, '\n",
    "#>  'according to a report published by the')\n",
    "\n",
    "# Important keywords from the paragraph\n",
    "print(keywords(text))\n",
    "#> force zhang technology experts pla rocket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment\n",
    "\n",
    "a quick example from chatgpt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.643, 'neu': 0.357, 'pos': 0.0, 'compound': -0.5574}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence = r\"this is shit\"\n",
    "# \"NLTK is a great library for natural language processing!\"\n",
    "\n",
    "polarity = sia.polarity_scores(sentence)\n",
    "\n",
    "print(polarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `polarity_scores` method returns a dictionary with the negative, neutral, positive, and compound sentiment scores. The compound score is a single metric that calculates the sum of all the lexicon ratings and normalizes it between -1 (most extreme negative) and +1 (most extreme positive).\n",
    "\n",
    "Please note that VADER is best used for language used in social media, like short sentences with some slang and abbreviations. It's not as good for longer texts with more formal language.\n",
    "\n",
    "\n",
    "## back to [the official gensim tutorials](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a corpus is like this, a list of \"documents\":\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simple_preprocess` converts a `document` into a list of lowercase tokens, ignoring tokens that are too short or too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human',\n",
       "  'machine',\n",
       "  'interface',\n",
       "  'for',\n",
       "  'lab',\n",
       "  'abc',\n",
       "  'computer',\n",
       "  'applications'],\n",
       " ['survey',\n",
       "  'of',\n",
       "  'user',\n",
       "  'opinion',\n",
       "  'of',\n",
       "  'computer',\n",
       "  'system',\n",
       "  'response',\n",
       "  'time'],\n",
       " ['the', 'eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'and', 'human', 'system', 'engineering', 'testing', 'of', 'eps'],\n",
       " ['relation',\n",
       "  'of',\n",
       "  'user',\n",
       "  'perceived',\n",
       "  'response',\n",
       "  'time',\n",
       "  'to',\n",
       "  'error',\n",
       "  'measurement'],\n",
       " ['the', 'generation', 'of', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['the', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'],\n",
       " ['graph',\n",
       "  'minors',\n",
       "  'iv',\n",
       "  'widths',\n",
       "  'of',\n",
       "  'trees',\n",
       "  'and',\n",
       "  'well',\n",
       "  'quasi',\n",
       "  'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "docs = []\n",
    "for doc in text_corpus:\n",
    "    text = simple_preprocess(doc)\n",
    "    docs.append(text)\n",
    "    \n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More preprocessing:**\n",
    "- omit stopwords\n",
    "- frequency count\n",
    "- omit words that appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to associate each word in the corpus with a unique integer ID. We can do this using the gensim.corpora.Dictionary class. This dictionary defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dictionary associates each word in the corpus with a unique integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside on streaming\n",
    "\n",
    "To avoid download in RAM: **Corpus Streaming**  One Document at a Time\n",
    "\n",
    "Gensim only requires that a corpus must be able to return one document vector at a time.\n",
    "\n",
    "The full power of Gensim comes from the fact that a corpus doesnt have to be a list, or a NumPy array, or a Pandas dataframe, or whatever. Gensim accepts any object that, when iterated over, successively yields documents.\n",
    "\n",
    "you can mold the __iter__ function to fit your input format, whatever it is. Walking directories, parsing XML, accessing the network Just parse your input to retrieve a clean list of tokens in each document, then convert the tokens via a dictionary to their ids and yield the resulting sparse vector inside __iter__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open  # for transparently opening remote files\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('https://radimrehurek.com/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "            \n",
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to construct the dictionary without loading all texts into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:33:36,634 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-06-02 12:33:36,635 : INFO : built Dictionary<42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...> from 9 documents (total 69 corpus positions)\n",
      "2023-06-02 12:33:36,636 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...> from 9 documents (total 69 corpus positions)\", 'datetime': '2023-06-02T12:33:36.636066', 'gensim': '4.3.1', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('https://radimrehurek.com/mycorpus.txt'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [\n",
    "    dictionary.token2id[stopword]\n",
    "    for stopword in stoplist\n",
    "    if stopword in dictionary.token2id\n",
    "    ]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to tutorials:\n",
    "\n",
    "`doc2bow` takes the IDs from the dictionary and transforms any document it is fed based on them. \n",
    "\n",
    "1. We can apply a new document to an existing dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first entry in the tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token. interaction did not occur in the original corpus and so it was not included in the vectorization. \n",
    "\n",
    "2. We can also apply it to the entire corpus. This converts it to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "These transform one document representation to another.\n",
    "\n",
    "A simple example is `tf-idf`:\n",
    "\n",
    "(short for term frequencyinverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:35:34,728 : INFO : collecting document frequencies\n",
      "2023-06-02 12:35:34,731 : INFO : PROGRESS: processing document #0\n",
      "2023-06-02 12:35:34,732 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 9 documents and 12 features (28 matrix non-zeros)', 'datetime': '2023-06-02T12:35:34.732126', 'gensim': '4.3.1', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'initialize'}\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# initialise and train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "# use the model to transform a new string \n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf weighting. Note that the ID corresponding to system (which occurred 4 times in the original corpus) has been weighted lower than the ID corresponding to minors (which only occurred twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "# us the model to transform any other bow\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that a model, like `tfidf[bow_corpus]`, can only be iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:42:00,441 : INFO : using serial LSI version on this node\n",
      "2023-06-02 12:42:00,442 : INFO : updating model with new documents\n",
      "2023-06-02 12:42:00,443 : INFO : preparing a new chunk of documents\n",
      "2023-06-02 12:42:00,444 : INFO : using 100 extra samples and 2 power iterations\n",
      "2023-06-02 12:42:00,444 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2023-06-02 12:42:00,446 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2023-06-02 12:42:00,451 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2023-06-02 12:42:00,454 : INFO : computing the final decomposition\n",
      "2023-06-02 12:42:00,455 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2023-06-02 12:42:00,457 : INFO : processed documents up to #9\n",
      "2023-06-02 12:42:00,458 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2023-06-02 12:42:00,459 : INFO : topic #1(1.476): 0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"\n",
      "2023-06-02 12:42:00,460 : INFO : LsiModel lifecycle event {'msg': 'trained LsiModel<num_terms=12, num_topics=2, decay=1.0, chunksize=20000> in 0.02s', 'datetime': '2023-06-02T12:42:00.460371', 'gensim': '4.3.1', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `corpus_lsi` applies the LSI transformation to the TF-IDF representation. The original corpus is \"double-wrapped\" by these two transformations (TF-IDF and LSI), one applied after the other. \n",
    "\n",
    "This is a common practice in text processing and Natural Language Processing (NLP) to extract more meaningful and condensed information from the original text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:46:04,099 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2023-06-02 12:46:04,101 : INFO : topic #1(1.476): 0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090422), (1, 0.5200703306361851)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.19667592859142566), (1, 0.7609563167700046)] A survey of user opinion of computer system response time\n",
      "[(0, 0.08992639972446491), (1, 0.7241860626752508)] The EPS user interface management system\n",
      "[(0, 0.07585847652178185), (1, 0.6320551586003428)] System and human system engineering testing of EPS\n",
      "[(0, 0.10150299184980142), (1, 0.5737308483002954)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378311), (1, -0.16115180214025854)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119835), (1, -0.16758906864659506)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818582), (1, -0.14086553628719128)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569288), (1, 0.05392907566389275)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "for doc, as_text in zip(corpus_lsi, text_corpus):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while `tfidf` keeps the number of dimensions intact but only weights towards rarity, `lsi` reduces dimensions to whatever number of topics you may want to extract."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:37:47,376 : INFO : creating sparse index\n",
      "2023-06-02 12:37:47,377 : INFO : creating sparse matrix from corpus\n",
      "2023-06-02 12:37:47,377 : INFO : PROGRESS: at document #0\n",
      "2023-06-02 12:37:47,379 : INFO : created <9x12 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 28 stored elements in Compressed Sparse Row format>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)\n",
    "\n",
    "# another string to check similarity\n",
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the class similarities.MatrixSimilarity is only appropriate when the whole set of vectors fits into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 12:50:00,945 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2023-06-02 12:50:00,947 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi_model[bow_corpus])  # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.07910475117444915), (1, 0.5732835243079404)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi_model[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999408), (1, 0.9946708), (2, 0.9999428), (3, 0.999879), (4, 0.99935204), (5, -0.08804217), (6, -0.0515742), (7, -0.023664713), (8, 0.1938726)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "\n",
    "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9999428),\n",
       " (0, 0.9999408),\n",
       " (3, 0.999879),\n",
       " (4, 0.99935204),\n",
       " (1, 0.9946708),\n",
       " (8, 0.1938726),\n",
       " (7, -0.023664713),\n",
       " (6, -0.0515742),\n",
       " (5, -0.08804217)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999428 The EPS user interface management system\n",
      "0.9999408 Human machine interface for lab abc computer applications\n",
      "0.999879 System and human system engineering testing of EPS\n",
      "0.99935204 Relation of user perceived response time to error measurement\n",
      "0.9946708 A survey of user opinion of computer system response time\n",
      "0.1938726 Graph minors A survey\n",
      "-0.023664713 Graph minors IV Widths of trees and well quasi ordering\n",
      "-0.0515742 The intersection graph of paths in trees\n",
      "-0.08804217 The generation of random binary unordered trees\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(sims, key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims:\n",
    "    print(doc_score, text_corpus[doc_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "- https://spacy.io/usage/embeddings-transformers\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
