{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation using NumPy\n",
    "\n",
    "- Sequence input, Char-level, Batch training, Python 3\n",
    "\n",
    "- You can train RNNs using different vector representations. (Try \"glove\" or \"word2vec\" to train the sequence of words instead of the \"One-hot-vector\")\n",
    "\n",
    "- This is a slightly modified version of the original code(Karpathy).\n",
    "\n",
    "URL\n",
    "\n",
    "-> https://github.com/JY-Yoon\n",
    "\n",
    "-> Original code(Karpathy) : https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  56\n",
      "txt_data_size :  2226\n"
     ]
    }
   ],
   "source": [
    "# load text data\n",
    "\n",
    "# txt_data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz \" # input data\n",
    "txt_data = open('Quotes.txt', encoding='cp1252').read() # test external files\n",
    "\n",
    "chars = list(set(txt_data)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(txt_data)\n",
    "\n",
    "print(\"unique characters : \", num_chars) # You can see the number of unique characters in your input data.\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': 0, ':': 1, 'L': 2, '…': 3, 'u': 4, 'r': 5, 'w': 6, 'p': 7, 'j': 8, 'P': 9, 'm': 10, ',': 11, ' ': 12, 'v': 13, 'M': 14, 'A': 15, 'q': 16, 'o': 17, 'K': 18, 's': 19, 'z': 20, 'y': 21, 'I': 22, ')': 23, 'T': 24, 'D': 25, 'd': 26, '(': 27, 'i': 28, 'S': 29, '?': 30, 'b': 31, 'C': 32, 'x': 33, 'Q': 34, 'B': 35, 'N': 36, 'f': 37, '.': 38, 'R': 39, 'O': 40, 'a': 41, 'k': 42, 'h': 43, 'l': 44, '\\n': 45, '’': 46, 'c': 47, 'F': 48, 'Y': 49, 'e': 50, 'n': 51, 'J': 52, 'W': 53, 't': 54, 'G': 55}\n",
      "----------------------------------------------------\n",
      "{0: 'g', 1: ':', 2: 'L', 3: '…', 4: 'u', 5: 'r', 6: 'w', 7: 'p', 8: 'j', 9: 'P', 10: 'm', 11: ',', 12: ' ', 13: 'v', 14: 'M', 15: 'A', 16: 'q', 17: 'o', 18: 'K', 19: 's', 20: 'z', 21: 'y', 22: 'I', 23: ')', 24: 'T', 25: 'D', 26: 'd', 27: '(', 28: 'i', 29: 'S', 30: '?', 31: 'b', 32: 'C', 33: 'x', 34: 'Q', 35: 'B', 36: 'N', 37: 'f', 38: '.', 39: 'R', 40: 'O', 41: 'a', 42: 'k', 43: 'h', 44: 'l', 45: '\\n', 46: '’', 47: 'c', 48: 'F', 49: 'Y', 50: 'e', 51: 'n', 52: 'J', 53: 'W', 54: 't', 55: 'G'}\n",
      "----------------------------------------------------\n",
      "[34, 4, 17, 54, 50, 19, 12, 9, 5, 50, 13, 28, 50, 6, 12, 35, 41, 42, 41, 51, 41, 44, 12, 27, 14, 28, 51, 28, 19, 54, 50, 5, 12, 17, 37, 12, 24, 17, 4, 5, 28, 19, 10, 23, 45, 45, 22, 51, 54, 5, 17, 12, 16, 4, 17, 54, 50, 19, 1, 45, 45, 45, 53, 43, 41, 54, 50, 13, 50, 5, 12, 43, 41, 7, 7, 50, 51, 26, 12, 28, 51, 12, 54, 43, 50, 12, 7, 41, 19, 54, 11, 12, 6, 50, 12, 54, 41, 42, 50, 12, 28, 54, 12, 41, 51, 26, 12, 6, 50, 12, 10, 41, 42, 50, 12, 19, 17, 10, 50, 54, 43, 28, 51, 0, 12, 17, 4, 54, 12, 17, 37, 12, 28, 54, 12, 41, 51, 26, 12, 10, 41, 42, 50, 12, 28, 54, 12, 8, 17, 21, 12, 41, 12, 19, 54, 28, 44, 38, 12, 27, 35, 50, 5, 51, 41, 5, 26, 11, 12, 24, 41, 10, 31, 17, 17, 12, 35, 41, 10, 31, 17, 17, 23, 45, 45, 25, 17, 51, 46, 54, 12, 47, 41, 5, 50, 12, 6, 43, 41, 54, 12, 44, 28, 37, 50, 12, 54, 43, 5, 17, 6, 19, 12, 41, 54, 12, 4, 19, 11, 12, 6, 50, 12, 41, 5, 50, 12, 51, 17, 54, 12, 44, 50, 54, 12, 28, 54, 12, 19, 54, 50, 41, 44, 12, 17, 4, 5, 12, 8, 17, 21, 12, 27, 39, 17, 31, 28, 51, 12, 48, 17, 19, 54, 50, 5, 23, 12, 45, 45, 40, 4, 54, 12, 17, 37, 12, 41, 12, 10, 4, 26, 26, 21, 12, 7, 17, 51, 26, 11, 12, 54, 50, 51, 12, 54, 43, 17, 4, 19, 41, 51, 26, 12, 37, 44, 17, 6, 50, 5, 19, 12, 31, 44, 17, 17, 10, 11, 12, 28, 54, 46, 19, 12, 54, 5, 41, 51, 19, 37, 17, 5, 10, 50, 26, 12, 28, 51, 54, 17, 12, 10, 50, 44, 17, 26, 21, 11, 12, 19, 6, 50, 50, 54, 50, 5, 12, 41, 51, 26, 12, 19, 6, 50, 50, 54, 50, 5, 12, 27, 25, 41, 13, 28, 26, 12, 39, 4, 26, 26, 50, 5, 23, 45, 45, 24, 43, 50, 12, 7, 5, 17, 31, 44, 50, 10, 12, 6, 28, 54, 43, 12, 21, 17, 4, 51, 0, 50, 5, 12, 7, 50, 17, 7, 44, 50, 12, 51, 17, 6, 12, 28, 19, 12, 54, 43, 41, 54, 12, 28, 54, 46, 19, 12, 51, 17, 54, 12, 54, 43, 50, 28, 5, 12, 37, 41, 4, 44, 54, 38, 12, 22, 54, 46, 19, 12, 17, 4, 5, 12, 37, 41, 4, 44, 54, 38, 12, 53, 50, 12, 51, 50, 13, 50, 5, 12, 54, 50, 41, 47, 43, 50, 26, 12, 54, 43, 50, 10, 38, 12, 27, 39, 17, 31, 28, 51, 12, 48, 17, 19, 54, 50, 5, 23, 12, 45, 45, 35, 41, 31, 21, 12, 32, 5, 28, 31, 45, 45, 24, 43, 50, 12, 47, 5, 50, 41, 54, 28, 17, 51, 12, 17, 37, 12, 29, 17, 47, 41, 12, 6, 41, 19, 12, 54, 43, 50, 12, 28, 51, 13, 50, 51, 54, 28, 17, 51, 12, 17, 37, 12, 2, 17, 5, 26, 12, 29, 43, 17, 5, 54, 21, 38, 12, 24, 43, 50, 12, 28, 26, 50, 41, 12, 6, 41, 19, 12, 54, 17, 12, 4, 19, 50, 12, 54, 43, 50, 12, 5, 43, 21, 54, 43, 10, 19, 12, 17, 37, 12, 54, 43, 50, 12, 50, 41, 19, 54, 12, 28, 51, 26, 28, 41, 51, 19, 12, 41, 51, 26, 12, 54, 43, 50, 12, 15, 37, 5, 28, 47, 41, 51, 12, 19, 44, 41, 13, 50, 19, 38, 12, 27, 35, 4, 51, 8, 28, 23, 45, 45, 35, 5, 28, 51, 0, 12, 54, 43, 50, 12, 54, 6, 17, 12, 47, 4, 44, 54, 4, 5, 50, 19, 12, 54, 17, 0, 50, 54, 43, 50, 5, 12, 27, 55, 5, 41, 51, 26, 19, 17, 51, 12, 2, 17, 5, 26, 12, 29, 43, 17, 5, 54, 21, 23, 45, 45, 2, 50, 54, 46, 19, 12, 37, 28, 51, 26, 12, 17, 4, 5, 19, 50, 44, 13, 50, 19, 12, 27, 52, 41, 44, 41, 44, 4, 26, 28, 51, 12, 18, 43, 41, 51, 23, 45, 45, 47, 45, 29, 17, 47, 41, 12, 45, 45, 22, 54, 12, 47, 17, 10, 31, 28, 51, 50, 19, 12, 4, 19, 38, 12, 29, 17, 4, 44, 12, 17, 37, 12, 24, 5, 28, 51, 28, 26, 41, 26, 12, 41, 51, 26, 12, 24, 17, 31, 41, 0, 17, 38, 12, 9, 50, 17, 7, 44, 50, 12, 54, 43, 17, 4, 0, 43, 54, 12, 28, 54, 12, 6, 41, 19, 12, 15, 10, 50, 5, 28, 47, 41, 51, 12, 29, 17, 4, 44, 38, 12, 36, 17, 51, 12, 17, 37, 12, 54, 43, 41, 54, 38, 12, 22, 54, 46, 19, 12, 54, 43, 50, 12, 19, 17, 4, 44, 12, 31, 50, 47, 41, 4, 19, 50, 12, 6, 50, 12, 41, 5, 50, 12, 24, 5, 28, 51, 28, 26, 41, 26, 12, 41, 51, 26, 12, 24, 17, 31, 41, 0, 17, 11, 12, 54, 43, 50, 12, 7, 50, 17, 7, 44, 50, 11, 12, 54, 43, 41, 54, 12, 28, 19, 12, 6, 43, 17, 12, 6, 50, 12, 41, 5, 50, 38, 12, 27, 55, 5, 41, 51, 26, 19, 17, 51, 23, 45, 45, 29, 17, 51, 0, 19, 12, 54, 43, 41, 54, 12, 10, 41, 42, 50, 12, 21, 17, 4, 12, 26, 41, 51, 47, 50, 11, 12, 19, 17, 51, 0, 19, 12, 54, 43, 41, 54, 12, 10, 41, 42, 50, 12, 21, 17, 4, 12, 19, 28, 51, 0, 11, 12, 19, 17, 51, 0, 19, 12, 54, 43, 41, 54, 12, 10, 41, 42, 50, 12, 21, 17, 4, 12, 43, 41, 7, 7, 21, 38, 12, 22, 54, 12, 28, 19, 12, 44, 28, 42, 50, 12, 41, 12, 13, 50, 51, 17, 10, 38, 12, 27, 25, 50, 19, 54, 5, 41, 12, 55, 41, 5, 47, 28, 41, 23, 45, 45, 22, 54, 46, 19, 12, 19, 54, 5, 41, 28, 0, 43, 54, 12, 50, 51, 50, 5, 0, 21, 11, 12, 19, 54, 5, 41, 28, 0, 43, 54, 12, 44, 17, 13, 50, 51, 12, 28, 54, 46, 19, 12, 54, 5, 41, 28, 0, 43, 54, 12, 13, 28, 31, 50, 19, 11, 12, 28, 54, 46, 19, 12, 7, 17, 19, 28, 54, 28, 13, 28, 54, 21, 38, 12, 22, 54, 46, 19, 12, 41, 12, 44, 17, 13, 50, 12, 0, 50, 51, 5, 50, 38, 12, 27, 25, 8, 12, 9, 4, 37, 37, 21, 23, 45, 45, 24, 43, 28, 19, 12, 28, 19, 12, 54, 43, 50, 12, 7, 17, 6, 50, 5, 12, 17, 37, 12, 10, 4, 19, 28, 47, 12, 41, 54, 12, 28, 54, 46, 19, 12, 31, 50, 19, 54, 38, 12, 45, 45, 22, 54, 46, 19, 12, 41, 12, 43, 41, 7, 7, 21, 12, 5, 50, 13, 17, 44, 4, 54, 28, 17, 51, 11, 12, 31, 4, 54, 12, 41, 12, 5, 50, 13, 17, 44, 4, 54, 28, 17, 51, 12, 4, 51, 44, 50, 19, 19, 12, 27, 18, 50, 19, 11, 12, 19, 47, 43, 5, 41, 7, 7, 50, 51, 30, 23, 45, 45, 53, 50, 12, 47, 41, 51, 12, 31, 50, 12, 41, 51, 12, 50, 33, 41, 10, 7, 44, 50, 12, 37, 17, 5, 12, 54, 43, 50, 12, 5, 50, 19, 54, 12, 17, 37, 12, 54, 43, 50, 12, 6, 17, 5, 44, 26, 38, 12, 24, 43, 50, 12, 6, 17, 5, 44, 26, 12, 28, 19, 12, 31, 50, 47, 17, 10, 28, 51, 0, 12, 10, 17, 5, 50, 12, 41, 51, 26, 12, 10, 17, 5, 50, 12, 6, 43, 41, 54, 12, 24, 5, 28, 51, 28, 26, 41, 26, 12, 41, 44, 5, 50, 41, 26, 21, 12, 28, 19, 38, 12, 22, 54, 46, 19, 12, 50, 28, 54, 43, 50, 5, 12, 21, 17, 4, 12, 50, 10, 31, 5, 41, 47, 50, 12, 28, 54, 12, 17, 5, 12, 21, 17, 4, 46, 5, 50, 12, 0, 17, 28, 51, 0, 12, 54, 17, 12, 31, 50, 12, 37, 28, 0, 43, 54, 28, 51, 0, 12, 28, 54, 12, 26, 17, 6, 51, 12, 50, 13, 50, 5, 21, 12, 26, 41, 21, 12, 37, 17, 5, 12, 21, 17, 4, 5, 12, 6, 43, 17, 44, 50, 12, 44, 28, 13, 50, 38, 12, 53, 43, 41, 54, 12, 41, 5, 50, 12, 21, 17, 4, 12, 37, 28, 0, 43, 54, 28, 51, 0, 30, 12, 22, 54, 12, 26, 17, 50, 19, 51, 46, 54, 12, 10, 41, 42, 50, 12, 19, 50, 51, 19, 50, 38, 12, 27, 15, 51, 21, 41, 12, 32, 43, 50, 50, 23, 45, 45, 40, 4, 54, 5, 17, 45, 45, 22, 19, 12, 54, 43, 50, 5, 50, 12, 19, 17, 10, 50, 54, 43, 28, 51, 0, 12, 54, 43, 41, 54, 12, 22, 12, 37, 17, 5, 0, 17, 54, 12, 54, 17, 12, 41, 19, 42, 12, 21, 17, 4, 12, 0, 4, 21, 19, 12, 54, 43, 41, 54, 12, 54, 43, 50, 12, 6, 17, 5, 44, 26, 12, 51, 50, 50, 26, 19, 12, 54, 17, 12, 42, 51, 17, 6, 12, 41, 31, 17, 4, 54, 30, 12, 27, 14, 5, 38, 12, 39, 41, 42, 42, 41, 23, 45, 45, 53, 50, 44, 44, 11, 12, 22, 12, 54, 43, 28, 51, 42, 38, 12, 22, 12, 10, 50, 41, 51, 3, 12, 22, 12, 6, 17, 4, 44, 26, 51, 46, 54, 12, 19, 41, 21, 12, 10, 50, 38, 12, 49, 17, 4, 12, 42, 51, 17, 6, 38, 12, 24, 43, 50, 12, 21, 17, 4, 51, 0, 50, 5, 12, 17, 51, 50, 19, 38, 12, 53, 50, 12, 51, 50, 50, 26, 12, 54, 17, 12, 31, 50, 12, 17, 4, 54, 12, 54, 43, 50, 5, 50, 38, 12, 29, 17, 12, 22, 12, 41, 10, 12, 5, 50, 41, 44, 44, 21, 12, 43, 17, 7, 28, 51, 0, 12, 54, 43, 41, 54, 12, 7, 50, 17, 7, 44, 50, 12, 5, 50, 41, 44, 44, 21, 12, 4, 51, 26, 50, 5, 19, 54, 41, 51, 26, 12, 6, 43, 41, 54, 12, 6, 50, 12, 41, 5, 50, 12, 41, 31, 17, 4, 54, 11, 12, 54, 17, 12, 0, 50, 54, 12, 54, 43, 50, 19, 50, 12, 21, 17, 4, 51, 0, 12, 7, 50, 17, 7, 44, 50, 11, 12, 54, 17, 12, 0, 50, 54, 12, 54, 43, 50, 10, 12, 17, 4, 54, 12, 54, 43, 50, 5, 50, 12, 41, 51, 26, 12, 44, 50, 54, 12, 54, 43, 50, 10, 12, 19, 50, 50, 12, 54, 43, 50, 12, 43, 17, 5, 28, 20, 17, 51, 11, 12, 44, 50, 54, 12, 54, 43, 50, 10, 12, 19, 50, 50, 12, 6, 43, 41, 54, 12, 54, 43, 50, 21, 12, 41, 5, 50, 12, 10, 28, 19, 19, 28, 51, 0, 12, 6, 43, 41, 54, 12, 54, 43, 50, 21, 12, 47, 17, 4, 44, 26, 12, 31, 50, 38, 12, 35, 4, 54, 12, 6, 50, 12, 26, 50, 37, 28, 51, 28, 54, 50, 44, 21, 12, 47, 41, 51, 46, 54, 12, 26, 17, 12, 28, 54, 12, 41, 44, 17, 51, 50, 38, 12, 53, 50, 12, 51, 50, 50, 26, 12, 54, 17, 12, 0, 50, 54, 12, 17, 4, 5, 12, 13, 17, 28, 47, 50, 19, 12, 17, 4, 54, 12, 54, 43, 50, 5, 50, 12, 19, 17, 12, 54, 43, 41, 54, 12, 28, 54, 46, 19, 12, 43, 50, 41, 5, 26, 38, 12, 24, 43, 41, 54, 12, 6, 41, 19, 12, 19, 17, 10, 50, 54, 43, 28, 51, 0, 12, 22, 12, 51, 50, 50, 26, 50, 26, 12, 54, 43, 50, 12, 6, 17, 5, 44, 26, 12, 54, 17, 12, 43, 50, 41, 5, 38, 12, 53, 50, 12, 47, 41, 51, 12, 19, 43, 17, 6, 12, 54, 43, 50, 12, 6, 17, 5, 44, 26, 12, 54, 43, 41, 54, 12, 17, 4, 5, 12, 10, 4, 19, 28, 47, 12, 47, 41, 51, 12, 31, 5, 28, 51, 0, 12, 4, 19, 12, 54, 17, 12, 6, 43, 50, 5, 50, 12, 21, 17, 4, 12, 41, 5, 50, 38, 12, 40, 5, 12, 54, 17, 12, 31, 5, 28, 51, 0, 12, 21, 17, 4, 12, 54, 17, 12, 6, 50, 5, 50, 12, 6, 50, 12, 41, 5, 50, 38, 12, 27, 2, 41, 13, 50, 51, 54, 28, 44, 44, 50, 12, 39, 43, 21, 54, 43, 10, 12, 29, 50, 47, 54, 28, 17, 51, 23, 45, 45, 45, 45, 45]\n",
      "----------------------------------------------------\n",
      "data length :  2226\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 500\n",
    "sequence_length = 10\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 100  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    " \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 15.482792\n",
      "iter 100, loss: 1.562318\n",
      "iter 200, loss: 1.491933\n",
      "iter 300, loss: 1.359891\n",
      "iter 400, loss: 1.345444\n"
     ]
    }
   ],
   "source": [
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " it tobk pelle toul the pon the wos we ref Louneonrin ande the wom ofkk aneestet I sines tother. Tabicere veneddkd you y nhe wsse, the wou pand on unled. Tha, (Rus. \n",
      "\n",
      "Duty ar cant. (an wune be teve ite  it. Sou pvithat se the pestet I ’t’o hat of Lorg toub ge the We am o\n",
      "\n",
      "\n",
      "BOusw aude the ing be tht t \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('t',300) # (char, len of output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
