{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "from langid import set_languages, classify\n",
    "set_languages(['nl', 'en'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.io import push_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "df_grote_word_doc = pd.read_json(r'C:\\Users\\johan\\Documents\\GitHub\\sandbox\\NLP\\data\\NLP for grote word-filev2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('dutch') + stopwords.words('english') \n",
    "\n",
    "more_words = ['vooral', 'gaan', 'één', 'value', 'part', 'use', 'blijven', 'waarbij', 'stuk', 'wanneer', 'much', 'kennen', 'always', 'tegelijk', 'however', 'geven', 'nooit', 'weg', 'vaak', 'soort', 'wellicht', 'leggen', 'steken', 'leven', 'zoal,', 'waar', 'allemaal', 'net', 'eigen', 'stefaf', 'vallen', 'zaak', 'feit', 'waaruit', 'zelfs', 'year', 'echter', 'zien', 'come', 'willen', 'spreken', 'straf', 'lijken', 'staan', 'even', 'hoog', 'pas', 'liggen', 'waarom', 'helemaal', 'situatie', 'waaraan', 'zitten', 'take', 'waarin', 'often', 'wel', 'maken', 'nieuw', 'waarop', 'plots', 'say', 'goed', 'way', 'terug', 'mogelijk', 'many', 'daarom', 'omwille', 'leren', 'nemen', 'kijken', 'waarde', 'gebruiken', 'iphone', 'eerder', 'weer', 'zoeken', 'dienen', 'alleen', 'houden', 'see', 'well', 'good', 'deel', 'find', 'misschien', 'make', 'vinden', 'also', 'manier', 'natuurlijk', 'laten', 'louter', 'komen', 'stellen', 'ergens', 'live', 'ver', 'daarentegen', 'facebook', 'steeds', 'time', 'need', 'enkel', 'new', 'nodig', 'vormen', 'halen', 'duidelijk', 'zeggen', 'camera', 'krijgen', 'brengen', 'eigenlijk', 'proberen', 'gewoon', 'heel', 'zeer', 'telkens', 'look', 'eerst', 'belangrijk', 'nochtans', 'waarmee', 'lang', 'zeker']\n",
    "more_words = more_words\n",
    "stop_words = list(set(stop_words + more_words))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_preprocess(stop_words):\n",
    "    for stop in stop_words:\n",
    "        sent = gensim.utils.simple_preprocess(str(stop.strip()), deacc=True)\n",
    "        if sent:\n",
    "            yield(sent)\n",
    "            \n",
    "stop_words += ['zoals']\n",
    "stop_words = [' '.join(w) for w in list(stopwords_preprocess(stop_words))]\n",
    "\n",
    "'en' in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_grote_word_doc.text.values.tolist()\n",
    "\n",
    "def sent_to_words(texts):\n",
    "    for text in texts:\n",
    "        sent = gensim.utils.simple_preprocess(str(text), deacc=True, min_len=3) \n",
    "        yield ' '.join(sent)\n",
    "\n",
    "# Convert to list\n",
    "data_words = list(sent_to_words(data))\n",
    "len(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vector approach needs minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl only\n",
    "def sent_to_words_nl(texts):\n",
    "    for text in texts:\n",
    "        lang, _ = classify(text)\n",
    "        if lang == 'nl':\n",
    "            yield text\n",
    "            \n",
    "data = df_grote_word_doc.text.values.tolist()\n",
    "\n",
    "data_words_nl = list(sent_to_words_nl(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download nl_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('nl_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_list = []\n",
    "for words in data_words_nl:\n",
    "    spacy_list.append(nlp(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = [doc.vector for doc in spacy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [doc.text for doc in spacy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 35\n",
    "\n",
    "model = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=300, n_init='auto')\n",
    "\n",
    "model.fit(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "```\n",
    "\n",
    "the direct interpretation of these feature indices might not be straightforward for doc_vectors\n",
    "\n",
    "Same for feature names from tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne_model = TSNE(n_components=2, \n",
    "                  random_state=20, \n",
    "                  learning_rate='auto', # 10 is small and 200 is large (but fast)\n",
    "                  angle=.99, \n",
    "                  # init=\"random\",\n",
    "                  init='pca',\n",
    "                  perplexity=40, \n",
    "                  early_exaggeration=70)\n",
    "\n",
    "low_dim_data = tsne_model.fit_transform(np.array(doc_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, HoverTool, TapTool, CustomJS, Div\n",
    "from bokeh.layouts import column\n",
    "\n",
    "output_notebook()\n",
    "mycolors = np.array(list(mcolors.TABLEAU_COLORS.values()) + \\\n",
    "         [mcolors.to_hex(c) for c in plt.cm.Pastel1.colors] + \\\n",
    "         [mcolors.to_hex(c) for c in plt.cm.Set1.colors])\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=low_dim_data[:, 0],\n",
    "    y=low_dim_data[:, 1],\n",
    "    colors=mycolors[labels % len(mycolors)],\n",
    "    onderwerp=[s[:250] for s in sentences],\n",
    "    description=sentences,\n",
    "    # keywords = keyword_list\n",
    "    )\n",
    "                          )\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"onderwerp\", \"@onderwerp\"),\n",
    "                #   (\"keywords\", \"@keywords\")\n",
    "                  ]\n",
    "\n",
    "description_div = Div(text=\"\", width=1200, height=400)\n",
    "\n",
    "callback = CustomJS(args=dict(source=source, div=description_div), code=\"\"\"\n",
    "    const indices = source.selected.indices;\n",
    "    if (indices.length == 0)\n",
    "        return;\n",
    "    const desc = source.data['description'][indices[0]];\n",
    "    div.text = desc;\n",
    "\"\"\")\n",
    "tap_tool = TapTool(callback=callback)\n",
    "\n",
    "plot = figure(tools=\"wheel_zoom, reset\", title=f\"t-SNE Clustering of {true_k} KMeans Topics\", width=1200, height=800)\n",
    "plot.add_tools(hover)\n",
    "plot.add_tools(tap_tool)\n",
    "\n",
    "plot.scatter('x', 'y', source=source, color='colors', size=8)\n",
    "\n",
    "layout = column(plot, description_div)\n",
    "\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity scores\n",
    "\n",
    "Experimenting with similarity in spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"dom\").similarity(nlp(\"stupid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data = [nlp(d) for d in data if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "similarities = []\n",
    "for doc1, doc2 in combinations(nlp_data, 2):\n",
    "    similarity_score = doc1.similarity(doc2)\n",
    "    similarities.append((doc1, doc2, similarity_score))\n",
    "\n",
    "# Sort the similarities list in descending order\n",
    "similarities.sort(key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_v2 = [(sim, sam, score) for sim, sam, score in similarities if score < .98][:50] # this gets big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_v2[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(similarities_v2) # 1 gig = 1 * 10^9 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johannes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
