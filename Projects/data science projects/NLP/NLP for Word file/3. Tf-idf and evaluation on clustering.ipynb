{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "from langid import set_languages, classify\n",
    "set_languages(['nl', 'en'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.io import push_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "df_grote_word_doc = pd.read_json(r'C:\\Users\\johan\\Documents\\GitHub\\sandbox\\NLP\\data\\NLP for grote word-filev2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('dutch') + stopwords.words('english') \n",
    "\n",
    "more_words = ['vooral', 'gaan', 'één', 'value', 'part', 'use', 'blijven', 'waarbij', 'stuk', 'wanneer', 'much', 'kennen', 'always', 'tegelijk', 'however', 'geven', 'nooit', 'weg', 'vaak', 'soort', 'wellicht', 'leggen', 'steken', 'leven', 'zoal,', 'waar', 'allemaal', 'net', 'eigen', 'stefaf', 'vallen', 'zaak', 'feit', 'waaruit', 'zelfs', 'year', 'echter', 'zien', 'come', 'willen', 'spreken', 'straf', 'lijken', 'staan', 'even', 'hoog', 'pas', 'liggen', 'waarom', 'helemaal', 'situatie', 'waaraan', 'zitten', 'take', 'waarin', 'often', 'wel', 'maken', 'nieuw', 'waarop', 'plots', 'say', 'goed', 'way', 'terug', 'mogelijk', 'many', 'daarom', 'omwille', 'leren', 'nemen', 'kijken', 'waarde', 'gebruiken', 'iphone', 'eerder', 'weer', 'zoeken', 'dienen', 'alleen', 'houden', 'see', 'well', 'good', 'deel', 'find', 'misschien', 'make', 'vinden', 'also', 'manier', 'natuurlijk', 'laten', 'louter', 'komen', 'stellen', 'ergens', 'live', 'ver', 'daarentegen', 'facebook', 'steeds', 'time', 'need', 'enkel', 'new', 'nodig', 'vormen', 'halen', 'duidelijk', 'zeggen', 'camera', 'krijgen', 'brengen', 'eigenlijk', 'proberen', 'gewoon', 'heel', 'zeer', 'telkens', 'look', 'eerst', 'belangrijk', 'nochtans', 'waarmee', 'lang', 'zeker']\n",
    "more_words = more_words\n",
    "stop_words = list(set(stop_words + more_words))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_preprocess(stop_words):\n",
    "    for stop in stop_words:\n",
    "        sent = gensim.utils.simple_preprocess(str(stop.strip()), deacc=True)\n",
    "        if sent:\n",
    "            yield(sent)\n",
    "            \n",
    "stop_words += ['zoals']\n",
    "stop_words = [' '.join(w) for w in list(stopwords_preprocess(stop_words))]\n",
    "\n",
    "'en' in stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_grote_word_doc.text.values.tolist()\n",
    "\n",
    "def sent_to_words(texts):\n",
    "    for text in texts:\n",
    "        sent = gensim.utils.simple_preprocess(str(text), deacc=True, min_len=3) \n",
    "        yield' '.join(sent)\n",
    "\n",
    "# Convert to list\n",
    "data_words = list(sent_to_words(data))\n",
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "sentences = df_grote_word_doc.text.values.tolist()\n",
    "\n",
    "def sent_to_wordsv2(sentences):\n",
    "    for sent in sentences:\n",
    "        sent_out = []\n",
    "        sent = re.sub(r'\\d+', ' ', sent)\n",
    "        sent = re.sub(r'\\W+', ' ', sent)\n",
    "        for word in sent.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            if word and word not in stop_words:\n",
    "                sent_out.append(word) \n",
    "        yield ' '.join(sent_out)\n",
    "        \n",
    "# clean_word_list = list(sent_to_wordsv2(sentences))\n",
    "\n",
    "len(clean_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "see digital humanities notebook. Based on https://www.youtube.com/watch?v=i74DVqMsRWY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                            max_features=75, # tutorial put 100. when increasing it, there's a whole bunch on unclustered stuff in the middle\n",
    "                            max_df=0.65, #  This ignores terms that appear in more than 60% of the documents.\n",
    "                            min_df=2, #  This ignores terms that appear in fewer than 3 documents to filter out typos or extremely rare words. \n",
    "                            ngram_range = (1,3),\n",
    "                            stop_words = stop_words,\n",
    "                            # sublinear_tf=True\n",
    "                            )\n",
    "\n",
    "vectors = vectorizer.fit_transform(data_words) # \n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "denselist = vectors.todense().tolist()\n",
    "\n",
    "all_keywords = []\n",
    "for description in denselist:\n",
    "    x=0\n",
    "    keywords = []\n",
    "    for word in description:\n",
    "        if word > 0:\n",
    "            keywords.append(feature_names[x])\n",
    "        x+=1\n",
    "    all_keywords.append(keywords)\n",
    "    \n",
    "# print(data_words[0])\n",
    "# print(all_keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 35\n",
    "\n",
    "model = KMeans(n_clusters=true_k, init=\"k-means++\", max_iter=400, n_init=10)\n",
    "\n",
    "model.fit(vectors)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(true_k):\n",
    "    print(f\"Cluster {i}\")\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print('- ', feature_names[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels assigned by KMeans and organise them as a list of the same length of the emails\n",
    "labels = model.labels_\n",
    "keyword_list = []\n",
    "for label in labels:    \n",
    "    keywords_for_label = ', '.join(feature_names[i] for i in order_centroids[label, :5])\n",
    "    keyword_list.append(keywords_for_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`colors = np.array(mycolors)[labels % len(mycolors)] # note that this kind of indexing only works on np.arrays not on lists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=0, random_state=20, angle=.99, init='pca', perplexity=95) #, learning_rate=50, early_exaggeration=35)\n",
    "low_dim_data = tsne_model.fit_transform(np.array(vectors.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, HoverTool, TapTool, CustomJS, Div\n",
    "from bokeh.layouts import column\n",
    "\n",
    "output_notebook()\n",
    "# mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "mycolors = np.array(list(mcolors.TABLEAU_COLORS.values()) + \\\n",
    "         [mcolors.to_hex(c) for c in plt.cm.Pastel1.colors] + \\\n",
    "         [mcolors.to_hex(c) for c in plt.cm.Set1.colors])\n",
    "         \n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=low_dim_data[:, 0],\n",
    "    y=low_dim_data[:, 1],\n",
    "    colors=mycolors[labels % len(mycolors)],\n",
    "    onderwerp=df_grote_word_doc.heading.tolist(),\n",
    "    description=sentences,\n",
    "    keywords = keyword_list))\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"onderwerp\", \"@onderwerp\"),\n",
    "                  (\"keywords\", \"@keywords\")\n",
    "                  ]\n",
    "\n",
    "description_div = Div(text=\"\", width=1200, height=150)\n",
    "\n",
    "callback = CustomJS(args=dict(source=source, div=description_div), code=\"\"\"\n",
    "    const indices = source.selected.indices;\n",
    "    if (indices.length == 0)\n",
    "        return;\n",
    "    const desc = source.data['description'][indices[0]];\n",
    "    div.text = desc;\n",
    "\"\"\")\n",
    "tap_tool = TapTool(callback=callback)\n",
    "\n",
    "plot = figure(tools=\"wheel_zoom, reset\", title=f\"t-SNE Clustering of {true_k} KMeans Topics\", width=1200, height=800)\n",
    "plot.add_tools(hover)\n",
    "plot.add_tools(tap_tool)\n",
    "\n",
    "plot.scatter('x', 'y', source=source, color='colors')\n",
    "\n",
    "layout = column(plot, description_div)\n",
    "\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation methods\n",
    "\n",
    "With tf-idf => KMEANS => TSNE there are 3 models that need parameter tuning.\n",
    "\n",
    "For TSNE I found it important to tweak perplexity and early exaggeration. see https://distill.pub/2016/misread-tsne/ and https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a\n",
    "\n",
    "## K-MEANS\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "\n",
    "- The K-means algorithm aims to choose centroids that minimise the inertia\n",
    "- prone to Curse of Dimensionality > Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "for i in range(2, limit):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1)\n",
    "    kmeans.fit(vectors)\n",
    "    wcss.append(kmeans.inertia_) # Inertia measures the sum of squared distances of samples to their closest cluster center. A lower inertia indicates that the clusters are more compact.\n",
    "    \n",
    "plt.plot(range(2, limit), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see here https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "The silhouette_score is a comparison between its tightness and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil = []\n",
    "for k in range(2, limit):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=1).fit(vectors)\n",
    "    preds = kmeans.predict(vectors)\n",
    "    sil.append(silhouette_score(vectors, preds, metric='euclidean'))\n",
    "\n",
    "plt.plot(range(2, limit), sil)\n",
    "plt.title('Silhouette Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tutorial\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.davies_bouldin_score(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more on DBI here https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index\n",
    "\n",
    "Values closer to zero indicate a better partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "db = []\n",
    "for k in range(2, limit):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=1).fit(vectors)\n",
    "    preds = kmeans.predict(vectors)\n",
    "    db.append(davies_bouldin_score(vectors.toarray(), preds))\n",
    "\n",
    "plt.plot(range(2, limit), db)\n",
    "plt.title('Davies-Bouldin Index')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DBI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johannes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
