{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for `de grote word file`\n",
    "\n",
    "This is the start of a series in which I try to visualise my wordfile in clusters of meaning.\n",
    "\n",
    "This focusses on LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the docx as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "output = []\n",
    "\n",
    "chunk = {}\n",
    "\n",
    "current_heading = None\n",
    "doc = Document(\"D:/OneDrive/_Schrijverij/Boek/die ene grote word-file.docx\")\n",
    "for paragraph in doc.paragraphs:\n",
    "    if paragraph.style.name == 'Heading 1':\n",
    "        # Save previous chunk if any\n",
    "        if chunk:\n",
    "            output.append(chunk)\n",
    "        \n",
    "        # Start a new chunk\n",
    "        current_heading = paragraph.text\n",
    "        chunk = {'heading': current_heading, 'text': \"\"}\n",
    "        \n",
    "    elif current_heading is not None:\n",
    "        if paragraph.text.strip() == '':\n",
    "            if chunk: \n",
    "                output.append(chunk)\n",
    "            chunk = {'heading': current_heading, 'text': \"\"}\n",
    "        else:\n",
    "            chunk['text'] += paragraph.text.strip() + \"\\n\"\n",
    "\n",
    "# Append the last chunk\n",
    "if chunk:\n",
    "    output.append(chunk)\n",
    "\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn it into a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grote_word_doc = pd.DataFrame(output)\n",
    "\n",
    "df_grote_word_doc.to_json('NLP for grote word-file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grote_word_doc = pd.read_json(r'C:\\Users\\johan\\Documents\\GitHub\\sandbox\\NLP\\data\\NLP for grote word-file.json')\n",
    "df_grote_word_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get rid of rows with empty strings in the text column\n",
    "df_grote_word_doc = df_grote_word_doc[df_grote_word_doc['text'] != '']\n",
    "\n",
    "# 22 duplicates\n",
    "df_grote_word_doc = df_grote_word_doc.drop_duplicates(subset='text', keep='last')\n",
    "df_grote_word_doc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grote_word_doc.to_json('NLP for grote word-filev2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grote_word_doc = pd.read_json(r'C:\\Users\\johan\\Documents\\GitHub\\sandbox\\NLP\\data\\NLP for grote word-filev2.json')\n",
    "df_grote_word_doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(texts):\n",
    "    for text in texts:\n",
    "        sent = gensim.utils.simple_preprocess(str(text), deacc=True, min_len=3) \n",
    "        yield(sent)\n",
    "\n",
    "# Convert to list\n",
    "data = df_grote_word_doc.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# data_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('dutch') + stopwords.words('english') \n",
    "more_words = ['vooral', 'gaan', 'één', 'value', 'part', 'use', 'blijven', 'waarbij', 'stuk', 'wanneer', 'much', 'kennen', 'always', 'tegelijk', 'however', 'geven', 'nooit', 'weg', 'vaak', 'soort', 'wellicht', 'leggen', 'steken', 'leven', 'zoal,', 'waar', 'allemaal', 'net', 'eigen', 'stefaf', 'vallen', 'zaak', 'feit', 'waaruit', 'zelfs', 'year', 'echter', 'zien', 'come', 'willen', 'spreken', 'straf', 'lijken', 'staan', 'even', 'hoog', 'pas', 'liggen', 'waarom', 'helemaal', 'situatie', 'waaraan', 'zitten', 'take', 'waarin', 'often', 'wel', 'maken', 'nieuw', 'waarop', 'plots', 'say', 'goed', 'way', 'terug', 'mogelijk', 'many', 'daarom', 'omwille', 'leren', 'nemen', 'kijken', 'waarde', 'gebruiken', 'iphone', 'eerder', 'weer', 'zoeken', 'dienen', 'alleen', 'houden', 'see', 'well', 'good', 'deel', 'find', 'misschien', 'make', 'vinden', 'also', 'manier', 'natuurlijk', 'laten', 'louter', 'komen', 'stellen', 'ergens', 'live', 'ver', 'daarentegen', 'facebook', 'steeds', 'time', 'need', 'enkel', 'new', 'nodig', 'vormen', 'halen', 'duidelijk', 'zeggen', 'camera', 'krijgen', 'brengen', 'eigenlijk', 'proberen', 'gewoon', 'heel', 'zeer', 'telkens', 'look', 'eerst', 'belangrijk', 'nochtans', 'waarmee', 'lang', 'zeker']\n",
    "more_words = list(set(more_words))\n",
    "stop_words = set(stop_words + more_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid import set_languages, classify\n",
    "\n",
    "set_languages(['nl', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check language, lemmatize, remove stopwords\n",
    "def process_lang(mails, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"language detection and Lemmatization\"\"\"\n",
    "    \n",
    "    mails = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in mails]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    nlp_en = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    nlp_nl = spacy.load(\"nl_core_news_sm\", disable=['parser', 'ner'])\n",
    "    \n",
    "    for mail in mails:\n",
    "        mail_text = []\n",
    "        for i in range(0, len(mail) - 2, 3):\n",
    "            term = mail[i] + \" \" + mail[i+1] + \" \" + mail[i+2]\n",
    "            lang, _ = classify(term)\n",
    "            if lang == 'nl':\n",
    "                doc = nlp_nl(term) \n",
    "            else:\n",
    "                doc = nlp_en(term) \n",
    "                            \n",
    "            # print(lang, doc, [token.lemma_ for token in doc])\n",
    "            \n",
    "            mail_text.extend(doc)\n",
    "                        \n",
    "        texts_out.append([token.lemma_ for token in mail_text if token.pos_ in allowed_postags])\n",
    "            \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]  \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUTCH only, lemmatize, remove stopwords\n",
    "def process_lang(mails, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"language detection and Lemmatization\"\"\"\n",
    "    \n",
    "    mails = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in mails]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    # nlp_en = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    nlp_nl = spacy.load(\"nl_core_news_sm\", disable=['parser', 'ner'])\n",
    "    \n",
    "    for mail in mails:\n",
    "        mail_text = []\n",
    "        for i in range(0, len(mail) - 2, 3):\n",
    "            term = mail[i] + \" \" + mail[i+1] + \" \" + mail[i+2]\n",
    "            lang, _ = classify(term)\n",
    "            if lang == 'nl':\n",
    "                doc = nlp_nl(term) \n",
    "            # else:\n",
    "            #     doc = nlp_en(term) \n",
    "            \n",
    "                mail_text.extend(doc)\n",
    "                        \n",
    "        texts_out.append([token.lemma_ for token in mail_text if token.pos_ in allowed_postags])\n",
    "            \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]  \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=30) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=30)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def process_ngrams(texts, stop_words=stop_words):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does ALL the preprocessing (slow)\n",
    "data_words = process_lang(data_words)\n",
    "data_ready = process_ngrams(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dictionary and corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dictionary stores each token (word) that exists in each list in an alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "id2word  = corpora.Dictionary(data_ready)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,j in id2word.most_common(100)] # check for uninformative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus,\n",
    "                            id2word=id2word,\n",
    "                            num_topics=4, \n",
    "                            random_state=75,\n",
    "                            chunksize=350, # 20 is too low and 800 too high\n",
    "                            passes=35,\n",
    "                            iterations=10,\n",
    "                            eta='auto', \n",
    "                            alpha='auto', \n",
    "                            eval_every=5,\n",
    "                            per_word_topics=True\n",
    "                            )\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helped a lot to improve the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_coherence_values(start, limit,step):\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           chunksize=300, \n",
    "                                           )\n",
    "        \n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return coherence_values\n",
    "\n",
    "start=2\n",
    "limit=25\n",
    "step=4\n",
    "coherence_values = compute_coherence_values(start, limit, step)\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "johannes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
