{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf   \n",
    "\n",
    "from sklearn.metrics import mean_squared_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset using the pandas package.\n",
    "\n",
    "Write your own Python implementation of the cross-validation procedure (please do not use existing implementation). Feel free to shuffle the data so that the splits are different each time it is run.\n",
    "\n",
    "Perform cross-validation on the pasture dataset using multiple linear regression (feel free to use sci-kit learn). In other words, train a multiple linear regression learner on the training part and evaluate it using the test part (compute RMSE, feel free to write your own implementation or use sci-kit learn). Repeat this step as the cross-validation iterations progress (make sure the number of splits is specified by the user).\n",
    "\n",
    "Report the mean and standard deviation of the RMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullpath = r\"G:\\My Drive\\_Msc Data Science\\statistics\\pasture-data.csv\"\n",
    "data = pd.read_csv(fullpath, index_col='I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
       "  array([14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
       "         48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,\n",
       "         65, 66])),\n",
       " (array([14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]),\n",
       "  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 28, 29, 30,\n",
       "         31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
       "         48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,\n",
       "         65, 66])),\n",
       " (array([28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]),\n",
       "  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 41, 42, 43, 44, 45, 46,\n",
       "         47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
       "         64, 65, 66])),\n",
       " (array([41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]),\n",
       "  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "         34, 35, 36, 37, 38, 39, 40, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
       "         64, 65, 66])),\n",
       " (array([54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]),\n",
       "  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "         34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "         51, 52, 53]))]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# say, a 5-way split -> \n",
    "def kfold(splits, length):\n",
    "    idxs = range(length)\n",
    "    # np.random.shuffle(idxs)\n",
    "    \n",
    "    idx_bins = np.array_split(idxs, splits)\n",
    "    train_test_tuples = []\n",
    "\n",
    "    for split in range(splits):\n",
    "        test_idx = idx_bins[split]\n",
    "        set_of_rest_of_bins = set([bin for b in idx_bins for bin in b]) - set(test_idx)\n",
    "        train_idx = np.array(list(set_of_rest_of_bins))\n",
    "        train_test_tuples.append((test_idx, train_idx))\n",
    "        \n",
    "    assert len(train_test_tuples) == splits\n",
    "    return train_test_tuples \n",
    "\n",
    "length = data.shape[0]\n",
    "\n",
    "train_test_tuples = kfold(5, length)\n",
    "train_test_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the mean of the RMSE scores is 8.59 and the standard deviation 2.69'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "scores = []\n",
    "for test_idx, train_idx in kfold(5, length):\n",
    "    \n",
    "    # get the split sets\n",
    "    test = data.iloc[test_idx]\n",
    "    train = data.iloc[train_idx]\n",
    "    \n",
    "    # for the test set, get X and y\n",
    "    test_y = test.Y\n",
    "    test_X = test.drop('Y', axis=1)\n",
    "    \n",
    "    # idem for the train set\n",
    "    train_y = train.Y\n",
    "    train_X = train.drop('Y', axis=1)\n",
    "    \n",
    "    # train on the train set\n",
    "    lm.fit(train_X, train_y)\n",
    "    \n",
    "    # predict on test set\n",
    "    yhat = lm.predict(test_X)\n",
    "    \n",
    "    # compute the fold's score\n",
    "    RMSE = np.sqrt(mean_squared_error(test_y, yhat))\n",
    "    scores.append(RMSE)    \n",
    "\n",
    "f\"the mean of the RMSE scores is {np.mean(scores):.2f} and the standard deviation {np.std(scores):.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the mean of the RMSE scores is -8.59 and the standard deviation 2.69'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same with sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lm = LinearRegression()\n",
    "\n",
    "train_data = data.drop('Y', axis=1)\n",
    "train_target = data.Y\n",
    "test_data = data.drop('Y', axis=1)\n",
    "test_target = data.Y\n",
    "\n",
    "cv_score = cross_val_score(lm, train_data, train_target, cv=5, scoring='neg_root_mean_squared_error') \n",
    "# default scoring is 'explained_variance'\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "f\"the mean of the RMSE scores is {np.mean(cv_score):.2f} and the standard deviation {np.std(cv_score):.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared (uncentered):</th>      <td>   0.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   631.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 07 Jun 2024</td> <th>  Prob (F-statistic):</th>          <td>1.80e-47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:33:34</td>     <th>  Log-Likelihood:    </th>          <td> -240.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    67</td>      <th>  AIC:               </th>          <td>   486.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    64</td>      <th>  BIC:               </th>          <td>   492.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th> <td>    0.8349</td> <td>    0.033</td> <td>   25.066</td> <td> 0.000</td> <td>    0.768</td> <td>    0.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X2</th> <td>    0.4648</td> <td>    0.091</td> <td>    5.094</td> <td> 0.000</td> <td>    0.283</td> <td>    0.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X3</th> <td>  -19.5200</td> <td>    8.509</td> <td>   -2.294</td> <td> 0.025</td> <td>  -36.519</td> <td>   -2.521</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.916</td> <th>  Durbin-Watson:     </th> <td>   2.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.019</td> <th>  Jarque-Bera (JB):  </th> <td>   7.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.635</td> <th>  Prob(JB):          </th> <td>  0.0224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.053</td> <th>  Cond. No.          </th> <td>    411.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        Y         & \\textbf{  R-squared (uncentered):}      &     0.967   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.966   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          &     631.7   \\\\\n",
       "\\textbf{Date:}             & Fri, 07 Jun 2024 & \\textbf{  Prob (F-statistic):}          &  1.80e-47   \\\\\n",
       "\\textbf{Time:}             &     15:33:34     & \\textbf{  Log-Likelihood:    }          &   -240.00   \\\\\n",
       "\\textbf{No. Observations:} &          67      & \\textbf{  AIC:               }          &     486.0   \\\\\n",
       "\\textbf{Df Residuals:}     &          64      & \\textbf{  BIC:               }          &     492.6   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{X1} &       0.8349  &        0.033     &    25.066  &         0.000        &        0.768    &        0.901     \\\\\n",
       "\\textbf{X2} &       0.4648  &        0.091     &     5.094  &         0.000        &        0.283    &        0.647     \\\\\n",
       "\\textbf{X3} &     -19.5200  &        8.509     &    -2.294  &         0.025        &      -36.519    &       -2.521     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  7.916 & \\textbf{  Durbin-Watson:     } &    2.288  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.019 & \\textbf{  Jarque-Bera (JB):  } &    7.601  \\\\\n",
       "\\textbf{Skew:}          &  0.635 & \\textbf{  Prob(JB):          } &   0.0224  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.053 & \\textbf{  Cond. No.          } &     411.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      Y   R-squared (uncentered):                   0.967\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.966\n",
       "Method:                 Least Squares   F-statistic:                              631.7\n",
       "Date:                Fri, 07 Jun 2024   Prob (F-statistic):                    1.80e-47\n",
       "Time:                        15:33:34   Log-Likelihood:                         -240.00\n",
       "No. Observations:                  67   AIC:                                      486.0\n",
       "Df Residuals:                      64   BIC:                                      492.6\n",
       "Df Model:                           3                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "X1             0.8349      0.033     25.066      0.000       0.768       0.901\n",
       "X2             0.4648      0.091      5.094      0.000       0.283       0.647\n",
       "X3           -19.5200      8.509     -2.294      0.025     -36.519      -2.521\n",
       "==============================================================================\n",
       "Omnibus:                        7.916   Durbin-Watson:                   2.288\n",
       "Prob(Omnibus):                  0.019   Jarque-Bera (JB):                7.601\n",
       "Skew:                           0.635   Prob(JB):                       0.0224\n",
       "Kurtosis:                       4.053   Cond. No.                         411.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_lm = sm.OLS(train_target, train_data) # .assign(const=1)\n",
    "results = sm_lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the P>|t|. \n",
    "\n",
    "The p-value for two-tailed test is:\n",
    "$$\n",
    "p = 2 \\times P(T > |t|)\n",
    "$$\n",
    "where $ T $ follows the $ t $-distribution with $ df $ degrees of freedom.\n",
    "\n",
    "In words: \n",
    "> The p-value is the probability of observing a ùë°-statistic as extreme as, or more extreme than, the calculated value under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    8.429609e-35\n",
       "X2    3.326344e-06\n",
       "X3    2.508690e-02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5e-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same in R-style formula API\n",
    "\n",
    "see https://faculty.washington.edu/otoomet/machinelearning-py/linear-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   124.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 07 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>2.19e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:26:10</td>     <th>  Log-Likelihood:    </th> <td> -239.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    67</td>      <th>  AIC:               </th> <td>   486.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    63</td>      <th>  BIC:               </th> <td>   495.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -4.0758</td> <td>    3.767</td> <td>   -1.082</td> <td> 0.283</td> <td>  -11.603</td> <td>    3.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>        <td>    0.8955</td> <td>    0.065</td> <td>   13.749</td> <td> 0.000</td> <td>    0.765</td> <td>    1.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X2</th>        <td>    0.4519</td> <td>    0.092</td> <td>    4.917</td> <td> 0.000</td> <td>    0.268</td> <td>    0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X3</th>        <td>  -11.5671</td> <td>   11.236</td> <td>   -1.029</td> <td> 0.307</td> <td>  -34.020</td> <td>   10.886</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.237</td> <th>  Durbin-Watson:     </th> <td>   2.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.073</td> <th>  Jarque-Bera (JB):  </th> <td>   4.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.454</td> <th>  Prob(JB):          </th> <td>   0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.903</td> <th>  Cond. No.          </th> <td>    558.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        Y         & \\textbf{  R-squared:         } &     0.855   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.848   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     124.0   \\\\\n",
       "\\textbf{Date:}             & Fri, 07 Jun 2024 & \\textbf{  Prob (F-statistic):} &  2.19e-26   \\\\\n",
       "\\textbf{Time:}             &     15:26:10     & \\textbf{  Log-Likelihood:    } &   -239.39   \\\\\n",
       "\\textbf{No. Observations:} &          67      & \\textbf{  AIC:               } &     486.8   \\\\\n",
       "\\textbf{Df Residuals:}     &          63      & \\textbf{  BIC:               } &     495.6   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      -4.0758  &        3.767     &    -1.082  &         0.283        &      -11.603    &        3.452     \\\\\n",
       "\\textbf{X1}        &       0.8955  &        0.065     &    13.749  &         0.000        &        0.765    &        1.026     \\\\\n",
       "\\textbf{X2}        &       0.4519  &        0.092     &     4.917  &         0.000        &        0.268    &        0.636     \\\\\n",
       "\\textbf{X3}        &     -11.5671  &       11.236     &    -1.029  &         0.307        &      -34.020    &       10.886     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  5.237 & \\textbf{  Durbin-Watson:     } &    2.296  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.073 & \\textbf{  Jarque-Bera (JB):  } &    4.584  \\\\\n",
       "\\textbf{Skew:}          &  0.454 & \\textbf{  Prob(JB):          } &    0.101  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.903 & \\textbf{  Cond. No.          } &     558.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.855\n",
       "Model:                            OLS   Adj. R-squared:                  0.848\n",
       "Method:                 Least Squares   F-statistic:                     124.0\n",
       "Date:                Fri, 07 Jun 2024   Prob (F-statistic):           2.19e-26\n",
       "Time:                        15:26:10   Log-Likelihood:                -239.39\n",
       "No. Observations:                  67   AIC:                             486.8\n",
       "Df Residuals:                      63   BIC:                             495.6\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -4.0758      3.767     -1.082      0.283     -11.603       3.452\n",
       "X1             0.8955      0.065     13.749      0.000       0.765       1.026\n",
       "X2             0.4519      0.092      4.917      0.000       0.268       0.636\n",
       "X3           -11.5671     11.236     -1.029      0.307     -34.020      10.886\n",
       "==============================================================================\n",
       "Omnibus:                        5.237   Durbin-Watson:                   2.296\n",
       "Prob(Omnibus):                  0.073   Jarque-Bera (JB):                4.584\n",
       "Skew:                           0.454   Prob(JB):                        0.101\n",
       "Kurtosis:                       3.903   Cond. No.                         558.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula_lm = smf.ols(formula='Y ~ X1 + X2 + X3', data=data)\n",
    "results = formula_lm.fit()\n",
    "results.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
